% Chapter 1: Seminar - Time Series Analysis
% Quizzes, Practice Problems, and Discussion
% Target: Master Students in Statistics and Data Science

\documentclass[9pt, aspectratio=169, t]{beamer}

% Ensure content fits on slides
\setbeamersize{text margin left=8mm, text margin right=8mm}

%=============================================================================
% THEME AND STYLE CONFIGURATION
%=============================================================================
\usetheme{Madrid}
\usecolortheme{seahorse}

% IDA-Inspired Color Palette
\definecolor{MainBlue}{RGB}{26, 58, 110}
\definecolor{AccentBlue}{RGB}{42, 82, 140}
\definecolor{IDAred}{RGB}{220, 53, 69}
\definecolor{DarkGray}{RGB}{51, 51, 51}
\definecolor{MediumGray}{RGB}{128, 128, 128}
\definecolor{LightGray}{RGB}{248, 248, 248}
\definecolor{VeryLightGray}{RGB}{235, 235, 235}
\definecolor{Crimson}{RGB}{220, 53, 69}
\definecolor{Forest}{RGB}{46, 125, 50}
\definecolor{Amber}{RGB}{181, 133, 63}
\definecolor{Orange}{RGB}{230, 126, 34}

\setbeamercolor{palette primary}{bg=MainBlue, fg=white}
\setbeamercolor{palette secondary}{bg=MainBlue!85, fg=white}
\setbeamercolor{palette tertiary}{bg=MainBlue!70, fg=white}
\setbeamercolor{structure}{fg=MainBlue}
\setbeamercolor{title}{fg=MainBlue}
\setbeamercolor{frametitle}{fg=MainBlue, bg=white}
\setbeamercolor{block title}{bg=MainBlue, fg=white}
\setbeamercolor{block body}{bg=VeryLightGray, fg=DarkGray}
\setbeamercolor{block title alerted}{bg=Crimson, fg=white}
\setbeamercolor{block body alerted}{bg=Crimson!8, fg=DarkGray}
\setbeamercolor{block title example}{bg=Forest, fg=white}
\setbeamercolor{block body example}{bg=Forest!8, fg=DarkGray}
\setbeamercolor{item}{fg=MainBlue}

\setbeamertemplate{navigation symbols}{}

\setbeamertemplate{footline}{
    \leavevmode%
    \hbox{%
        \begin{beamercolorbox}[wd=.333333\paperwidth,ht=2.5ex,dp=1ex,center]{author in head/foot}%
            \usebeamerfont{author in head/foot}\insertshortauthor
        \end{beamercolorbox}%
        \begin{beamercolorbox}[wd=.333333\paperwidth,ht=2.5ex,dp=1ex,center]{title in head/foot}%
            \usebeamerfont{title in head/foot}\insertshorttitle
        \end{beamercolorbox}%
        \begin{beamercolorbox}[wd=.333333\paperwidth,ht=2.5ex,dp=1ex,right]{date in head/foot}%
            \usebeamerfont{date in head/foot}\insertshortdate{}\hspace*{2em}
            \insertframenumber{} / \inserttotalframenumber\hspace*{2ex}
        \end{beamercolorbox}}%
    \vskip0pt%
}

%=============================================================================
% PACKAGES
%=============================================================================
\usepackage{amsmath, amssymb, amsthm}
\usepackage{mathtools}
\usepackage{bm}
\usepackage{tikz}
\usetikzlibrary{arrows.meta, positioning, shapes, calc}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{array}
\usepackage{graphicx}
\usepackage{hyperref}
\hypersetup{colorlinks=false, pdfborder={0 0 0}}
% enumitem conflicts with beamer, using standard enumerate
\graphicspath{{logos/}{charts/}}

%=============================================================================
% CUSTOM COMMANDS
%=============================================================================
\newcommand{\E}{\mathbb{E}}
\newcommand{\Var}{\text{Var}}
\newcommand{\Cov}{\text{Cov}}
\newcommand{\Corr}{\text{Corr}}
\newcommand{\R}{\mathbb{R}}

% Quiz styling
\newcommand{\quizmark}{\textcolor{Orange}{\textbf{[QUIZ]}}}
\newcommand{\correct}{\textcolor{Forest}{\checkmark}}
\newcommand{\incorrect}{\textcolor{Crimson}{\texttimes}}

%=============================================================================
% TITLE INFORMATION
%=============================================================================
\title[Chapter 1: Seminar]{Chapter 1: Seminar \& Practice}
\subtitle{Bachelor program Faculty of Cybernetics, Statistics and Economic Informatics, Bucharest University of Economic Studies, Romania}
\author[Prof. dr. Daniel Traian Pele]{Prof. dr. Daniel Traian Pele\\[0.2cm]\footnotesize\texttt{danpele@ase.ro}}
\institute{Bucharest University of Economic Studies}
\date{Academic Year 2025--2026}

\begin{document}

%=============================================================================
% TITLE SLIDE
%=============================================================================
\begin{frame}[plain]
    \begin{tikzpicture}[remember picture, overlay]
        \node[anchor=north west] at ([xshift=0.5cm, yshift=-0.3cm]current page.north west) {
            \href{https://www.ase.ro}{\includegraphics[height=1.1cm]{ase_logo.png}}
        };
        \node[anchor=north] at ([yshift=-0.3cm]current page.north) {
            \href{https://ai4efin.ase.ro}{\includegraphics[height=1.1cm]{ai4efin_logo.png}}
        };
        \node[anchor=north east] at ([xshift=-0.5cm, yshift=-0.3cm]current page.north east) {
            \href{https://www.digital-finance-msca.com}{\includegraphics[height=1.1cm]{msca_logo.png}}
        };
    \end{tikzpicture}
    \vfill
    \begin{center}
        {\Huge\textbf{\textcolor{MainBlue}{Chapter 1: Introduction to Time Series}}}\\[0.5cm]
        {\Large\textcolor{MainBlue}{Seminar}}
    \end{center}
    \vfill

    \begin{tikzpicture}[remember picture, overlay]
        \node[anchor=south west] at ([xshift=0.5cm, yshift=0.8cm]current page.south west) {
            \href{https://theida.net}{\includegraphics[height=0.9cm]{ida_logo.png}}
        };
        \node[anchor=south] at ([xshift=-3cm, yshift=0.8cm]current page.south) {
            \href{https://blockchain-research-center.com}{\includegraphics[height=0.9cm]{brc_logo.png}}
        };
        \node[anchor=south] at ([yshift=0.8cm]current page.south) {
            \href{https://quantinar.com}{\includegraphics[height=0.9cm]{qr_logo.png}}
        };
        \node[anchor=south] at ([xshift=3cm, yshift=0.8cm]current page.south) {
            \href{https://quantlet.com}{\includegraphics[height=0.9cm]{ql_logo.png}}
        };
        \node[anchor=south east] at ([xshift=-0.5cm, yshift=0.8cm]current page.south east) {
            \href{https://ipe.ro/new}{\includegraphics[height=0.9cm]{acad_logo.png}}
        };
    \end{tikzpicture}
\end{frame}

%=============================================================================
% SEMINAR OUTLINE
%=============================================================================
\begin{frame}{Seminar Outline}
    \textbf{\large Today's Activities:}

    \vspace{0.4cm}

    \begin{enumerate}
        \item[\textcolor{MainBlue}{\textbf{1.}}] \textbf{Quick Review} -- Key concepts recap
        \vspace{0.15cm}
        \item[\textcolor{MainBlue}{\textbf{2.}}] \textbf{Multiple Choice Quizzes} -- Test your understanding
        \vspace{0.15cm}
        \item[\textcolor{MainBlue}{\textbf{3.}}] \textbf{True/False Questions} -- Conceptual checks
        \vspace{0.15cm}
        \item[\textcolor{MainBlue}{\textbf{4.}}] \textbf{Calculation Exercises} -- Hands-on practice
        \vspace{0.15cm}
        \item[\textcolor{MainBlue}{\textbf{5.}}] \textbf{Python Exercises} -- Coding practice
        \vspace{0.15cm}
        \item[\textcolor{MainBlue}{\textbf{6.}}] \textbf{Discussion Questions} -- Critical thinking
    \end{enumerate}
\end{frame}

%=============================================================================
% PART 1: QUICK REVIEW
%=============================================================================
\section{Quick Review}

\begin{frame}{Key Formulas to Remember}
    \begin{columns}[T]
        \begin{column}{0.48\textwidth}
            \textbf{Decomposition:}
            \begin{itemize}
                \item Additive: $X_t = T_t + S_t + \varepsilon_t$
                \item Multiplicative: $X_t = T_t \times S_t \times \varepsilon_t$
            \end{itemize}

            \vspace{0.3cm}

            \textbf{Exponential Smoothing:}
            \begin{itemize}
                \item SES: $\hat{X}_{t+1} = \alpha X_t + (1-\alpha)\hat{X}_t$
                \item Holt: adds trend $b_t$
                \item HW: adds seasonal $S_t$
            \end{itemize}
        \end{column}
        \begin{column}{0.48\textwidth}
            \textbf{Stationarity:}
            \begin{itemize}
                \item $\E[X_t] = \mu$ (constant)
                \item $\Var(X_t) = \sigma^2$ (constant)
                \item $\Cov(X_t, X_{t+h}) = \gamma(h)$
            \end{itemize}

            \vspace{0.3cm}

            \textbf{Random Walk:}
            \begin{itemize}
                \item $X_t = X_{t-1} + \varepsilon_t$
                \item $\Var(X_t) = t\sigma^2$ (grows!)
            \end{itemize}
        \end{column}
    \end{columns}
\end{frame}

\begin{frame}{Key Concepts Summary}
    \begin{center}
    \small
    \begin{tabular}{lll}
        \toprule
        \textbf{Concept} & \textbf{Key Point} & \textbf{When to Use} \\
        \midrule
        Additive decomp. & Constant seasonal amplitude & Stable variance \\
        Multiplicative decomp. & Seasonal grows with level & Increasing variance \\
        SES & Level only ($\alpha$) & No trend, no seasonality \\
        Holt & Level + Trend ($\alpha, \beta$) & Trend, no seasonality \\
        Holt-Winters & Level + Trend + Seasonal & Trend and seasonality \\
        \midrule
        ADF Test & $H_0$: unit root & Test for non-stationarity \\
        KPSS Test & $H_0$: stationary & Confirm stationarity \\
        \midrule
        Differencing & Remove stochastic trend & Random walk, unit root \\
        Regression & Remove deterministic trend & Linear/polynomial trend \\
        \bottomrule
    \end{tabular}
    \end{center}
\end{frame}

%=============================================================================
% PART 2: MULTIPLE CHOICE QUIZZES
%=============================================================================
\section{Multiple Choice Quizzes}

\begin{frame}{\quizmark{} Quiz 1: Time Series Basics}
    \textbf{Question:} Which of the following is NOT a characteristic of time series data?

    \vspace{0.4cm}

    \begin{enumerate}[A.]
        \item Observations are ordered in time
        \item Consecutive observations are typically correlated
        \item Observations are independent and identically distributed
        \item The data has a natural temporal ordering
    \end{enumerate}

    \vspace{0.5cm}

    \begin{center}
        \textit{Think about it before moving to the next slide...}
    \end{center}
\end{frame}

\begin{frame}{\quizmark{} Quiz 1: Answer}
    \textbf{Question:} Which is NOT a characteristic of time series data?

    \vspace{0.3cm}

    \begin{enumerate}[A.]
        \item Observations are ordered in time \incorrect
        \item Consecutive observations are typically correlated \incorrect
        \item \textbf{\textcolor{Forest}{Observations are independent and identically distributed}} \correct
        \item The data has a natural temporal ordering \incorrect
    \end{enumerate}

    \vspace{0.3cm}

    \begin{block}{Explanation}
        Time series observations are typically \textbf{dependent} (autocorrelated), not independent. The assumption of i.i.d. observations is fundamental to cross-sectional analysis but violated in time series. This temporal dependence is what makes time series analysis unique and requires specialized methods.
    \end{block}
\end{frame}

\begin{frame}{\quizmark{} Quiz 2: Decomposition}
    \textbf{Question:} When should you use multiplicative decomposition instead of additive?

    \vspace{0.4cm}

    \begin{enumerate}[A.]
        \item When the seasonal pattern has constant amplitude
        \item When the variance of the series is stable over time
        \item When the seasonal fluctuations grow proportionally with the level
        \item When the time series has no trend component
    \end{enumerate}

    \vspace{0.5cm}

    \begin{center}
        \textit{Think about it before moving to the next slide...}
    \end{center}
\end{frame}

\begin{frame}{\quizmark{} Quiz 2: Answer}
    \textbf{Question:} When should you use multiplicative decomposition?

    \vspace{0.3cm}

    \begin{enumerate}[A.]
        \item When the seasonal pattern has constant amplitude \incorrect
        \item When the variance of the series is stable over time \incorrect
        \item \textbf{\textcolor{Forest}{When the seasonal fluctuations grow proportionally with the level}} \correct
        \item When the time series has no trend component \incorrect
    \end{enumerate}

    \vspace{0.3cm}

    \begin{exampleblock}{Explanation}
        \vspace{-0.2cm}
        \begin{center}
            \includegraphics[width=0.95\textwidth, height=0.48\textheight, keepaspectratio]{charts/sem1_decomposition.pdf}
        \end{center}
        \vspace{-0.2cm}
        {\footnotesize
        \textbf{Multiplicative}: $X_t = T_t \times S_t \times \varepsilon_t$, seasonal amplitude \textbf{scales with level} (fan-shaped pattern)
        }
    \end{exampleblock}
\end{frame}

\begin{frame}{\quizmark{} Quiz 3: Exponential Smoothing}
    \textbf{Question:} In Simple Exponential Smoothing with $\alpha = 0.9$, what happens?

    \vspace{0.4cm}

    \begin{enumerate}[A.]
        \item Forecasts are very smooth and stable
        \item Recent observations have very little weight
        \item Forecasts react quickly to recent changes
        \item The forecast is essentially a long-term average
    \end{enumerate}

    \vspace{0.5cm}

    \begin{center}
        \textit{Think about it before moving to the next slide...}
    \end{center}
\end{frame}

\begin{frame}{\quizmark{} Quiz 3: Answer}
    \textbf{Question:} In SES with $\alpha = 0.9$, what happens?

    \vspace{0.3cm}

    \begin{enumerate}[A.]
        \item Forecasts are very smooth and stable \incorrect
        \item Recent observations have very little weight \incorrect
        \item \textbf{\textcolor{Forest}{Forecasts react quickly to recent changes}} \correct
        \item The forecast is essentially a long-term average \incorrect
    \end{enumerate}

    \vspace{0.3cm}

    \begin{block}{Explanation}
        With $\alpha = 0.9$: $\hat{X}_{t+1} = 0.9 X_t + 0.1 \hat{X}_t$

        This means 90\% weight on the most recent observation! High $\alpha$ values make forecasts very responsive to new data. Low $\alpha$ (e.g., 0.1) produces smoother, more stable forecasts that average over more history.
    \end{block}
\end{frame}

\begin{frame}{\quizmark{} Quiz 4: Stationarity}
    \textbf{Question:} A random walk process $X_t = X_{t-1} + \varepsilon_t$ is:

    \vspace{0.4cm}

    \begin{enumerate}[A.]
        \item Strictly stationary
        \item Weakly stationary
        \item Non-stationary because variance grows with time
        \item Stationary after adding a constant
    \end{enumerate}

    \vspace{0.5cm}

    \begin{center}
        \textit{Think about it before moving to the next slide...}
    \end{center}
\end{frame}

\begin{frame}{\quizmark{} Quiz 4: Answer}
    \textbf{Question:} A random walk is:

    \vspace{0.3cm}

    \begin{enumerate}[A.]
        \item Strictly stationary \incorrect
        \item Weakly stationary \incorrect
        \item \textbf{\textcolor{Forest}{Non-stationary because variance grows with time}} \correct
        \item Stationary after adding a constant \incorrect
    \end{enumerate}

    \vspace{0.3cm}

    \begin{block}{Explanation}
        For random walk: $X_t = \sum_{i=1}^{t} \varepsilon_i$
        \begin{itemize}
            \item $\E[X_t] = 0$ (constant mean -- OK)
            \item $\Var(X_t) = t\sigma^2$ (variance depends on $t$ -- NOT OK!)
        \end{itemize}
        Since variance is not constant, the process violates the stationarity condition. Solution: \textbf{differencing} gives $\Delta X_t = \varepsilon_t$ which IS stationary.
    \end{block}
\end{frame}

\begin{frame}{Visual: Random Walk vs Stationary}
    \begin{center}
        \includegraphics[width=0.95\textwidth]{charts/ch3_def_random_walk.pdf}
    \end{center}
    \vspace{-0.2cm}
    \small Random walk paths wander unpredictably; variance grows linearly with time $\Rightarrow$ non-stationary.
\end{frame}

\begin{frame}{\quizmark{} Quiz 5: Unit Root Tests}
    \textbf{Question:} You run ADF and KPSS tests. ADF fails to reject $H_0$, and KPSS rejects $H_0$. What do you conclude?

    \vspace{0.4cm}

    \begin{enumerate}[A.]
        \item The series is stationary
        \item The series has a unit root (non-stationary)
        \item The results are inconclusive
        \item You need to run more tests
    \end{enumerate}

    \vspace{0.5cm}

    \begin{center}
        \textit{Think about it before moving to the next slide...}
    \end{center}
\end{frame}

\begin{frame}{\quizmark{} Quiz 5: Answer}
    \textbf{Question:} ADF fails to reject, KPSS rejects. Conclusion?

    \vspace{0.3cm}

    \begin{enumerate}[A.]
        \item The series is stationary \incorrect
        \item \textbf{\textcolor{Forest}{The series has a unit root (non-stationary)}} \correct
        \item The results are inconclusive \incorrect
        \item You need to run more tests \incorrect
    \end{enumerate}

    \vspace{0.3cm}

    \begin{block}{Explanation}
        \begin{itemize}
            \item ADF: $H_0$ = unit root. Fail to reject $\Rightarrow$ evidence FOR unit root
            \item KPSS: $H_0$ = stationary. Reject $\Rightarrow$ evidence AGAINST stationarity
        \end{itemize}
        Both tests agree: the series is \textbf{non-stationary}. You should difference the series before modeling with ARMA.
    \end{block}
\end{frame}

\begin{frame}{\quizmark{} Quiz 6: Forecast Evaluation}
    \textbf{Question:} Which metric is most appropriate for comparing forecast accuracy across different time series with different scales?

    \vspace{0.4cm}

    \begin{enumerate}[A.]
        \item Mean Absolute Error (MAE)
        \item Root Mean Squared Error (RMSE)
        \item Mean Absolute Percentage Error (MAPE)
        \item Mean Squared Error (MSE)
    \end{enumerate}

    \vspace{0.5cm}

    \begin{center}
        \textit{Think about it before moving to the next slide...}
    \end{center}
\end{frame}

\begin{frame}{\quizmark{} Quiz 6: Answer}
    \textbf{Question:} Best metric for comparing across different scales?

    \vspace{0.3cm}

    \begin{enumerate}[A.]
        \item Mean Absolute Error (MAE) \incorrect
        \item Root Mean Squared Error (RMSE) \incorrect
        \item \textbf{\textcolor{Forest}{Mean Absolute Percentage Error (MAPE)}} \correct
        \item Mean Squared Error (MSE) \incorrect
    \end{enumerate}

    \vspace{0.3cm}

    \begin{block}{Explanation}
        MAPE $= \frac{100}{n}\sum\left|\frac{e_t}{X_t}\right|$ expresses errors as \textbf{percentages}.

        \begin{itemize}
            \item MAE, RMSE, MSE are \textbf{scale-dependent} (units of $X_t$)
            \item MAPE is \textbf{scale-independent} (always in \%)
            \item Caveat: MAPE fails when $X_t$ is near zero
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}{\quizmark{} Quiz 7: Trend Types}
    \textbf{Question:} A deterministic trend can be removed by:

    \vspace{0.4cm}

    \begin{enumerate}[A.]
        \item Differencing
        \item Regression on time
        \item Seasonal adjustment
        \item Moving average smoothing
    \end{enumerate}

    \vspace{0.5cm}
    \pause
    \begin{block}{Answer: B -- Regression on time}
        \textbf{Deterministic trend}: $Y_t = \alpha + \beta t + \varepsilon_t$ where $\beta$ is fixed.

        \textbf{Removal method}: Regress $Y_t$ on $t$, then analyze residuals $\hat{\varepsilon}_t = Y_t - \hat{\alpha} - \hat{\beta}t$

        \textbf{Why not differencing?} Differencing a deterministic trend gives: $\Delta Y_t = \beta + \Delta\varepsilon_t$, which removes the trend but leaves a constant. For \textit{stochastic} trends (unit roots), differencing is correct.
    \end{block}
\end{frame}

\begin{frame}{\quizmark{} Quiz 8: ACF Interpretation}
    \textbf{Question:} If the ACF of a time series decays very slowly (remains significant for many lags), this suggests:

    \vspace{0.4cm}

    \begin{enumerate}[A.]
        \item The series is white noise
        \item The series is likely non-stationary
        \item The series has no autocorrelation
        \item The series is perfectly predictable
    \end{enumerate}

    \vspace{0.5cm}
    \pause
    \begin{block}{Answer: B -- The series is likely non-stationary}
        \vspace{-0.2cm}
        \begin{center}
            \includegraphics[width=0.85\textwidth, height=0.45\textheight, keepaspectratio]{charts/sem1_acf_decay.pdf}
        \end{center}
        \vspace{-0.2cm}
        {\footnotesize
        \textbf{Stationary}: ACF decays quickly ($\rho_k = \phi^k \to 0$) \quad
        \textbf{Non-stationary}: ACF stays near 1 $\Rightarrow$ differencing needed
        }
    \end{block}
\end{frame}

\begin{frame}{\quizmark{} Quiz 9: Holt's Method}
    \textbf{Question:} Holt's exponential smoothing differs from SES by adding:

    \vspace{0.4cm}

    \begin{enumerate}[A.]
        \item A seasonal component
        \item A trend component
        \item A cyclical component
        \item An irregular component
    \end{enumerate}

    \vspace{0.5cm}
    \pause
    \begin{exampleblock}{Answer: B -- A trend component}
        \vspace{-0.2cm}
        \begin{center}
            \includegraphics[width=0.95\textwidth, height=0.48\textheight, keepaspectratio]{charts/sem1_holt_method.pdf}
        \end{center}
        \vspace{-0.2cm}
        {\footnotesize
        \textbf{Holt}: $L_t = \alpha Y_t + (1-\alpha)(L_{t-1} + b_{t-1})$; $b_t = \beta(L_t - L_{t-1}) + (1-\beta)b_{t-1}$ \quad
        \textbf{Forecast}: $\hat{Y}_{t+h} = L_t + h \cdot b_t$
        }
    \end{exampleblock}
\end{frame}

\begin{frame}{\quizmark{} Quiz 10: White Noise}
    \textbf{Question:} Which property is NOT required for a process to be white noise?

    \vspace{0.4cm}

    \begin{enumerate}[A.]
        \item $\E[\varepsilon_t] = 0$
        \item $\Var(\varepsilon_t) = \sigma^2$ (constant)
        \item $\Cov(\varepsilon_t, \varepsilon_s) = 0$ for $t \neq s$
        \item $\varepsilon_t \sim N(0, \sigma^2)$
    \end{enumerate}

    \vspace{0.5cm}
    \pause
    \begin{exampleblock}{Answer: D -- Normality is NOT required}
        \vspace{-0.2cm}
        \begin{center}
            \includegraphics[width=0.9\textwidth, height=0.45\textheight, keepaspectratio]{charts/sem1_white_noise.pdf}
        \end{center}
        \vspace{-0.2cm}
        {\footnotesize
        \textbf{White noise}: Zero mean, constant variance, uncorrelated. \textbf{Gaussian WN}: Adds normality $\Rightarrow$ also independent (not just uncorrelated)
        }
    \end{exampleblock}
\end{frame}

\begin{frame}{Visual: White Noise Properties}
    \begin{center}
        \includegraphics[width=0.95\textwidth]{charts/ch1_def_white_noise.pdf}
    \end{center}
    \vspace{-0.2cm}
    \small Left: white noise fluctuates around zero. Right: ACF shows no autocorrelation (all values near zero after lag 0).
\end{frame}

\begin{frame}{\quizmark{} Quiz 11: Forecast Horizon}
    \textbf{Question:} As forecast horizon $h$ increases, what typically happens to forecast intervals?

    \vspace{0.4cm}

    \begin{enumerate}[A.]
        \item They become narrower
        \item They stay the same width
        \item They become wider
        \item They disappear
    \end{enumerate}

    \vspace{0.5cm}
    \pause
    \begin{block}{Answer: C -- They become wider}
        \vspace{-0.2cm}
        \begin{center}
            \includegraphics[width=0.9\textwidth, height=0.45\textheight, keepaspectratio]{charts/sem1_forecast_intervals.pdf}
        \end{center}
        \vspace{-0.2cm}
        {\footnotesize
        \textbf{Random walk}: $\Var = h\sigma^2$ (grows linearly) \quad
        \textbf{95\% CI}: $\hat{Y}_{t+h} \pm 1.96\sqrt{h}\sigma$ (widens with $\sqrt{h}$)
        }
    \end{block}
\end{frame}

\begin{frame}{\quizmark{} Quiz 12: Seasonality Detection}
    \textbf{Question:} The ACF shows significant spikes at lags 12, 24, and 36 for monthly data. This suggests:

    \vspace{0.4cm}

    \begin{enumerate}[A.]
        \item No seasonality
        \item Annual seasonality
        \item Weekly seasonality
        \item Random noise
    \end{enumerate}

    \vspace{0.5cm}
    \pause
    \begin{block}{Answer: B -- Annual seasonality}
        \textbf{Pattern recognition}:
        \begin{itemize}
            \item Lag 12: correlation with same month last year
            \item Lag 24: correlation with same month two years ago
            \item Lag 36: correlation with same month three years ago
        \end{itemize}

        \textbf{Seasonal period}: $s = 12$ (monthly data with annual cycle)

        \textbf{Common patterns}: Retail sales (December peaks), energy consumption (summer/winter), tourism data
    \end{block}
\end{frame}

\begin{frame}{\quizmark{} Quiz 13: Cross-Validation in Time Series}
    \textbf{Question:} Why can't we use standard k-fold cross-validation for time series?

    \vspace{0.4cm}

    \begin{enumerate}[A.]
        \item Time series data is too small
        \item It would violate temporal ordering (future predicting past)
        \item Cross-validation is always invalid
        \item Time series don't need validation
    \end{enumerate}

    \vspace{0.5cm}
    \pause
    \begin{block}{Answer: B -- It would violate temporal ordering}
        \vspace{-0.2cm}
        \begin{center}
            \includegraphics[width=0.95\textwidth, height=0.5\textheight, keepaspectratio]{charts/sem1_timeseries_cv.pdf}
        \end{center}
        \vspace{-0.2cm}
        {\footnotesize
        \textbf{Rule}: Never use future data to predict past! Use rolling/expanding window CV.
        }
    \end{block}
\end{frame}

\begin{frame}{\quizmark{} Quiz 14: MAPE Limitation}
    \textbf{Question:} MAPE (Mean Absolute Percentage Error) should NOT be used when:

    \vspace{0.4cm}

    \begin{enumerate}[A.]
        \item Comparing models on the same dataset
        \item The actual values can be zero or near zero
        \item Forecasting stock prices
        \item The data has a trend
    \end{enumerate}

    \vspace{0.5cm}
    \pause
    \begin{block}{Answer: B -- When actual values can be zero or near zero}
        \textbf{MAPE formula}: $\text{MAPE} = \frac{100\%}{n}\sum_{t=1}^{n}\left|\frac{Y_t - \hat{Y}_t}{Y_t}\right|$

        \textbf{Problem}: When $Y_t \approx 0$, division causes MAPE $\to \infty$

        \textbf{Alternatives}:
        \begin{itemize}
            \item \textbf{SMAPE}: $\frac{200\%}{n}\sum\frac{|Y_t - \hat{Y}_t|}{|Y_t| + |\hat{Y}_t|}$ (bounded 0--200\%)
            \item \textbf{MASE}: $\frac{1}{n}\sum\frac{|e_t|}{\frac{1}{n-1}\sum|Y_t - Y_{t-1}|}$ (scale-free)
        \end{itemize}
    \end{block}
\end{frame}

%=============================================================================
% PART 3: TRUE/FALSE QUESTIONS
%=============================================================================
\section{True/False Questions}

\begin{frame}{\quizmark{} True or False? (Set 2)}
    \textbf{Mark each statement as True (T) or False (F):}

    \vspace{0.3cm}

    \begin{enumerate}
        \item A time series with constant mean is always stationary. \hfill \underline{\hspace{1cm}}
        \vspace{0.2cm}
        \item The variance of a random walk increases linearly with time. \hfill \underline{\hspace{1cm}}
        \vspace{0.2cm}
        \item SES forecasts are always flat (constant for all horizons). \hfill \underline{\hspace{1cm}}
        \vspace{0.2cm}
        \item ADF and KPSS tests have the same null hypothesis. \hfill \underline{\hspace{1cm}}
        \vspace{0.2cm}
        \item Lower RMSE always means better forecasts. \hfill \underline{\hspace{1cm}}
        \vspace{0.2cm}
        \item Autocorrelation at lag 0 is always equal to 1. \hfill \underline{\hspace{1cm}}
    \end{enumerate}
\end{frame}

\begin{frame}{\quizmark{} True or False: Answers (Set 2)}
    \begin{enumerate}
        \item A time series with constant mean is always stationary. \hfill \textbf{\textcolor{Crimson}{FALSE}}

        {\small \textcolor{MediumGray}{Also need constant variance and covariance depending only on lag.}}

        \vspace{0.15cm}

        \item The variance of a random walk increases linearly with time. \hfill \textbf{\textcolor{Forest}{TRUE}}

        {\small \textcolor{MediumGray}{$\Var(X_t) = t\sigma^2$ for random walk starting at $X_0$.}}

        \vspace{0.15cm}

        \item SES forecasts are always flat (constant for all horizons). \hfill \textbf{\textcolor{Forest}{TRUE}}

        {\small \textcolor{MediumGray}{SES has no trend component, so $\hat{X}_{t+h} = L_t$ for all $h$.}}

        \vspace{0.15cm}

        \item ADF and KPSS tests have the same null hypothesis. \hfill \textbf{\textcolor{Crimson}{FALSE}}

        {\small \textcolor{MediumGray}{ADF: $H_0$ = unit root. KPSS: $H_0$ = stationary. Opposite nulls!}}

        \vspace{0.15cm}

        \item Lower RMSE always means better forecasts. \hfill \textbf{\textcolor{Crimson}{FALSE}}

        {\small \textcolor{MediumGray}{Depends on context. RMSE is scale-dependent; may overfit to outliers.}}

        \vspace{0.15cm}

        \item Autocorrelation at lag 0 is always equal to 1. \hfill \textbf{\textcolor{Forest}{TRUE}}

        {\small \textcolor{MediumGray}{$\rho(0) = \gamma(0)/\gamma(0) = 1$ by definition.}}
    \end{enumerate}
\end{frame}

\begin{frame}{\quizmark{} True or False?}
    \textbf{Mark each statement as True (T) or False (F):}

    \vspace{0.3cm}

    \begin{enumerate}
        \item The ACF of a stationary AR(1) process decays exponentially. \hfill \underline{\hspace{1cm}}

        \vspace{0.2cm}

        \item White noise is always normally distributed. \hfill \underline{\hspace{1cm}}

        \vspace{0.2cm}

        \item Differencing can make a non-stationary series stationary. \hfill \underline{\hspace{1cm}}

        \vspace{0.2cm}

        \item The PACF of a MA(1) process cuts off after lag 1. \hfill \underline{\hspace{1cm}}

        \vspace{0.2cm}

        \item You should always use the test set for hyperparameter tuning. \hfill \underline{\hspace{1cm}}

        \vspace{0.2cm}

        \item Holt-Winters is appropriate for data with no seasonality. \hfill \underline{\hspace{1cm}}
    \end{enumerate}

    \vspace{0.3cm}

    \begin{center}
        \textit{Answers on next slide...}
    \end{center}
\end{frame}

\begin{frame}{\quizmark{} True or False: Answers}
    \begin{enumerate}
        \item The ACF of a stationary AR(1) decays exponentially. \hfill \textbf{\textcolor{Forest}{TRUE}}

        {\small \textcolor{MediumGray}{For AR(1): $\rho(h) = \phi^h$, which decays exponentially.}}

        \vspace{0.15cm}

        \item White noise is always normally distributed. \hfill \textbf{\textcolor{Crimson}{FALSE}}

        {\small \textcolor{MediumGray}{White noise only requires zero mean, constant variance, no autocorrelation. Gaussian white noise is a special case.}}

        \vspace{0.15cm}

        \item Differencing can make a non-stationary series stationary. \hfill \textbf{\textcolor{Forest}{TRUE}}

        {\small \textcolor{MediumGray}{Differencing removes stochastic trends (unit roots).}}

        \vspace{0.15cm}

        \item The PACF of a MA(1) cuts off after lag 1. \hfill \textbf{\textcolor{Crimson}{FALSE}}

        {\small \textcolor{MediumGray}{It's the ACF that cuts off for MA. PACF decays for MA processes.}}

        \vspace{0.15cm}

        \item You should always use the test set for hyperparameter tuning. \hfill \textbf{\textcolor{Crimson}{FALSE}}

        {\small \textcolor{MediumGray}{Use validation set for tuning. Test set is for final evaluation only!}}

        \vspace{0.15cm}

        \item Holt-Winters is appropriate for data with no seasonality. \hfill \textbf{\textcolor{Crimson}{FALSE}}

        {\small \textcolor{MediumGray}{Use Holt's method (no seasonal component) or SES for non-seasonal data.}}
    \end{enumerate}
\end{frame}

%=============================================================================
% PART 4: CALCULATION EXERCISES
%=============================================================================
\section{Calculation Exercises}

\begin{frame}{Exercise 1: Simple Exponential Smoothing}
    \textbf{Problem:} Given the following data and $\alpha = 0.3$:

    \begin{center}
    \begin{tabular}{c|ccccc}
        $t$ & 1 & 2 & 3 & 4 & 5 \\
        \hline
        $X_t$ & 10 & 12 & 11 & 14 & 13 \\
    \end{tabular}
    \end{center}

    Starting with $\hat{X}_1 = X_1 = 10$, calculate:

    \vspace{0.3cm}

    \begin{enumerate}[a)]
        \item The forecasts $\hat{X}_2, \hat{X}_3, \hat{X}_4, \hat{X}_5$
        \item The forecast for $t = 6$: $\hat{X}_6$
        \item The forecast errors $e_t = X_t - \hat{X}_t$ for $t = 2, 3, 4, 5$
        \item The MAE and RMSE
    \end{enumerate}

    \vspace{0.3cm}

    \textbf{Formula:} $\hat{X}_{t+1} = \alpha X_t + (1-\alpha)\hat{X}_t$
\end{frame}

\begin{frame}{Exercise 1: Solution}
    \textbf{Using} $\hat{X}_{t+1} = 0.3 X_t + 0.7 \hat{X}_t$:

    \vspace{0.2cm}

    \begin{center}
    \small
    \begin{tabular}{c|ccccc|c}
        $t$ & 1 & 2 & 3 & 4 & 5 & 6\\
        \hline
        $X_t$ & 10 & 12 & 11 & 14 & 13 & ?\\
        $\hat{X}_t$ & 10 & 10 & 10.6 & 10.72 & 11.70 & \textbf{12.09}\\
        $e_t$ & -- & 2 & 0.4 & 3.28 & 1.30 & --\\
    \end{tabular}
    \end{center}

    \vspace{0.2cm}

    \textbf{Calculations:}
    \begin{itemize}
        \item $\hat{X}_2 = 0.3(10) + 0.7(10) = 10$
        \item $\hat{X}_3 = 0.3(12) + 0.7(10) = 10.6$
        \item $\hat{X}_4 = 0.3(11) + 0.7(10.6) = 10.72$
        \item $\hat{X}_5 = 0.3(14) + 0.7(10.72) = 11.70$
        \item $\hat{X}_6 = 0.3(13) + 0.7(11.70) = \textbf{12.09}$
    \end{itemize}

    \vspace{0.2cm}

    \textbf{MAE} $= \frac{|2|+|0.4|+|3.28|+|1.30|}{4} = 1.745$ \quad \textbf{RMSE} $= \sqrt{\frac{4+0.16+10.76+1.69}{4}} = 2.04$
\end{frame}

\begin{frame}{Exercise 2: Autocovariance}
    \textbf{Problem:} For a stationary process with:
    \begin{itemize}
        \item $\E[X_t] = 5$
        \item $\gamma(0) = 4$ (variance)
        \item $\gamma(1) = 2$
        \item $\gamma(2) = 1$
    \end{itemize}

    \vspace{0.3cm}

    Calculate:
    \begin{enumerate}[a)]
        \item The autocorrelation function $\rho(0), \rho(1), \rho(2)$
        \item $\Cov(X_t, X_{t-1})$
        \item $\Corr(X_5, X_7)$
        \item If $X_t = 6$, what is $\E[X_{t+1} | X_t = 6]$ assuming AR(1)?
    \end{enumerate}
\end{frame}

\begin{frame}{Exercise 2: Solution}
    \textbf{a) Autocorrelations:}
    \[
        \rho(h) = \frac{\gamma(h)}{\gamma(0)}
    \]
    \begin{itemize}
        \item $\rho(0) = \gamma(0)/\gamma(0) = 1$
        \item $\rho(1) = \gamma(1)/\gamma(0) = 2/4 = 0.5$
        \item $\rho(2) = \gamma(2)/\gamma(0) = 1/4 = 0.25$
    \end{itemize}

    \vspace{0.2cm}

    \textbf{b)} $\Cov(X_t, X_{t-1}) = \gamma(1) = 2$ \quad (by stationarity, lag 1 covariance)

    \vspace{0.2cm}

    \textbf{c)} $\Corr(X_5, X_7) = \rho(|7-5|) = \rho(2) = 0.25$

    \vspace{0.2cm}

    \textbf{d)} For AR(1) with $\phi = \rho(1) = 0.5$:
    \[
        \E[X_{t+1} | X_t] = \mu + \phi(X_t - \mu) = 5 + 0.5(6-5) = 5.5
    \]
\end{frame}

\begin{frame}{Exercise 3: Random Walk Properties}
    \textbf{Problem:} Consider a random walk $X_t = X_{t-1} + \varepsilon_t$ where $\varepsilon_t \sim WN(0, 4)$ and $X_0 = 100$.

    \vspace{0.4cm}

    Calculate:
    \begin{enumerate}[a)]
        \item $\E[X_{10}]$
        \item $\Var(X_{10})$
        \item $\Cov(X_5, X_{10})$
        \item The 95\% confidence interval for $X_{100}$
        \item After observing $X_5 = 108$, what is your best forecast for $X_6$?
    \end{enumerate}
\end{frame}

\begin{frame}{Exercise 3: Solution}
    \textbf{Random walk:} $X_t = X_0 + \sum_{i=1}^{t} \varepsilon_i$ with $\sigma^2 = 4$

    \vspace{0.3cm}

    \textbf{a)} $\E[X_{10}] = X_0 = 100$ \quad (mean stays at starting value)

    \vspace{0.2cm}

    \textbf{b)} $\Var(X_{10}) = 10 \times \sigma^2 = 10 \times 4 = 40$

    \vspace{0.2cm}

    \textbf{c)} $\Cov(X_5, X_{10}) = \min(5, 10) \times \sigma^2 = 5 \times 4 = 20$

    \vspace{0.2cm}

    \textbf{d)} For $X_{100}$:
    \begin{itemize}
        \item $\E[X_{100}] = 100$, $\Var(X_{100}) = 400$, $SD = 20$
        \item 95\% CI: $100 \pm 1.96 \times 20 = [60.8, 139.2]$
    \end{itemize}

    \vspace{0.2cm}

    \textbf{e)} Best forecast: $\hat{X}_6 = X_5 = 108$

    {\small (Random walk: best forecast is the last observed value)}
\end{frame}

%=============================================================================
% PART 5: PYTHON EXERCISES
%=============================================================================
\section{Python Exercises}

\begin{frame}[fragile]{Python Exercise 1: Load and Plot}
    \textbf{Task:} Load S\&P 500 data and create a basic time series plot.

    \vspace{0.3cm}

    \begin{block}{Starter Code}
    \small
    \begin{verbatim}
import yfinance as yf
import matplotlib.pyplot as plt

# Download S&P 500 data
sp500 = yf.download('^GSPC', start='2020-01-01', end='2025-01-01')

# TODO: Plot the closing prices
# TODO: Add title, labels, and grid
# TODO: Calculate and print basic statistics
    \end{verbatim}
    \end{block}

    \vspace{0.2cm}

    \textbf{Questions:}
    \begin{enumerate}
        \item What is the mean and standard deviation of returns?
        \item Does the series appear stationary? Why or why not?
    \end{enumerate}
\end{frame}

\begin{frame}[fragile]{Python Exercise 2: Decomposition}
    \textbf{Task:} Perform STL decomposition on airline passengers data.

    \vspace{0.3cm}

    \begin{block}{Starter Code}
    \small
    \begin{verbatim}
from statsmodels.tsa.seasonal import STL
import pandas as pd

# Load airline passengers
url = 'https://raw.githubusercontent.com/..../airline.csv'
airline = pd.read_csv(url, parse_dates=['Month'],
                      index_col='Month')

# TODO: Apply STL decomposition with period=12
# TODO: Plot all components
# TODO: What percentage of variance is explained by trend?
    \end{verbatim}
    \end{block}

    \textbf{Hint:} Use \texttt{STL(data, period=12).fit()}
\end{frame}

\begin{frame}[fragile]{Python Exercise 3: Exponential Smoothing}
    \textbf{Task:} Compare SES, Holt, and Holt-Winters on real data.

    \vspace{0.3cm}

    \begin{block}{Starter Code}
    \small
    \begin{verbatim}
from statsmodels.tsa.holtwinters import (SimpleExpSmoothing,
    ExponentialSmoothing)

# Split data: 80% train, 20% test
train = airline[:'1958']
test = airline['1959':]

# TODO: Fit SES, Holt, and Holt-Winters
# TODO: Generate forecasts for test period
# TODO: Calculate RMSE for each method
# TODO: Which method performs best? Why?
    \end{verbatim}
    \end{block}
\end{frame}

\begin{frame}[fragile]{Python Exercise 4: Stationarity Testing}
    \textbf{Task:} Test for stationarity using ADF and KPSS tests.

    \vspace{0.3cm}

    \begin{block}{Starter Code}
    \small
    \begin{verbatim}
from statsmodels.tsa.stattools import adfuller, kpss

# Test S&P 500 prices
prices = sp500['Close']
returns = prices.pct_change().dropna()

# TODO: Run ADF test on prices and returns
# TODO: Run KPSS test on prices and returns
# TODO: Interpret the results

# ADF: adfuller(series)
# KPSS: kpss(series, regression='c')
    \end{verbatim}
    \end{block}

    \textbf{Questions:}
    \begin{enumerate}
        \item Are prices stationary? Are returns stationary?
        \item Do ADF and KPSS agree?
    \end{enumerate}
\end{frame}

%=============================================================================
% PART 6: REAL DATA ANALYSIS
%=============================================================================
\section{Real Data Analysis}

\begin{frame}{Case Study: S\&P 500 Index}
    \vspace{-0.3cm}
    \begin{center}
        \includegraphics[width=0.85\textwidth, height=0.55\textheight, keepaspectratio]{charts/sp500_prices_returns.pdf}
    \end{center}
    \vspace{-0.2cm}
    {\footnotesize
    \begin{itemize}
        \item \textbf{Top}: S\&P 500 price level -- clear upward trend (non-stationary)
        \item \textbf{Bottom}: Daily returns $r_t = \log(P_t/P_{t-1})$ -- stationary
        \item Returns fluctuate around zero mean with no trend
        \item Volatility clustering visible -- periods of high/low volatility
    \end{itemize}
    }
\end{frame}

\begin{frame}{Time Series Decomposition: Real Example}
    \vspace{-0.3cm}
    \begin{center}
        \includegraphics[width=0.85\textwidth, height=0.55\textheight, keepaspectratio]{charts/decomposition.pdf}
    \end{center}
    \vspace{-0.2cm}
    {\footnotesize
    \begin{itemize}
        \item \textbf{Trend}: Long-term direction of the series
        \item \textbf{Seasonal}: Regular periodic patterns
        \item \textbf{Residual}: What remains after removing trend and seasonality
        \item Decomposition helps understand data structure before modeling
    \end{itemize}
    }
\end{frame}

\begin{frame}{Stationarity Testing: ADF Results}
    \vspace{-0.3cm}
    \begin{center}
        \includegraphics[width=0.85\textwidth, height=0.55\textheight, keepaspectratio]{charts/adf_test_visualization.pdf}
    \end{center}
    \vspace{-0.2cm}
    {\footnotesize
    \begin{itemize}
        \item ADF test compares test statistic to critical values
        \item If test statistic $<$ critical value $\Rightarrow$ reject unit root (series is stationary)
        \item Prices: ADF statistic $> -2.86$ $\Rightarrow$ non-stationary
        \item Returns: ADF statistic $< -2.86$ $\Rightarrow$ stationary
    \end{itemize}
    }
\end{frame}

\begin{frame}{Stationarity Comparison: Prices vs Returns}
    {\small
    \begin{block}{ADF Test Results}
        \begin{center}
        \begin{tabular}{lccc}
            \toprule
            \textbf{Series} & \textbf{ADF Statistic} & \textbf{p-value} & \textbf{Conclusion} \\
            \midrule
            S\&P 500 Prices & $-0.82$ & $0.812$ & Non-stationary \\
            S\&P 500 Returns & $-45.3$ & $<0.001$ & Stationary \\
            \bottomrule
        \end{tabular}
        \end{center}
    \end{block}

    \vspace{0.3cm}

    \begin{exampleblock}{Key Insight}
        Financial prices are typically $I(1)$ -- integrated of order 1.

        Taking first differences (returns) achieves stationarity.

        This is why we model \textbf{returns}, not prices!
    \end{exampleblock}
    }
\end{frame}

\begin{frame}{Exponential Smoothing Forecast}
    \vspace{-0.3cm}
    \begin{center}
        \includegraphics[width=0.85\textwidth, height=0.55\textheight, keepaspectratio]{charts/holt_winters.pdf}
    \end{center}
    \vspace{-0.2cm}
    {\footnotesize
    \begin{itemize}
        \item Holt-Winters method for data with trend and seasonality
        \item Smoothing parameters $\alpha$, $\beta$, $\gamma$ control adaptiveness
        \item Forecasts capture both trend continuation and seasonal pattern
        \item Simple yet effective for many business applications
    \end{itemize}
    }
\end{frame}

%=============================================================================
% PART 7: DISCUSSION QUESTIONS
%=============================================================================
\section{Discussion Questions}

\begin{frame}{Discussion Question 1}
    \begin{block}{Scenario}
        You are analyzing monthly sales data for a retail company. The data shows clear seasonality (high sales in December) and an upward trend. The seasonal peaks have been getting larger over time.
    \end{block}

    \vspace{0.4cm}

    \textbf{Discuss:}
    \begin{enumerate}
        \item Should you use additive or multiplicative decomposition? Why?
        \item Which exponential smoothing method would you recommend?
        \item How would you evaluate your forecast model?
        \item What could go wrong if you used the wrong decomposition?
    \end{enumerate}
\end{frame}

\begin{frame}{Discussion Question 2}
    \begin{block}{Scenario}
        A colleague claims: "I ran the ADF test on my stock price data and got a p-value of 0.65, so my data is stationary and I can fit an ARMA model directly."
    \end{block}

    \vspace{0.4cm}

    \textbf{Discuss:}
    \begin{enumerate}
        \item What is wrong with this interpretation?
        \item What do the ADF hypotheses actually test?
        \item What should the colleague do before fitting an ARMA model?
        \item How could the KPSS test help clarify the situation?
    \end{enumerate}
\end{frame}

\begin{frame}{Discussion Question 3}
    \begin{block}{Scenario}
        You're building a forecasting model and achieve excellent results: MAPE of 2\% on your dataset. Your manager is impressed and wants to deploy the model immediately.
    \end{block}

    \vspace{0.4cm}

    \textbf{Discuss:}
    \begin{enumerate}
        \item What questions should you ask before deployment?
        \item Did you use proper train/validation/test splits?
        \item Could there be data leakage in your evaluation?
        \item What additional diagnostics would you run?
        \item How would you monitor the model in production?
    \end{enumerate}
\end{frame}

\begin{frame}{Discussion Question 4}
    \begin{block}{Scenario}
        You need to forecast daily electricity demand for the next week. The data shows: (1) strong daily patterns (peaks at 6pm), (2) weekly patterns (lower on weekends), and (3) annual patterns (higher in summer/winter).
    \end{block}

    \vspace{0.4cm}

    \textbf{Discuss:}
    \begin{enumerate}
        \item How would you handle multiple seasonal patterns?
        \item Would Holt-Winters work here? Why or why not?
        \item What's the advantage of Fourier terms in this case?
        \item How would you set up your train/validation/test split?
    \end{enumerate}
\end{frame}

%=============================================================================
% SUMMARY
%=============================================================================
\section{Summary}

\begin{frame}{Key Takeaways from Today}
    \begin{enumerate}
        \item \textbf{Time series are dependent} -- not i.i.d. like cross-sectional data

        \vspace{0.15cm}

        \item \textbf{Choose decomposition wisely} -- multiplicative when seasonal amplitude grows

        \vspace{0.15cm}

        \item \textbf{Understand smoothing parameters} -- high $\alpha$ = reactive, low $\alpha$ = smooth

        \vspace{0.15cm}

        \item \textbf{Test for stationarity} -- use both ADF and KPSS together

        \vspace{0.15cm}

        \item \textbf{Proper evaluation} -- never tune on test set!

        \vspace{0.15cm}

        \item \textbf{Random walk is non-stationary} -- variance grows with time
    \end{enumerate}

    \vspace{0.3cm}

    \begin{block}{Next Seminar}
        ARMA/ARIMA model identification, estimation, and forecasting
    \end{block}
\end{frame}

\begin{frame}{}
    \centering
    \Huge\textcolor{MainBlue}{Questions?}

    \vspace{1cm}

    \Large Good luck with the exercises!

    \vspace{1cm}

    \normalsize
    Practice makes perfect.
\end{frame}

\end{document}
