{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/danpele/Time-Series-Analysis/blob/main/chapter8_lecture_notebook.ipynb)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-1",
   "metadata": {},
   "source": [
    "# Chapter 8: Modern Extensions - ARFIMA, Random Forest, LSTM\n",
    "\n",
    "**Course:** Time Series Analysis and Forecasting  \n",
    "**Program:** Bachelor program, Faculty of Cybernetics, Statistics and Economic Informatics, Bucharest University of Economic Studies, Romania  \n",
    "**Academic Year:** 2025-2026\n",
    "\n",
    "---\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will be able to:\n",
    "1. Understand the concept of long memory in time series\n",
    "2. Estimate and interpret ARFIMA models\n",
    "3. Apply Random Forest for time series forecasting\n",
    "4. Build LSTM networks for sequential data\n",
    "5. Compare performance across different methods\n",
    "6. Choose the appropriate method based on data characteristics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-2",
   "metadata": {},
   "source": [
    "## Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages (uncomment if needed in Colab)\n",
    "# !pip install arch yfinance tensorflow scikit-learn statsmodels -q\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Statistical models\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from statsmodels.tsa.stattools import adfuller, acf, pacf\n",
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "\n",
    "# Machine Learning\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "\n",
    "# For fetching real data\n",
    "try:\n",
    "    import yfinance as yf\n",
    "    HAS_YF = True\n",
    "except ImportError:\n",
    "    HAS_YF = False\n",
    "    print(\"yfinance not installed. Install with: pip install yfinance\")\n",
    "\n",
    "# Deep Learning (optional)\n",
    "try:\n",
    "    import tensorflow as tf\n",
    "    from tensorflow.keras.models import Sequential\n",
    "    from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "    HAS_TF = True\n",
    "except ImportError:\n",
    "    HAS_TF = False\n",
    "    print(\"TensorFlow not installed. Install with: pip install tensorflow\")\n",
    "\n",
    "# Plotting style\n",
    "plt.rcParams['figure.figsize'] = (12, 5)\n",
    "plt.rcParams['font.size'] = 11\n",
    "plt.rcParams['axes.facecolor'] = 'none'\n",
    "plt.rcParams['figure.facecolor'] = 'none'\n",
    "plt.rcParams['axes.grid'] = False\n",
    "plt.rcParams['axes.spines.top'] = False\n",
    "plt.rcParams['axes.spines.right'] = False\n",
    "plt.rcParams['legend.frameon'] = False\n",
    "\n",
    "COLORS = {'blue': '#1A3A6E', 'red': '#DC3545', 'green': '#2E7D32', 'orange': '#E67E22', 'gray': '#666666'}\n",
    "\n",
    "print(\"Setup complete!\")\n",
    "print(f\"TensorFlow available: {HAS_TF}\")\n",
    "print(f\"yfinance available: {HAS_YF}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-4",
   "metadata": {},
   "source": [
    "## 1. Long Memory and ARFIMA\n",
    "\n",
    "### Short Memory vs Long Memory\n",
    "\n",
    "- **Short Memory (ARMA)**: Autocorrelations decay exponentially\n",
    "- **Long Memory (ARFIMA)**: Autocorrelations decay hyperbolically (slowly)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-5",
   "metadata": {},
   "outputs": [],
   "source": "# Demonstrate short vs long memory\nnp.random.seed(42)\nn = 1000\n\n# Short memory: White noise (H ≈ 0.5)\nwhite_noise = np.random.randn(n)\n\n# Short memory: AR(1) with moderate phi (for comparison)\nphi = 0.3\nar1_process = np.zeros(n)\nfor t in range(1, n):\n    ar1_process[t] = phi * ar1_process[t-1] + np.random.randn()\n\n# Long memory simulation (fractional Gaussian noise approximation)\nd = 0.4  # Long memory parameter (0 < d < 0.5)\n\ndef generate_long_memory(n, d):\n    \"\"\"Generate long memory series using Hosking's method\"\"\"\n    # Generate fractional Gaussian noise\n    gamma = np.zeros(n)\n    gamma[0] = 1.0\n    for k in range(1, n):\n        gamma[k] = gamma[k-1] * (k - 1 + d) / (k - d)\n    \n    # Build covariance matrix and generate\n    # Use Cholesky decomposition for proper correlation structure\n    cov = np.zeros((n, n))\n    for i in range(n):\n        for j in range(n):\n            k = abs(i - j)\n            if k < n:\n                cov[i, j] = gamma[k]\n    \n    # Add small regularization for numerical stability\n    cov += np.eye(n) * 1e-10\n    \n    try:\n        L = np.linalg.cholesky(cov)\n        z = np.random.randn(n)\n        long_memory = L @ z\n    except:\n        # Fallback: use spectral method\n        freqs = np.fft.fftfreq(n)\n        freqs[0] = 1e-10\n        spectrum = np.abs(freqs) ** (-2 * d)\n        spectrum[0] = 0\n        phases = np.random.uniform(0, 2*np.pi, n)\n        fft_vals = np.sqrt(spectrum) * np.exp(1j * phases)\n        long_memory = np.real(np.fft.ifft(fft_vals))\n    \n    return long_memory\n\nlong_memory = generate_long_memory(n, d)\n\n# Plot comparison\nfig, axes = plt.subplots(2, 2, figsize=(14, 8))\n\n# Time series\naxes[0, 0].plot(white_noise, color=COLORS['blue'], linewidth=0.8)\naxes[0, 0].set_title('Short Memory: White Noise', fontweight='bold')\naxes[0, 0].set_xlabel('Time')\n\naxes[0, 1].plot(long_memory, color=COLORS['orange'], linewidth=0.8)\naxes[0, 1].set_title(f'Long Memory: Fractional Noise (d={d})', fontweight='bold')\naxes[0, 1].set_xlabel('Time')\n\n# ACF comparison\nacf_short = acf(white_noise, nlags=50)\nacf_long = acf(long_memory, nlags=50)\n\naxes[1, 0].bar(range(51), acf_short, color=COLORS['blue'], alpha=0.7, label='ACF values')\naxes[1, 0].axhline(y=0, color='black', linestyle='-')\naxes[1, 0].axhline(y=1.96/np.sqrt(n), color='red', linestyle='--', alpha=0.5, label='95% confidence')\naxes[1, 0].axhline(y=-1.96/np.sqrt(n), color='red', linestyle='--', alpha=0.5)\naxes[1, 0].set_title('ACF: Short Memory (fast decay)', fontweight='bold')\naxes[1, 0].set_xlabel('Lag')\naxes[1, 0].set_ylim(-0.2, 1.0)\naxes[1, 0].legend(loc='upper center', bbox_to_anchor=(0.5, -0.15), ncol=2, frameon=False)\n\naxes[1, 1].bar(range(51), acf_long, color=COLORS['orange'], alpha=0.7, label='ACF values')\naxes[1, 1].axhline(y=0, color='black', linestyle='-')\naxes[1, 1].axhline(y=1.96/np.sqrt(n), color='red', linestyle='--', alpha=0.5, label='95% confidence')\naxes[1, 1].axhline(y=-1.96/np.sqrt(n), color='red', linestyle='--', alpha=0.5)\naxes[1, 1].set_title('ACF: Long Memory (slow/hyperbolic decay)', fontweight='bold')\naxes[1, 1].set_xlabel('Lag')\naxes[1, 1].set_ylim(-0.2, 1.0)\naxes[1, 1].legend(loc='upper center', bbox_to_anchor=(0.5, -0.15), ncol=2, frameon=False)\n\nplt.tight_layout()\nplt.subplots_adjust(bottom=0.12)\nplt.show()\n\nprint(\"Key difference:\")\nprint(\"- Short memory (white noise): ACF drops to zero immediately\")\nprint(\"- Long memory: ACF remains significant at high lags (slow decay)\")"
  },
  {
   "cell_type": "markdown",
   "id": "cell-6",
   "metadata": {},
   "source": [
    "### Estimating the Long Memory Parameter d\n",
    "\n",
    "The **Hurst exponent** H and fractional differencing parameter d are related:\n",
    "$$d = H - 0.5$$\n",
    "\n",
    "- H = 0.5: Random walk (no memory)\n",
    "- H > 0.5: Persistent (trending)\n",
    "- H < 0.5: Anti-persistent (mean-reverting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-7",
   "metadata": {},
   "outputs": [],
   "source": "# R/S Analysis to estimate Hurst exponent\ndef hurst_rs(ts, min_window=10):\n    \"\"\"\n    Estimate Hurst exponent using Rescaled Range (R/S) analysis.\n    \n    The idea: For a series with Hurst exponent H:\n        R/S ~ n^H\n    \n    So in log-log space: log(R/S) = H * log(n) + c\n    The slope H tells us about memory.\n    \"\"\"\n    n = len(ts)\n    max_k = int(np.log2(n))\n    \n    rs_list = []\n    n_list = []\n    \n    for k in range(4, max_k):\n        size = 2**k\n        if size > n:\n            break\n        \n        rs_values = []\n        for start in range(0, n - size + 1, size):\n            segment = ts[start:start + size]\n            mean_seg = np.mean(segment)\n            # Cumulative deviation from mean\n            cumdev = np.cumsum(segment - mean_seg)\n            # Range\n            R = np.max(cumdev) - np.min(cumdev)\n            # Standard deviation\n            S = np.std(segment, ddof=1)\n            if S > 0:\n                rs_values.append(R / S)\n        \n        if rs_values:\n            rs_list.append(np.mean(rs_values))\n            n_list.append(size)\n    \n    # Linear regression in log-log space gives H as slope\n    log_n = np.log(n_list)\n    log_rs = np.log(rs_list)\n    H, c = np.polyfit(log_n, log_rs, 1)\n    \n    return H, n_list, rs_list\n\n# Estimate Hurst for both series\nH_short, n_short, rs_short = hurst_rs(white_noise)\nH_long, n_long, rs_long = hurst_rs(long_memory)\n\nprint(\"Hurst Exponent Estimation (R/S Analysis)\")\nprint(\"=\"*50)\nprint(f\"White Noise (short memory): H = {H_short:.3f}\")\nprint(f\"Fractional Noise (long memory): H = {H_long:.3f}\")\nprint(f\"\\nInterpretation:\")\nprint(f\"  H < 0.5: Anti-persistent (mean-reverting)\")\nprint(f\"  H = 0.5: Random walk (no memory)\")\nprint(f\"  H > 0.5: Persistent (trending/long memory)\")\n\n# Plot R/S analysis - clearer visualization\nfig, axes = plt.subplots(1, 2, figsize=(14, 5))\n\n# Left: Show the R/S values and fitted lines\nax = axes[0]\nax.scatter(n_short, rs_short, color=COLORS['blue'], s=80, zorder=3, label='White Noise data points')\nax.scatter(n_long, rs_long, color=COLORS['orange'], s=80, zorder=3, label='Long Memory data points')\n\n# Fitted lines\nn_fit = np.array([min(n_short), max(n_short)])\nax.plot(n_fit, np.exp(np.log(n_fit) * H_short) * 0.8, '--', color=COLORS['blue'], \n        linewidth=2, label=f'White Noise fit (H={H_short:.2f})')\nax.plot(n_fit, np.exp(np.log(n_fit) * H_long) * 0.5, '--', color=COLORS['orange'], \n        linewidth=2, label=f'Long Memory fit (H={H_long:.2f})')\n\nax.set_xscale('log')\nax.set_yscale('log')\nax.set_xlabel('Window Size (n)', fontsize=11)\nax.set_ylabel('R/S Statistic', fontsize=11)\nax.set_title('R/S Analysis: Slope = Hurst Exponent', fontweight='bold')\nax.legend(loc='upper center', bbox_to_anchor=(0.5, -0.15), ncol=2, frameon=False)\n\n# Right: Show interpretation\nax2 = axes[1]\nh_values = [0.3, 0.5, 0.7, 0.9]\ncolors_h = [COLORS['green'], COLORS['gray'], COLORS['orange'], COLORS['red']]\nlabels_h = ['H=0.3 (mean-reverting)', 'H=0.5 (random walk)', 'H=0.7 (persistent)', 'H=0.9 (strong persistence)']\n\nn_demo = np.linspace(16, 512, 100)\nfor h, c, lab in zip(h_values, colors_h, labels_h):\n    ax2.plot(n_demo, n_demo**h, color=c, linewidth=2, label=lab)\n\n# Mark our estimated values\nax2.axhline(y=100, color='black', linestyle=':', alpha=0.3)\nax2.text(20, 120, f'Our estimates:\\nWhite Noise H={H_short:.2f}\\nLong Memory H={H_long:.2f}', \n         fontsize=10, bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))\n\nax2.set_xscale('log')\nax2.set_yscale('log')\nax2.set_xlabel('Window Size (n)', fontsize=11)\nax2.set_ylabel('R/S Statistic', fontsize=11)\nax2.set_title('Interpretation: Steeper Slope = More Persistence', fontweight='bold')\nax2.legend(loc='upper center', bbox_to_anchor=(0.5, -0.15), ncol=2, frameon=False)\n\nplt.tight_layout()\nplt.subplots_adjust(bottom=0.25)\nplt.show()\n\nprint(\"\\nKey insight: The SLOPE in log-log space equals H.\")\nprint(\"Steeper slope = higher H = more persistent/trending behavior.\")"
  },
  {
   "cell_type": "markdown",
   "id": "cell-8",
   "metadata": {},
   "source": [
    "## 2. Real Data: Bitcoin Volatility (Long Memory)\n",
    "\n",
    "Financial volatility is known to exhibit long memory properties."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-9",
   "metadata": {},
   "outputs": [],
   "source": "# Fetch Bitcoin data\nif HAS_YF:\n    btc = yf.download('BTC-USD', start='2020-01-01', end='2024-12-31', progress=False)\n    # Handle MultiIndex columns from newer yfinance versions\n    if isinstance(btc.columns, pd.MultiIndex):\n        btc.columns = btc.columns.droplevel(1)\n    # Ensure we get a Series (not DataFrame) and it has proper index\n    close_prices = btc['Close'].squeeze()\n    btc_returns = close_prices.pct_change().dropna() * 100\n    btc_volatility = btc_returns.abs()  # Absolute returns as volatility proxy\n    DATA_SOURCE = \"Yahoo Finance\"\nelse:\n    # Simulated data\n    np.random.seed(123)\n    n = 1000\n    btc_returns = pd.Series(np.random.randn(n) * 3, \n                           index=pd.date_range('2020-01-01', periods=n, freq='D'))\n    btc_volatility = btc_returns.abs()\n    DATA_SOURCE = \"Simulated\"\n\n# Verify btc_returns is a Series with index\nprint(f\"btc_returns type: {type(btc_returns)}\")\nprint(f\"btc_returns has index: {hasattr(btc_returns, 'index') and len(btc_returns.index) > 0}\")\n\nprint(f\"\\nBitcoin Data ({DATA_SOURCE})\")\nprint(\"=\"*50)\nprint(f\"Period: {btc_returns.index[0].date()} to {btc_returns.index[-1].date()}\")\nprint(f\"Observations: {len(btc_returns)}\")\nprint(f\"\\nReturns statistics:\")\nprint(btc_returns.describe().round(2))"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-10",
   "metadata": {},
   "outputs": [],
   "source": "# Test for long memory in Bitcoin volatility\nH_btc, n_btc, rs_btc = hurst_rs(btc_volatility.values)\n\nprint(\"Long Memory Test: Bitcoin Absolute Returns\")\nprint(\"=\"*50)\nprint(f\"Hurst Exponent: H = {H_btc:.3f}\")\nprint(f\"Implied d: d = {H_btc - 0.5:.3f}\")\nprint(f\"\\nInterpretation:\")\nif H_btc > 0.5:\n    print(f\"  H = {H_btc:.2f} > 0.5 → Long memory/Persistence\")\n    print(\"  Volatility clusters tend to persist over time\")\nelse:\n    print(f\"  H = {H_btc:.2f} ≈ 0.5 → No significant long memory\")\n\n# Plot ACF of absolute returns\nfig, axes = plt.subplots(1, 2, figsize=(14, 5))\n\naxes[0].plot(btc_volatility.index, btc_volatility, color=COLORS['blue'], linewidth=0.5, alpha=0.7, label='|Returns|')\naxes[0].set_title(f'Bitcoin Absolute Returns ({DATA_SOURCE})', fontweight='bold')\naxes[0].set_xlabel('Date')\naxes[0].set_ylabel('|Return| (%)')\naxes[0].legend(loc='upper center', bbox_to_anchor=(0.5, -0.12), ncol=1, frameon=False)\n\n# ACF\nacf_vol = acf(btc_volatility, nlags=100)\naxes[1].bar(range(101), acf_vol, color=COLORS['orange'], alpha=0.7, label='ACF of |Returns|')\naxes[1].axhline(y=0, color='black', linestyle='-')\naxes[1].axhline(y=1.96/np.sqrt(len(btc_volatility)), color='red', linestyle='--', alpha=0.5, label='95% confidence')\naxes[1].axhline(y=-1.96/np.sqrt(len(btc_volatility)), color='red', linestyle='--', alpha=0.5)\naxes[1].set_title('ACF of Absolute Returns (slow decay = long memory)', fontweight='bold')\naxes[1].set_xlabel('Lag')\naxes[1].legend(loc='upper center', bbox_to_anchor=(0.5, -0.12), ncol=2, frameon=False)\n\nplt.tight_layout()\nplt.subplots_adjust(bottom=0.18)\nplt.show()"
  },
  {
   "cell_type": "markdown",
   "id": "cell-11",
   "metadata": {},
   "source": [
    "## 3. Random Forest for Time Series\n",
    "\n",
    "### Feature Engineering\n",
    "\n",
    "Key to using ML for time series: create meaningful features from lagged values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-12",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_features(df, target_col, lags=5, rolling_windows=[5, 10, 20]):\n",
    "    \"\"\"\n",
    "    Create features for time series forecasting.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : DataFrame with datetime index\n",
    "    target_col : name of target column\n",
    "    lags : number of lag features\n",
    "    rolling_windows : list of windows for rolling statistics\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    DataFrame with features\n",
    "    \"\"\"\n",
    "    data = df[[target_col]].copy()\n",
    "    \n",
    "    # Lag features\n",
    "    for i in range(1, lags + 1):\n",
    "        data[f'lag_{i}'] = data[target_col].shift(i)\n",
    "    \n",
    "    # Rolling statistics (calculated on past data only!)\n",
    "    for w in rolling_windows:\n",
    "        data[f'rolling_mean_{w}'] = data[target_col].shift(1).rolling(window=w).mean()\n",
    "        data[f'rolling_std_{w}'] = data[target_col].shift(1).rolling(window=w).std()\n",
    "    \n",
    "    # Calendar features\n",
    "    if hasattr(data.index, 'dayofweek'):\n",
    "        data['dayofweek'] = data.index.dayofweek\n",
    "        data['month'] = data.index.month\n",
    "        data['dayofyear'] = data.index.dayofyear\n",
    "    \n",
    "    return data.dropna()\n",
    "\n",
    "# Prepare Bitcoin data for Random Forest\n",
    "btc_df = pd.DataFrame({'returns': btc_returns})\n",
    "btc_features = create_features(btc_df, 'returns', lags=10, rolling_windows=[5, 10, 20])\n",
    "\n",
    "print(\"Feature Engineering for Random Forest\")\n",
    "print(\"=\"*50)\n",
    "print(f\"\\nFeatures created: {btc_features.columns.tolist()}\")\n",
    "print(f\"\\nDataset shape: {btc_features.shape}\")\n",
    "print(f\"\\nFirst few rows:\")\n",
    "print(btc_features.head().round(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train/Test split (TEMPORAL - not random!)\n",
    "train_size = int(len(btc_features) * 0.8)\n",
    "train_data = btc_features.iloc[:train_size]\n",
    "test_data = btc_features.iloc[train_size:]\n",
    "\n",
    "# Separate features and target\n",
    "feature_cols = [c for c in btc_features.columns if c != 'returns']\n",
    "X_train = train_data[feature_cols]\n",
    "y_train = train_data['returns']\n",
    "X_test = test_data[feature_cols]\n",
    "y_test = test_data['returns']\n",
    "\n",
    "print(f\"Train set: {len(X_train)} samples\")\n",
    "print(f\"Test set: {len(X_test)} samples\")\n",
    "print(f\"\\nTrain period: {train_data.index[0].date()} to {train_data.index[-1].date()}\")\n",
    "print(f\"Test period: {test_data.index[0].date()} to {test_data.index[-1].date()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Random Forest\n",
    "rf_model = RandomForestRegressor(\n",
    "    n_estimators=100,\n",
    "    max_depth=10,\n",
    "    min_samples_split=5,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_pred_rf = rf_model.predict(X_test)\n",
    "\n",
    "# Metrics\n",
    "rmse_rf = np.sqrt(mean_squared_error(y_test, y_pred_rf))\n",
    "mae_rf = mean_absolute_error(y_test, y_pred_rf)\n",
    "\n",
    "print(\"Random Forest Results\")\n",
    "print(\"=\"*50)\n",
    "print(f\"RMSE: {rmse_rf:.4f}\")\n",
    "print(f\"MAE:  {mae_rf:.4f}\")\n",
    "\n",
    "# Naive benchmark (predict previous value)\n",
    "y_pred_naive = X_test['lag_1'].values\n",
    "rmse_naive = np.sqrt(mean_squared_error(y_test, y_pred_naive))\n",
    "mae_naive = mean_absolute_error(y_test, y_pred_naive)\n",
    "\n",
    "print(f\"\\nNaive Benchmark (lag_1):\")\n",
    "print(f\"RMSE: {rmse_naive:.4f}\")\n",
    "print(f\"MAE:  {mae_naive:.4f}\")\n",
    "\n",
    "print(f\"\\nRF improvement over naive:\")\n",
    "print(f\"RMSE: {(1 - rmse_rf/rmse_naive)*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-15",
   "metadata": {},
   "outputs": [],
   "source": "# Feature Importance\nfeature_importance = pd.DataFrame({\n    'feature': feature_cols,\n    'importance': rf_model.feature_importances_\n}).sort_values('importance', ascending=False)\n\n# Plot\nfig, axes = plt.subplots(1, 2, figsize=(14, 5))\n\n# Feature importance\ntop_features = feature_importance.head(10)\naxes[0].barh(top_features['feature'], top_features['importance'], color=COLORS['blue'], label='Feature importance')\naxes[0].set_xlabel('Importance')\naxes[0].set_title('Top 10 Feature Importances', fontweight='bold')\naxes[0].invert_yaxis()\naxes[0].legend(loc='upper center', bbox_to_anchor=(0.5, -0.12), ncol=1, frameon=False)\n\n# Actual vs Predicted\naxes[1].scatter(y_test, y_pred_rf, alpha=0.5, s=10, color=COLORS['blue'], label='Predictions')\naxes[1].plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', linewidth=2, label='Perfect prediction')\naxes[1].set_xlabel('Actual Returns')\naxes[1].set_ylabel('Predicted Returns')\naxes[1].set_title('Random Forest: Actual vs Predicted', fontweight='bold')\naxes[1].legend(loc='upper center', bbox_to_anchor=(0.5, -0.12), ncol=2, frameon=False)\n\nplt.tight_layout()\nplt.subplots_adjust(bottom=0.18)\nplt.show()\n\nprint(\"\\nTop 5 Most Important Features:\")\nprint(feature_importance.head().to_string(index=False))"
  },
  {
   "cell_type": "markdown",
   "id": "cell-16",
   "metadata": {},
   "source": [
    "## 4. LSTM for Time Series\n",
    "\n",
    "### Data Preparation for LSTM\n",
    "\n",
    "LSTM requires:\n",
    "1. **Scaled data** (usually MinMax or Standard scaling)\n",
    "2. **3D input shape**: (samples, timesteps, features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-17",
   "metadata": {},
   "outputs": [],
   "source": [
    "if HAS_TF:\n",
    "    def create_sequences(data, n_steps):\n",
    "        \"\"\"\n",
    "        Create sequences for LSTM.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        data : 1D array of values\n",
    "        n_steps : number of time steps in each sequence\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        X : sequences of shape (samples, n_steps, 1)\n",
    "        y : target values\n",
    "        \"\"\"\n",
    "        X, y = [], []\n",
    "        for i in range(len(data) - n_steps):\n",
    "            X.append(data[i:(i + n_steps)])\n",
    "            y.append(data[i + n_steps])\n",
    "        return np.array(X), np.array(y)\n",
    "    \n",
    "    # Prepare data\n",
    "    returns_values = btc_returns.values.reshape(-1, 1)\n",
    "    \n",
    "    # Scale data\n",
    "    scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "    scaled_data = scaler.fit_transform(returns_values)\n",
    "    \n",
    "    # Create sequences\n",
    "    n_steps = 20  # Use 20 days to predict next day\n",
    "    X_lstm, y_lstm = create_sequences(scaled_data.flatten(), n_steps)\n",
    "    \n",
    "    # Reshape for LSTM: (samples, timesteps, features)\n",
    "    X_lstm = X_lstm.reshape((X_lstm.shape[0], X_lstm.shape[1], 1))\n",
    "    \n",
    "    # Train/Test split (temporal)\n",
    "    train_size = int(len(X_lstm) * 0.8)\n",
    "    X_train_lstm = X_lstm[:train_size]\n",
    "    y_train_lstm = y_lstm[:train_size]\n",
    "    X_test_lstm = X_lstm[train_size:]\n",
    "    y_test_lstm = y_lstm[train_size:]\n",
    "    \n",
    "    print(\"LSTM Data Preparation\")\n",
    "    print(\"=\"*50)\n",
    "    print(f\"Sequence length: {n_steps} days\")\n",
    "    print(f\"X shape: {X_lstm.shape} (samples, timesteps, features)\")\n",
    "    print(f\"y shape: {y_lstm.shape}\")\n",
    "    print(f\"\\nTrain samples: {len(X_train_lstm)}\")\n",
    "    print(f\"Test samples: {len(X_test_lstm)}\")\n",
    "else:\n",
    "    print(\"TensorFlow not available. Skipping LSTM section.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-18",
   "metadata": {},
   "outputs": [],
   "source": [
    "if HAS_TF:\n",
    "    # Build LSTM model\n",
    "    model = Sequential([\n",
    "        LSTM(50, activation='relu', return_sequences=True, \n",
    "             input_shape=(n_steps, 1)),\n",
    "        Dropout(0.2),\n",
    "        LSTM(50, activation='relu'),\n",
    "        Dropout(0.2),\n",
    "        Dense(1)\n",
    "    ])\n",
    "    \n",
    "    model.compile(optimizer='adam', loss='mse')\n",
    "    \n",
    "    print(\"LSTM Model Architecture\")\n",
    "    print(\"=\"*50)\n",
    "    model.summary()\n",
    "    \n",
    "    # Train model\n",
    "    print(\"\\nTraining LSTM (this may take a while)...\")\n",
    "    history = model.fit(\n",
    "        X_train_lstm, y_train_lstm,\n",
    "        epochs=50,\n",
    "        batch_size=32,\n",
    "        validation_split=0.1,\n",
    "        verbose=0\n",
    "    )\n",
    "    print(\"Training complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-19",
   "metadata": {},
   "outputs": [],
   "source": [
    "if HAS_TF:\n",
    "    # Make predictions\n",
    "    y_pred_lstm_scaled = model.predict(X_test_lstm, verbose=0)\n",
    "    \n",
    "    # Inverse transform\n",
    "    y_pred_lstm = scaler.inverse_transform(y_pred_lstm_scaled.reshape(-1, 1)).flatten()\n",
    "    y_test_lstm_orig = scaler.inverse_transform(y_test_lstm.reshape(-1, 1)).flatten()\n",
    "    \n",
    "    # Metrics\n",
    "    rmse_lstm = np.sqrt(mean_squared_error(y_test_lstm_orig, y_pred_lstm))\n",
    "    mae_lstm = mean_absolute_error(y_test_lstm_orig, y_pred_lstm)\n",
    "    \n",
    "    print(\"LSTM Results\")\n",
    "    print(\"=\"*50)\n",
    "    print(f\"RMSE: {rmse_lstm:.4f}\")\n",
    "    print(f\"MAE:  {mae_lstm:.4f}\")\n",
    "    \n",
    "    # Compare with Random Forest\n",
    "    print(f\"\\nComparison:\")\n",
    "    print(f\"Random Forest RMSE: {rmse_rf:.4f}\")\n",
    "    print(f\"LSTM RMSE:          {rmse_lstm:.4f}\")\n",
    "    print(f\"Naive RMSE:         {rmse_naive:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-20",
   "metadata": {},
   "outputs": [],
   "source": "if HAS_TF:\n    # Plot results\n    fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n    \n    # Training history\n    axes[0, 0].plot(history.history['loss'], label='Train Loss', color=COLORS['blue'])\n    axes[0, 0].plot(history.history['val_loss'], label='Validation Loss', color=COLORS['orange'])\n    axes[0, 0].set_title('LSTM Training History', fontweight='bold')\n    axes[0, 0].set_xlabel('Epoch')\n    axes[0, 0].set_ylabel('MSE Loss')\n    axes[0, 0].legend(loc='upper center', bbox_to_anchor=(0.5, -0.12), ncol=2, frameon=False)\n    \n    # Actual vs Predicted (last 100 points)\n    n_plot = 100\n    axes[0, 1].plot(range(n_plot), y_test_lstm_orig[-n_plot:], \n                    color=COLORS['blue'], label='Actual', linewidth=1)\n    axes[0, 1].plot(range(n_plot), y_pred_lstm[-n_plot:], \n                    color=COLORS['red'], label='LSTM Prediction', linewidth=1, alpha=0.7)\n    axes[0, 1].set_title('LSTM: Actual vs Predicted (last 100 days)', fontweight='bold')\n    axes[0, 1].set_xlabel('Day')\n    axes[0, 1].set_ylabel('Return (%)')\n    axes[0, 1].legend(loc='upper center', bbox_to_anchor=(0.5, -0.12), ncol=2, frameon=False)\n    \n    # Scatter plot\n    axes[1, 0].scatter(y_test_lstm_orig, y_pred_lstm, alpha=0.5, s=10, color=COLORS['green'], label='Predictions')\n    axes[1, 0].plot([y_test_lstm_orig.min(), y_test_lstm_orig.max()], \n                    [y_test_lstm_orig.min(), y_test_lstm_orig.max()], 'r--', linewidth=2, label='Perfect prediction')\n    axes[1, 0].set_xlabel('Actual Returns')\n    axes[1, 0].set_ylabel('Predicted Returns')\n    axes[1, 0].set_title('LSTM: Actual vs Predicted', fontweight='bold')\n    axes[1, 0].legend(loc='upper center', bbox_to_anchor=(0.5, -0.12), ncol=2, frameon=False)\n    \n    # Model comparison\n    models = ['Naive', 'Random Forest', 'LSTM']\n    rmses = [rmse_naive, rmse_rf, rmse_lstm]\n    colors = [COLORS['gray'], COLORS['blue'], COLORS['green']]\n    bars = axes[1, 1].bar(models, rmses, color=colors)\n    axes[1, 1].set_ylabel('RMSE')\n    axes[1, 1].set_title('Model Comparison: RMSE (lower is better)', fontweight='bold')\n    for i, v in enumerate(rmses):\n        axes[1, 1].text(i, v + 0.05, f'{v:.2f}', ha='center')\n    \n    plt.tight_layout()\n    plt.subplots_adjust(bottom=0.08, hspace=0.35)\n    plt.show()"
  },
  {
   "cell_type": "markdown",
   "id": "cell-21",
   "metadata": {},
   "source": [
    "## 5. Time Series Cross-Validation\n",
    "\n",
    "**Important**: Never use standard k-fold CV for time series!\n",
    "\n",
    "Use **walk-forward validation** instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time Series Cross-Validation for Random Forest\n",
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "\n",
    "cv_scores = []\n",
    "fold_results = []\n",
    "\n",
    "for fold, (train_idx, test_idx) in enumerate(tscv.split(btc_features), 1):\n",
    "    X_train_cv = btc_features.iloc[train_idx][feature_cols]\n",
    "    y_train_cv = btc_features.iloc[train_idx]['returns']\n",
    "    X_test_cv = btc_features.iloc[test_idx][feature_cols]\n",
    "    y_test_cv = btc_features.iloc[test_idx]['returns']\n",
    "    \n",
    "    rf_cv = RandomForestRegressor(n_estimators=100, max_depth=10, random_state=42, n_jobs=-1)\n",
    "    rf_cv.fit(X_train_cv, y_train_cv)\n",
    "    y_pred_cv = rf_cv.predict(X_test_cv)\n",
    "    \n",
    "    rmse_cv = np.sqrt(mean_squared_error(y_test_cv, y_pred_cv))\n",
    "    cv_scores.append(rmse_cv)\n",
    "    fold_results.append({\n",
    "        'fold': fold,\n",
    "        'train_size': len(train_idx),\n",
    "        'test_size': len(test_idx),\n",
    "        'rmse': rmse_cv\n",
    "    })\n",
    "\n",
    "print(\"Time Series Cross-Validation Results\")\n",
    "print(\"=\"*50)\n",
    "for r in fold_results:\n",
    "    print(f\"Fold {r['fold']}: Train={r['train_size']:4d}, Test={r['test_size']:4d}, RMSE={r['rmse']:.4f}\")\n",
    "\n",
    "print(f\"\\nMean RMSE: {np.mean(cv_scores):.4f} (+/- {np.std(cv_scores):.4f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-23",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "1. **ARFIMA** extends ARIMA for long memory (fractional d)\n",
    "   - Useful when ACF decays slowly (hyperbolic)\n",
    "   - Common in financial volatility\n",
    "\n",
    "2. **Random Forest** for time series:\n",
    "   - Feature engineering is crucial (lags, rolling stats)\n",
    "   - Handles non-linear relationships\n",
    "   - Provides feature importance (interpretability)\n",
    "\n",
    "3. **LSTM** for sequential data:\n",
    "   - Captures complex patterns and long dependencies\n",
    "   - Requires lots of data and careful tuning\n",
    "   - \"Black box\" - limited interpretability\n",
    "\n",
    "4. **Always use temporal train/test split**\n",
    "   - Never shuffle time series data\n",
    "   - Use TimeSeriesSplit for cross-validation\n",
    "\n",
    "### When to Use What?\n",
    "\n",
    "| Situation | Recommendation |\n",
    "|-----------|----------------|\n",
    "| Small data, linear | ARIMA/ARFIMA |\n",
    "| Non-linear, interpretability needed | Random Forest |\n",
    "| Large data, complex patterns | LSTM |\n",
    "| Long memory in volatility | ARFIMA or GARCH |\n",
    "| Multiple seasonalities | Prophet or TBATS |"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}