% Chapter 6: VAR and Granger Causality
% Harvard-quality academic presentation
% Bachelor program, Bucharest University of Economic Studies

\documentclass[9pt, aspectratio=169, t]{beamer}
\input{preamble}
\subtitle{Chapter 6: VAR and Granger Causality}

\begin{document}

% Title page (no header/footer)
{
\setbeamertemplate{headline}{}
\setbeamertemplate{footline}{}
\begin{frame}
    \titlepage
\end{frame}
}

%=============================================================================
% LEARNING OBJECTIVES
%=============================================================================
\section{Motivation}

\begin{frame}{Learning Objectives}
    \begin{cminipage}{0.95\textwidth}
    \begin{block}{By the end of this chapter, you will be able to:}
        \begin{enumerate}\setlength{\itemsep}{0pt}
            \item Understand the \textbf{motivation} for multivariate time series analysis
            \item Specify and estimate \textbf{VAR(p)} models
            \item Apply \textbf{Granger causality} tests
            \item Interpret \textbf{Impulse Response Functions (IRFs)}
            \item Perform \textbf{Forecast Error Variance Decomposition (FEVD)}
            \item Select the optimal lag order using information criteria
            \item Implement VAR analysis in \textbf{Python}
        \end{enumerate}
    \end{block}
    \end{cminipage}
\end{frame}

%=============================================================================
% TABLE OF CONTENTS
%=============================================================================
\begin{frame}{Outline}
    \vspace{-0.15cm}
    {\small
    \begin{columns}[T]
        \begin{column}{0.48\textwidth}
            \begin{block}{Foundations}
                \begin{itemize}\setlength{\itemsep}{3pt}
                    \item Motivation
                    \item Introduction to multivariate time series
                    \item Vector Autoregression (VAR)
                    \item Granger Causality
                    \item Impulse Response Functions
                    \item Forecast Error Variance Decomposition
                \end{itemize}
            \end{block}
        \end{column}
        \begin{column}{0.48\textwidth}
            \begin{exampleblock}{Applications}
                \begin{itemize}\setlength{\itemsep}{3pt}
                    \item VAR Diagnostics
                    \item VAR Forecasting
                    \item Practical Example
                    \item \mbox{Case Study: GDP and Unemployment}
                    \item Summary and Quiz
                \end{itemize}
            \end{exampleblock}
        \end{column}
    \end{columns}
    }
\end{frame}

%=============================================================================
% MOTIVATION
%=============================================================================
\begin{frame}{Motivating Example: Macroeconomic Dynamics}
    \vspace{-0.2cm}
    {\footnotesize
    \begin{block}{Observations}
            \begin{itemize}\setlength{\itemsep}{0pt}
                \item Economic variables are \textbf{interconnected}: GDP affects unemployment, inflation affects interest rates
                \item Changes in one variable \textbf{propagate} through the system
                \item Understanding these dynamics requires \textbf{multivariate} analysis
            \end{itemize}
        \end{block}
    }
    \begin{center}
        \includegraphics[width=0.95\textwidth, height=0.50\textheight, keepaspectratio]{ch6_motivation_econ.pdf}
    \end{center}
    \quantlet{TSA\_ch6\_motivation\_econ}{https://github.com/QuantLet/TSA/tree/main/TSA_ch6/TSA_ch6_motivation_econ}
\end{frame}

\begin{frame}{The Key Insight: Variables Interact}
    \vspace{-0.2cm}
    {\footnotesize
    \begin{itemize}\setlength{\itemsep}{0pt}
            \item \textbf{Okun's Law}: Higher GDP growth $\Rightarrow$ lower unemployment
            \item \textbf{Taylor Rule}: Higher inflation $\Rightarrow$ higher interest rates
            \item \textbf{Phillips Curve}: Unemployment-inflation tradeoff
        \end{itemize}
    }
    \begin{center}
        \includegraphics[width=0.95\textwidth, height=0.50\textheight, keepaspectratio]{ch6_motivation_scatter.pdf}
    \end{center}
    \quantlet{TSA\_ch6\_motivation\_scatter}{https://github.com/QuantLet/TSA/tree/main/TSA_ch6/TSA_ch6_motivation_scatter}
\end{frame}

\begin{frame}{Lead-Lag Relationships}
    \vspace{-0.2cm}
    {\footnotesize
        \begin{itemize}\setlength{\itemsep}{0pt}
            \item Some variables \textbf{lead} others: stock market predicts economic activity
            \item Cross-correlation reveals the \textbf{timing} of relationships
            \item Peak correlation at lag 4: stock market leads unemployment by $\sim$4 months
        \end{itemize}
        }
    \begin{center}
        \includegraphics[width=0.95\textwidth, height=0.50\textheight, keepaspectratio]{ch6_motivation_leadlag.pdf}
    \end{center}
    \quantlet{TSA\_ch6\_motivation\_leadlag}{https://github.com/QuantLet/TSA/tree/main/TSA_ch6/TSA_ch6_motivation_leadlag}
\end{frame}

\begin{frame}{Why Univariate Models Are Not Enough}
    \vspace{-0.2cm}
    {\footnotesize
    {\small
        \begin{alertblock}{The Problem}
            ARIMA treats each variable \textbf{in isolation}; it ignores \textbf{interactions} and \textbf{feedback effects}
        \end{alertblock}
        }
        \vspace{-1mm}
        {\footnotesize
        \begin{exampleblock}{Examples}
            GDP--Unemployment, Interest Rate--Inflation, Stocks--Volume, Exchange Rate--Trade Balance
        \end{exampleblock}
        }
    }
    \begin{center}
        \includegraphics[width=0.95\textwidth, height=0.50\textheight, keepaspectratio]{ch6_motivation_univariate.pdf}
    \end{center}
    \quantlet{TSA\_ch6\_motivation\_univariate}{https://github.com/QuantLet/TSA/tree/main/TSA_ch6/TSA_ch6_motivation_univariate}
\end{frame}

\begin{frame}{What We'll Learn Today}
    \begin{cminipage}{0.95\textwidth}
    \vspace{-0.1cm}
    \begin{block}{Core Concepts}
        {\small
        \begin{enumerate}\setlength{\itemsep}{1pt}
            \item \textbf{VAR Models}: How to model multiple time series jointly
            \item \textbf{Granger Causality}: Does $X$ help predict $Y$?
            \item \textbf{Impulse Response Functions}: How do shocks propagate?
            \item \textbf{Variance Decomposition}: What drives each variable?
        \end{enumerate}
        }
    \end{block}
    \vspace{-0.05cm}
    {\small
    \begin{exampleblock}{Recurring Example: GDP Growth and Unemployment}
        \begin{itemize}\setlength{\itemsep}{0pt}
            \item $Y_{1t}$: \textbf{GDP Growth} \quad and \quad $Y_{2t}$: \textbf{Unemployment Rate} (\textit{Okun's Law})
            \item Central question: Does GDP help predict unemployment, or vice versa, or both?
        \end{itemize}
    \end{exampleblock}
    }
    \vspace{-0.05cm}
    {\small
    \begin{block}{Applications}
        \begin{itemize}\setlength{\itemsep}{0pt}
            \item Macroeconomic policy \quad\textbullet\quad Financial markets \quad\textbullet\quad Business cycle \quad\textbullet\quad Risk management
        \end{itemize}
    \end{block}
    }
    \end{cminipage}
\end{frame}

%=============================================================================
% SECTION 1: INTRODUCTION
%=============================================================================
\section{Introduction to Multivariate Time Series}

\begin{frame}{Multivariate Time Series Notation}
    \begin{cminipage}{0.95\textwidth}
    \begin{block}{Vector of Variables}
        Let $\bY_t = (Y_{1t}, Y_{2t}, \ldots, Y_{Kt})'$ be a $K \times 1$ vector of time series.

        \vspace{0.2cm}
        Example with $K=2$:
        $$\bY_t = \begin{pmatrix} Y_{1t} \\ Y_{2t} \end{pmatrix} = \begin{pmatrix} \text{GDP growth}_t \\ \text{Inflation}_t \end{pmatrix}$$
    \end{block}

    \vspace{0.15cm}

    \begin{block}{Key Questions}
        \begin{enumerate}\setlength{\itemsep}{0pt}
            \item Does $Y_1$ help predict $Y_2$? (Granger causality)
            \item How do shocks to $Y_1$ affect $Y_2$? (Impulse responses)
            \item What proportion of $Y_2$'s variance is due to $Y_1$? (Variance decomposition)
        \end{enumerate}
    \end{block}
    \end{cminipage}
\end{frame}

\begin{frame}{Multivariate Stationarity}
    \begin{cminipage}{0.95\textwidth}
    \begin{block}{Definition: Weak Stationarity}
        A $K$-dimensional time series $\bY_t$ is \textbf{weakly stationary} if:
        \begin{enumerate}\setlength{\itemsep}{0pt}
            \item $\E[\bY_t] = \boldsymbol{\mu}$ (constant mean vector)
            \item $\Cov(\bY_t, \bY_{t-h}) = \boldsymbol{\Gamma}(h)$ depends only on $h$, not $t$
        \end{enumerate}
    \end{block}

    \vspace{0.15cm}

    \begin{block}{Autocovariance Matrix}
        $$\boldsymbol{\Gamma}(h) = \E[(\bY_t - \boldsymbol{\mu})(\bY_{t-h} - \boldsymbol{\mu})'] = \begin{pmatrix} \gamma_{11}(h) & \gamma_{12}(h) \\ \gamma_{21}(h) & \gamma_{22}(h) \end{pmatrix}$$
        \vspace{-0.15cm}
        \begin{itemize}\setlength{\itemsep}{0pt}
            \item Note: $\boldsymbol{\Gamma}(-h) = \boldsymbol{\Gamma}(h)'$ (transpose, not equal!)
        \end{itemize}
    \end{block}
    \end{cminipage}
\end{frame}

\begin{frame}{Cross-Covariance Properties}
    \vspace{-2mm}
    {\small
    \begin{cminipage}{0.95\textwidth}
    \begin{block}{Cross-Covariance Function}
        For variables $Y_{it}$ and $Y_{jt}$:
        $$\gamma_{ij}(h) = \Cov(Y_{it}, Y_{j,t-h}) = \E[(Y_{it} - \mu_i)(Y_{j,t-h} - \mu_j)]$$
    \end{block}

    \begin{alertblock}{Key Difference from Univariate Case}
        \begin{itemize}\setlength{\itemsep}{0pt}
            \item In general: $\gamma_{ij}(h) \neq \gamma_{ij}(-h)$
            \item But: $\gamma_{ij}(h) = \gamma_{ji}(-h)$
            \item The cross-covariance matrix is \textbf{not symmetric} for $h \neq 0$
        \end{itemize}
    \end{alertblock}

    \begin{exampleblock}{Example}
        \begin{itemize}\setlength{\itemsep}{0pt}
            \item If $Y_1$ leads $Y_2$: $\gamma_{12}(h) > 0$ for $h > 0$ but $\gamma_{12}(h) \approx 0$ for $h < 0$
        \end{itemize}
    \end{exampleblock}
    \end{cminipage}
    }
\end{frame}

\begin{frame}{Correlation Matrix Function}
    \begin{cminipage}{0.95\textwidth}
    \vspace{-0.1cm}
    \begin{block}{Definition}
        \begin{itemize}\setlength{\itemsep}{0pt}
            \item The \textbf{autocorrelation matrix} at lag $h$:
                \vspace{-0.2cm}
                $$\mathbf{R}(h) = \mathbf{D}^{-1} \boldsymbol{\Gamma}(h) \mathbf{D}^{-1}$$
                \vspace{-0.15cm}
            \item $\mathbf{D} = \text{diag}(\sigma_1, \sigma_2, \ldots, \sigma_K)$ and $\sigma_i = \sqrt{\gamma_{ii}(0)}$
        \end{itemize}
    \end{block}

    \begin{exampleblock}{For Bivariate Case}
        \begin{itemize}\setlength{\itemsep}{0pt}
            \item \textbf{Matrix}: $\mathbf{R}(h) = \begin{pmatrix} \rho_{11}(h) & \rho_{12}(h) \\ \rho_{21}(h) & \rho_{22}(h) \end{pmatrix}$, where $\rho_{ij}(h) = \frac{\gamma_{ij}(h)}{\sigma_i \sigma_j}$
            \item \textbf{Interpretation}:
                \begin{itemize}\setlength{\itemsep}{0pt}
                    \item Diagonal elements: usual ACFs
                    \item Off-diagonal: cross-correlations
                \end{itemize}
        \end{itemize}
    \end{exampleblock}
    \end{cminipage}
\end{frame}

%=============================================================================
% SECTION 2: VAR MODELS
%=============================================================================
\section{Vector Autoregression (VAR)}

\begin{frame}{Researcher Spotlight: Sims \& Granger}
    \vspace{-0.2cm}
    \begin{columns}[T]
        \begin{column}{0.22\textwidth}
            \centering
            \includegraphics[width=0.95\textwidth, height=0.22\textheight, keepaspectratio]{photo_christopher_sims.jpg}
            \\[0.05cm]
            {\tiny\textcolor{MediumGray}{Christopher Sims (*1942)}}\\[-0.05cm]
            {\tiny\textcolor{MediumGray}{Nobel Prize 2011}}\\[0.02cm]
            \href{https://en.wikipedia.org/wiki/Christopher_A._Sims}{\faWikipediaW\ \textcolor{MainBlue}{\tiny Wikipedia}}
        \end{column}
        \begin{column}{0.76\textwidth}
            \begin{block}{Biography}
                {\footnotesize \begin{itemize}\setlength{\itemsep}{0pt}
                    \item \textbf{Christopher Sims}: American econometrician at Princeton. Nobel Prize (2011) ``for empirical research on cause and effect in the macroeconomy''
                    \item \textbf{Clive Granger}: British-American economist at UC San Diego. Nobel Prize (2003) ``for methods of analyzing economic time series with common trends (cointegration)''
                \end{itemize}}
            \end{block}
        \end{column}
    \end{columns}
    \vspace{0.1cm}
    \begin{columns}[T]
        \begin{column}{0.22\textwidth}
            \centering
            \includegraphics[width=0.95\textwidth, height=0.22\textheight, keepaspectratio]{photo_clive_granger.jpg}
            \\[0.05cm]
            {\tiny\textcolor{MediumGray}{Clive Granger (1934--2009)}}\\[-0.05cm]
            {\tiny\textcolor{MediumGray}{Nobel Prize 2003}}\\[0.02cm]
            \href{https://en.wikipedia.org/wiki/Clive_Granger}{\faWikipediaW\ \textcolor{MainBlue}{\tiny Wikipedia}}
        \end{column}
        \begin{column}{0.76\textwidth}
            \begin{exampleblock}{Key Contributions}
                {\footnotesize
                \begin{itemize}\setlength{\itemsep}{0pt}
                    \item \textbf{VAR models} (Sims, 1980) --- vector autoregression for macroeconomics
                    \item \textbf{Granger causality} (Granger, 1969) --- predictive causality concept
                    \item \textbf{Impulse response functions} and structural VAR identification
                    \item \textbf{Cointegration} (Granger, 1981) --- long-run equilibrium relationships
                \end{itemize}}
            \end{exampleblock}
        \end{column}
    \end{columns}
\end{frame}

\begin{frame}{The VAR(p) Model}
    \begin{cminipage}{0.95\textwidth}
    \begin{block}{Definition}
        A \textbf{VAR(p)} model for $K$ variables:
        $$\bY_t = \mathbf{c} + \bA_1 \bY_{t-1} + \bA_2 \bY_{t-2} + \cdots + \bA_p \bY_{t-p} + \bepsilon_t$$

        where:
        \begin{itemize}\setlength{\itemsep}{0pt}
            \item $\bY_t$: $K \times 1$ vector of endogenous variables
            \item $\mathbf{c}$: $K \times 1$ vector of constants
            \item $\bA_i$: $K \times K$ coefficient matrices
            \item $\bepsilon_t$: $K \times 1$ vector of error terms with $\E[\bepsilon_t] = \mathbf{0}$, $\E[\bepsilon_t \bepsilon_t'] = \bSigma$
        \end{itemize}
    \end{block}
    \end{cminipage}
\end{frame}

\begin{frame}{VAR(1) with Two Variables}
    \begin{cminipage}{0.95\textwidth}
    \begin{block}{Bivariate VAR(1)}
        $$\begin{pmatrix} Y_{1t} \\ Y_{2t} \end{pmatrix} = \begin{pmatrix} c_1 \\ c_2 \end{pmatrix} + \begin{pmatrix} a_{11} & a_{12} \\ a_{21} & a_{22} \end{pmatrix} \begin{pmatrix} Y_{1,t-1} \\ Y_{2,t-1} \end{pmatrix} + \begin{pmatrix} \varepsilon_{1t} \\ \varepsilon_{2t} \end{pmatrix}$$
    \end{block}

    \vspace{0.15cm}

    \begin{exampleblock}{Equation by Equation}
        \begin{align*}
            Y_{1t} &= c_1 + a_{11} Y_{1,t-1} + a_{12} Y_{2,t-1} + \varepsilon_{1t} \\
            Y_{2t} &= c_2 + a_{21} Y_{1,t-1} + a_{22} Y_{2,t-1} + \varepsilon_{2t}
        \end{align*}

        \textbf{Key insight}: Each equation includes lags of \textbf{all} variables!
    \end{exampleblock}
    \end{cminipage}
\end{frame}

\begin{frame}{Numerical Example: VAR(1)}
    \vspace{-2mm}
    {\small
    \begin{cminipage}{0.95\textwidth}
    \begin{exampleblock}{Specific VAR(1) Model}
        $$\begin{pmatrix} Y_{1t} \\ Y_{2t} \end{pmatrix} = \begin{pmatrix} 0.5 \\ 0.3 \end{pmatrix} + \begin{pmatrix} 0.7 & 0.2 \\ -0.1 & 0.6 \end{pmatrix} \begin{pmatrix} Y_{1,t-1} \\ Y_{2,t-1} \end{pmatrix} + \begin{pmatrix} \varepsilon_{1t} \\ \varepsilon_{2t} \end{pmatrix}$$
    \end{exampleblock}

    \begin{block}{Interpretation of Coefficients}
        \begin{itemize}\setlength{\itemsep}{0pt}
            \item $a_{11} = 0.7$: A 1-unit increase in $Y_1$ at $t-1$ increases $Y_1$ at $t$ by 0.7
            \item $a_{12} = 0.2$: A 1-unit increase in $Y_2$ at $t-1$ increases $Y_1$ at $t$ by 0.2
            \item $a_{21} = -0.1$: A 1-unit increase in $Y_1$ at $t-1$ \textbf{decreases} $Y_2$ at $t$ by 0.1
            \item $a_{22} = 0.6$:
                \begin{itemize}\setlength{\itemsep}{0pt}
                    \item A 1-unit increase in $Y_2$ at $t-1$ increases $Y_2$ at $t$ by 0.6
                \end{itemize}
            \end{itemize}
    \end{block}
    \end{cminipage}
    }
\end{frame}

\begin{frame}{VAR(2): Higher Order Dynamics}
    \begin{cminipage}{0.95\textwidth}
    \vspace{-0.15cm}
    {\small
    \begin{block}{VAR(2) Specification}
        $\bY_t = \mathbf{c} + \bA_1 \bY_{t-1} + \bA_2 \bY_{t-2} + \bepsilon_t$. For $K=2$: $K + pK^2 = 2 + 2 \times 4 = 10$ parameters.
    \end{block}

    \vspace{0.1cm}

    {\small
    \begin{exampleblock}{Written Out}
        \begin{align*}
            Y_{1t} &= c_1 + a_{11}^{(1)} Y_{1,t-1} + a_{12}^{(1)} Y_{2,t-1} + a_{11}^{(2)} Y_{1,t-2} + a_{12}^{(2)} Y_{2,t-2} + \varepsilon_{1t} \\
            Y_{2t} &= c_2 + a_{21}^{(1)} Y_{1,t-1} + a_{22}^{(1)} Y_{2,t-1} + a_{21}^{(2)} Y_{1,t-2} + a_{22}^{(2)} Y_{2,t-2} + \varepsilon_{2t}
        \end{align*}
    \end{exampleblock}
    }

    \vspace{0.1cm}

    {\footnotesize
    \begin{alertblock}{Curse of Dimensionality}
        VAR($p$) with $K$ variables has $K + pK^2$ parameters. With $K=5$, $p=4$: $5 + 4 \times 25 = 105$ parameters!
    \end{alertblock}
    }
    }
    \end{cminipage}
\end{frame}

\begin{frame}{VAR Process: GDP and Unemployment (FRED)}
    \vspace{-0.2cm}
    {\footnotesize
    \begin{itemize}\setlength{\itemsep}{0pt}
            \item \textbf{Data}: US GDP Growth (GDPC1) and Unemployment Rate (UNRATE) from FRED
            \item Each variable responds to both its own past and the other variable's past
            \item Classic example of macroeconomic interdependence (Okun's Law)
        \end{itemize}
    }
    \begin{center}
        \includegraphics[width=0.95\textwidth, height=0.50\textheight, keepaspectratio]{ch6_var_simulation.pdf}
    \end{center}
    \quantlet{TSA\_ch6\_var\_simulation}{https://github.com/QuantLet/TSA/tree/main/TSA_ch6/TSA_ch6_var_simulation}
\end{frame}

\begin{frame}{The Companion Form}
    \begin{cminipage}{0.95\textwidth}
    \vspace{-3mm}
    {\small
    \begin{block}{Converting VAR(p) to VAR(1)}
        Any VAR($p$) can be written as a VAR(1) in \textbf{companion form}:
        $\boldsymbol{\xi}_t = \mathbf{A} \boldsymbol{\xi}_{t-1} + \mathbf{v}_t$
    \end{block}

    \vspace{0.1cm}

    {\small
    \begin{exampleblock}{For VAR(2)}
        $$\underbrace{\begin{pmatrix} \bY_t \\ \bY_{t-1} \end{pmatrix}}_{\boldsymbol{\xi}_t} = \underbrace{\begin{pmatrix} \bA_1 & \bA_2 \\ \mathbf{I}_K & \mathbf{0} \end{pmatrix}}_{\mathbf{A}} \underbrace{\begin{pmatrix} \bY_{t-1} \\ \bY_{t-2} \end{pmatrix}}_{\boldsymbol{\xi}_{t-1}} + \underbrace{\begin{pmatrix} \bepsilon_t \\ \mathbf{0} \end{pmatrix}}_{\mathbf{v}_t}$$
    \end{exampleblock}
    }

    \vspace{0.1cm}

    {\footnotesize
    \begin{block}{Why Useful?}
        \begin{itemize}\setlength{\itemsep}{0pt}
            \item Stationarity, forecasting, and IRFs are easier in companion form
            \item Matrix $\mathbf{A}$ is $Kp \times Kp$
        \end{itemize}
    \end{block}
    }
    }
    \end{cminipage}
\end{frame}

\begin{frame}{Stationarity of VAR}
    \begin{cminipage}{0.95\textwidth}
    \begin{block}{Stability Condition}
        \begin{itemize}\setlength{\itemsep}{0pt}
            \item VAR(p) is \textbf{stable} (stationary) if all roots of:
                $$\det(\mathbf{I}_K - \bA_1 z - \bA_2 z^2 - \cdots - \bA_p z^p) = 0$$
                \vspace{-0.15cm}
            \item Lie \textbf{outside} the unit circle (i.e., $|z| > 1$)
        \end{itemize}
    \end{block}

    \vspace{0.15cm}

    \begin{alertblock}{For VAR(1)}
        \begin{itemize}\setlength{\itemsep}{2pt}
            \item The model is stable if all \textbf{eigenvalues} of $\bA_1$ are less than 1 in absolute value
            \item Example: For $\bA_1 = \begin{pmatrix} 0.5 & 0.1 \\ 0.2 & 0.3 \end{pmatrix}$, eigenvalues are $\lambda_1 = 0.6$ and $\lambda_2 = 0.2$
                \begin{itemize}\setlength{\itemsep}{0pt}
                    \item Both $< 1$ $\Rightarrow$ stable!
                \end{itemize}
        \end{itemize}
    \end{alertblock}
    \end{cminipage}
\end{frame}

\begin{frame}{Stability Condition: Numerical Example}
    \begin{cminipage}{0.95\textwidth}
    {\small
    \begin{block}{For $\bA = \begin{pmatrix} 0.7 & 0.2 \\ -0.1 & 0.6 \end{pmatrix}$}
        Characteristic polynomial: $\det(\bA - \lambda \mathbf{I}) = 0$
        \vspace{-0.2cm}
        $$\det\begin{pmatrix} 0.7 - \lambda & 0.2 \\ -0.1 & 0.6 - \lambda \end{pmatrix} = (0.7-\lambda)(0.6-\lambda) + 0.02 = 0 \implies \lambda^2 - 1.3\lambda + 0.44 = 0$$
    \end{block}

    \begin{exampleblock}{Solution}
        $\lambda = \frac{1.3 \pm \sqrt{1.69 - 1.76}}{2} = 0.65 \pm 0.132i$, \quad
        $|\lambda| = \sqrt{0.65^2 + 0.132^2} = \sqrt{0.44} = 0.663 < 1$ \quad $\checkmark$ Stable!
    \end{exampleblock}
    }
    \end{cminipage}
\end{frame}

\begin{frame}{Stability Condition: Visual Interpretation}
    \begin{center}
        \includegraphics[width=0.95\textwidth, height=0.78\textheight, keepaspectratio]{ch6_stability_roots.pdf}
    \end{center}
    \quantlet{TSA\_ch6\_stability\_roots}{https://github.com/QuantLet/TSA/tree/main/TSA_ch6/TSA_ch6_stability_roots}
\end{frame}

\begin{frame}{Mean of a Stationary VAR}
    \begin{cminipage}{0.95\textwidth}
    {\small
    \begin{block}{Unconditional Mean}
        \begin{itemize}\setlength{\itemsep}{2pt}
            \item For stationary VAR(1): $\bY_t = \mathbf{c} + \bA \bY_{t-1} + \bepsilon_t$
            \item Taking expectations: $\E[\bY_t] = \mathbf{c} + \bA \E[\bY_{t-1}]$
            \item Since $\E[\bY_t] = \E[\bY_{t-1}] = \boldsymbol{\mu}$ (stationarity):
                $$\boldsymbol{\mu} = \mathbf{c} + \bA \boldsymbol{\mu} \quad \Rightarrow \quad \boldsymbol{\mu} = (\mathbf{I}_K - \bA)^{-1} \mathbf{c}$$
                \vspace{-0.15cm}
        \end{itemize}
    \end{block}

    \begin{exampleblock}{Example}
        If $\mathbf{c} = \begin{pmatrix} 0.5 \\ 0.3 \end{pmatrix}$ and $\bA = \begin{pmatrix} 0.7 & 0.2 \\ -0.1 & 0.6 \end{pmatrix}$:
        $\boldsymbol{\mu} = \begin{pmatrix} 0.3 & -0.2 \\ 0.1 & 0.4 \end{pmatrix}^{-1} \begin{pmatrix} 0.5 \\ 0.3 \end{pmatrix} = \begin{pmatrix} 2.3 \\ 1.0 \end{pmatrix}$
    \end{exampleblock}
    }
    \end{cminipage}
\end{frame}

\begin{frame}{Covariance Structure of VAR(1)}
    \vspace{-2mm}
    {\small
    \begin{cminipage}{0.95\textwidth}
    \begin{block}{Variance-Covariance Matrix $\boldsymbol{\Gamma}(0)$}
        For VAR(1), the variance satisfies the \textbf{discrete Lyapunov equation}:
        $$\boldsymbol{\Gamma}(0) = \bA \boldsymbol{\Gamma}(0) \bA' + \bSigma$$
    \end{block}

    \begin{block}{Autocovariance at Lag $h$}
        $$\boldsymbol{\Gamma}(h) = \bA^h \boldsymbol{\Gamma}(0), \quad h \geq 0$$

        This shows that autocovariances decay geometrically with the eigenvalues of $\bA$.
    \end{block}

    \begin{alertblock}{Solving the Lyapunov Equation}
        Can solve by vectorization:
        $$\text{vec}(\boldsymbol{\Gamma}(0)) = (\mathbf{I}_{K^2} - \bA \otimes \bA)^{-1} \text{vec}(\bSigma)$$
        where $\otimes$ denotes the Kronecker product.
    \end{alertblock}
    \end{cminipage}
    }
\end{frame}

\begin{frame}{Estimation of VAR}
    \begin{cminipage}{0.95\textwidth}
    \vspace{-0.1cm}
    \begin{block}{OLS Estimation}
        \begin{itemize}\setlength{\itemsep}{0pt}
            \item Each equation can be estimated by \textbf{OLS separately}:
                \vspace{-0.15cm}
                $$\hat{\bA} = \left(\sum_{t=1}^{T} \bY_t \bY_{t-1}'\right) \left(\sum_{t=1}^{T} \bY_{t-1} \bY_{t-1}'\right)^{-1}$$
                \vspace{-0.2cm}
            \item Efficient because all equations have the \textbf{same regressors}
        \end{itemize}
    \end{block}

    \vspace{0.15cm}

    \begin{block}{Covariance Matrix}
        \begin{itemize}\setlength{\itemsep}{0pt}
            \item Estimator: $\hat{\bSigma} = \frac{1}{T-Kp-1} \sum_{t=1}^{T} \hat{\bepsilon}_t \hat{\bepsilon}_t'$
            \item The errors $\varepsilon_{1t}$ and $\varepsilon_{2t}$ may be \textbf{contemporaneously correlated}
        \end{itemize}
    \end{block}
    \end{cminipage}
\end{frame}

\begin{frame}{Lag Selection: Example}
    \vspace{-0.2cm}
    {\footnotesize
    \begin{block}{Observations}
            \begin{itemize}\setlength{\itemsep}{0pt}
                \item Real US data (FRED): GDP and Unemployment, $T = 140$ quarters
                \item Information criteria: AIC and BIC for lag $p = 1, \ldots, 10$ (may suggest different orders)
                \item Interpretation: lower values = better fit; both select $p = 2$
            \end{itemize}
        \end{block}
    }
    \begin{center}
        \includegraphics[width=0.95\textwidth, height=0.50\textheight, keepaspectratio]{ch6_lag_selection.pdf}
    \end{center}
    \quantlet{TSA\_ch6\_lag\_selection}{https://github.com/QuantLet/TSA/tree/main/TSA_ch6/TSA_ch6_lag_selection}
\end{frame}

\begin{frame}{Lag Order Selection}
    \vspace{-2mm}
    \begin{cminipage}{0.95\textwidth}
    {\small
    \begin{block}{Information Criteria}
        Choose $p$ that minimizes:
        \vspace{-0.2cm}
        \begin{align*}
            \text{AIC}(p) &= \ln|\hat{\bSigma}_p| + \frac{2pK^2}{T} \\[-0.05cm]
            \text{BIC}(p) &= \ln|\hat{\bSigma}_p| + \frac{pK^2 \ln T}{T} \\[-0.05cm]
            \text{HQ}(p) &= \ln|\hat{\bSigma}_p| + \frac{2pK^2 \ln\ln T}{T}
        \end{align*}
        \vspace{-0.2cm}
        {\scriptsize \textbf{where:} $\hat{\bSigma}_p$ = residual covariance matrix, $K$ = no.\ of variables, $p$ = no.\ of lags, $T$ = sample size}
    \end{block}
    }

    \vspace{0.15cm}

    \begin{block}{Guidelines}
        \begin{itemize}\setlength{\itemsep}{0pt}
            \item AIC: larger models (better forecasting); BIC: smaller models (consistent)
            \item Start with $p_{max}$ based on frequency (4 quarterly, 12 monthly)
        \end{itemize}
    \end{block}
    \end{cminipage}
\end{frame}

\begin{frame}{Restricted VAR Models}
    \vspace{-0.2cm}
    {\small
    \begin{cminipage}{0.95\textwidth}
    \begin{block}{Why Restrict?}
        Full VAR models can be \textbf{overparameterized}:
        \begin{itemize}\setlength{\itemsep}{0pt}
            \item Many coefficients may be insignificant
            \item Poor forecasting performance
            \item Loss of degrees of freedom
        \end{itemize}
    \end{block}

    \begin{block}{Common Restrictions}
        \begin{itemize}\setlength{\itemsep}{0pt}
            \item \textbf{Zero restrictions}: Set small coefficients to zero
            \item \textbf{Block exogeneity}: Some variables don't affect others
            \item \textbf{Lag exclusion}: Exclude certain lags
        \end{itemize}
    \end{block}

    \begin{alertblock}{Testing Restrictions}
        \begin{itemize}\setlength{\itemsep}{0pt}
            \item Use likelihood ratio test:
                $LR = T(\ln|\hat{\bSigma}_R| - \ln|\hat{\bSigma}_U|) \sim \chi^2_r$
            \item $r$ = number of restrictions
        \end{itemize}
    \end{alertblock}
    \end{cminipage}
    }
\end{frame}

%=============================================================================
% SECTION 3: GRANGER CAUSALITY
%=============================================================================
\section{Granger Causality}

\begin{frame}{What is Granger Causality?}
    \begin{cminipage}{0.95\textwidth}
    \begin{block}{Clive Granger (1969, Nobel Prize 2003)}
        ``$X$ \textbf{Granger-causes} $Y$'' if past values of $X$ help predict $Y$, \textbf{beyond} what past values of $Y$ alone can predict.
    \end{block}

    \vspace{0.15cm}

    \begin{alertblock}{Important Distinction}
        \textbf{Granger causality $\neq$ True causality}

        \begin{itemize}\setlength{\itemsep}{0pt}
            \item Granger causality is about \textbf{predictive content}
            \item Does NOT imply economic/structural causation
            \item ``$X$ Granger-causes $Y$'' means:
                \begin{itemize}\setlength{\itemsep}{0pt}
                    \item $X$ contains useful information for forecasting $Y$
                \end{itemize}
            \end{itemize}
    \end{alertblock}
    \end{cminipage}
\end{frame}

\begin{frame}{Formal Definition}
    \begin{cminipage}{0.95\textwidth}
    \begin{block}{Granger Causality}
        $X$ \textbf{does not Granger-cause} $Y$ if:
        $$\E[Y_t | Y_{t-1}, Y_{t-2}, \ldots, X_{t-1}, X_{t-2}, \ldots] = \E[Y_t | Y_{t-1}, Y_{t-2}, \ldots]$$

        In other words: adding $X$'s history does not improve the prediction of $Y$.
    \end{block}

    \vspace{0.15cm}

    \begin{exampleblock}{In the VAR Context}
        \begin{itemize}\setlength{\itemsep}{2pt}
            \item For VAR(1): $Y_{1t} = c_1 + a_{11} Y_{1,t-1} + a_{12} Y_{2,t-1} + \varepsilon_{1t}$
            \item $Y_2$ does \textbf{not} Granger-cause $Y_1$ if $a_{12} = 0$
            \item For VAR(p): $Y_2$ does not Granger-cause $Y_1$ if $a_{12}^{(1)} = a_{12}^{(2)} = \cdots = a_{12}^{(p)} = 0$
        \end{itemize}
    \end{exampleblock}
    \end{cminipage}
\end{frame}

\begin{frame}{Testing for Granger Causality}
    \begin{cminipage}{0.95\textwidth}
    \vspace{-0.1cm}
    \begin{block}{Hypothesis Test}
        \begin{itemize}\setlength{\itemsep}{2pt}
            \item \textbf{$H_0$}: $Y_2$ does \textbf{not} Granger-cause $Y_1$:
                $H_0: a_{12}^{(1)} = a_{12}^{(2)} = \cdots = a_{12}^{(p)} = 0$
            \item \textbf{$H_1$}: At least one $a_{12}^{(i)} \neq 0$ (Granger causality exists)
        \end{itemize}
    \end{block}

    \vspace{0.15cm}

    \begin{block}{Test Statistic: Wald Test}
        $$F = \frac{(RSS_R - RSS_U)/p}{RSS_U/(T-2p-1)} \sim F_{p, T-2p-1}$$

        where:
        \begin{itemize}\setlength{\itemsep}{0pt}
            \item $RSS_R$: Residual sum of squares from restricted model (without $Y_2$ lags)
            \item $RSS_U$: Residual sum of squares from unrestricted model (full VAR)
        \end{itemize}
    \end{block}
    \end{cminipage}
\end{frame}

\begin{frame}{Types of Granger Causality}
    \begin{cminipage}{0.95\textwidth}
    \vspace{-0.2cm}
    \begin{center}
    \begin{tikzpicture}[scale=0.8,
        box/.style={rectangle, draw=MainBlue, thick, minimum width=2cm, minimum height=0.8cm, font=\small},
        arrow/.style={-{Stealth[length=2.5mm]}, thick, MainBlue}
    ]
        % Unidirectional X -> Y
        \node[box] (x1) at (0, 1.5) {$X$};
        \node[box] (y1) at (3.5, 1.5) {$Y$};
        \draw[arrow] (x1) -- (y1);
        \node[below=0.05cm of x1, xshift=1.75cm, font=\scriptsize] {$X \rightarrow Y$};

        % Unidirectional Y -> X
        \node[box] (x2) at (0, 0) {$X$};
        \node[box] (y2) at (3.5, 0) {$Y$};
        \draw[arrow] (y2) -- (x2);
        \node[below=0.05cm of x2, xshift=1.75cm, font=\scriptsize] {$Y \rightarrow X$};

        % Bidirectional
        \node[box] (x3) at (7, 1.5) {$X$};
        \node[box] (y3) at (10.5, 1.5) {$Y$};
        \draw[arrow] (x3.east) -- (y3.west);
        \draw[arrow] (y3.west) ++(0, -0.25) -- (x3.east |- {0, 1.25});
        \node[below=0.05cm of x3, xshift=1.75cm, font=\scriptsize] {$X \leftrightarrow Y$};

        % No causality
        \node[box] (x4) at (7, 0) {$X$};
        \node[box] (y4) at (10.5, 0) {$Y$};
        \node[below=0.05cm of x4, xshift=1.75cm, font=\scriptsize] {No causality};
    \end{tikzpicture}
    \end{center}

    \vspace{0.1cm}

    {\footnotesize
    \begin{block}{Economic Examples}
        \begin{itemize}\setlength{\itemsep}{0pt}
            \item Money $\rightarrow$ Output? (monetarist); Stock prices $\leftrightarrow$ Volume (bidirectional)
        \end{itemize}
    \end{block}
    }
    \end{cminipage}
\end{frame}

\begin{frame}{Cross-Correlation: Visual Illustration}
    \vspace{-0.2cm}
    {\scriptsize
    \begin{block}{Interpretation}
        \begin{itemize}\setlength{\itemsep}{0pt}
            \item Left: two related series; Right: CCF reveals that $X$ leads $Y$ (significant correlations at positive lags)
        \end{itemize}
    \end{block}
    }
    \begin{center}
        \includegraphics[width=0.95\textwidth, height=0.50\textheight, keepaspectratio]{ch6_def_ccf.pdf}
    \end{center}
    \quantlet{TSA\_ch6\_def\_ccf}{https://github.com/QuantLet/TSA/tree/main/TSA_ch6/TSA_ch6_def_ccf}
\end{frame}

\begin{frame}{Cross-Correlation Function}
    \vspace{-0.1cm}
    {\small
    \begin{cminipage}{0.95\textwidth}
    \begin{defn}[Cross-Correlation Function]
        The \textbf{cross-correlation} between $X_t$ and $Y_t$ at lag $k$ is:
        $$\rho_{XY}(k) = \frac{\gamma_{XY}(k)}{\sigma_X \sigma_Y} = \frac{\Cov(X_t, Y_{t+k})}{\sqrt{\Var(X_t)\Var(Y_t)}}$$
    \end{defn}

    \begin{block}{Interpretation}
        \begin{itemize}\setlength{\itemsep}{0pt}
            \item $\rho_{XY}(k) > 0$ at $k > 0$: $X$ is positively correlated with future $Y$ (X may lead Y)
            \item $\rho_{XY}(k) > 0$ at $k < 0$: $X$ is positively correlated with past $Y$ (Y may lead X)
        \end{itemize}
    \end{block}

    \begin{alertblock}{Note}
        Unlike ACF, cross-correlation is \textbf{not symmetric}: $\rho_{XY}(k) \neq \rho_{XY}(-k)$ in general.
    \end{alertblock}
    \end{cminipage}
    }
\end{frame}

\begin{frame}{Granger Causality: Practical Considerations}
    \begin{cminipage}{0.95\textwidth}
    \begin{alertblock}{Common Pitfalls}
        \begin{enumerate}\setlength{\itemsep}{0pt}
            \item \textbf{Omitted variables}: A third variable $Z$ may cause both $X$ and $Y$
            \item \textbf{Non-stationarity}: Test requires stationary data (or cointegration)
            \item \textbf{Lag selection}: Results can be sensitive to $p$
            \item \textbf{Sample size}: Need sufficient observations
        \end{enumerate}
    \end{alertblock}

    \vspace{0.15cm}

    \begin{block}{Best Practices}
        \begin{itemize}\setlength{\itemsep}{0pt}
            \item Test for unit roots first
            \item Use multiple lag selection criteria
            \item Check robustness to different lag lengths
            \item Report results for both directions
        \end{itemize}
    \end{block}
    \end{cminipage}
\end{frame}

\begin{frame}{Granger Causality Test: Numerical Example}
    {\small
    \begin{cminipage}{0.95\textwidth}
    \begin{exampleblock}{Testing: Does Money Growth Granger-cause Output?}
        \textbf{Unrestricted model} (VAR with 2 lags):
        $$\Delta Y_t = c + \alpha_1 \Delta Y_{t-1} + \alpha_2 \Delta Y_{t-2} + \beta_1 \Delta M_{t-1} + \beta_2 \Delta M_{t-2} + \varepsilon_t$$

        \textbf{Restricted model} ($H_0$: $\beta_1 = \beta_2 = 0$):
        $$\Delta Y_t = c + \alpha_1 \Delta Y_{t-1} + \alpha_2 \Delta Y_{t-2} + \varepsilon_t$$
    \end{exampleblock}

    \begin{block}{Test Computation}
        With $T = 100$, $RSS_U = 45.2$, $RSS_R = 52.8$:
        $$F = \frac{(52.8 - 45.2)/2}{45.2/(100 - 5)} = \frac{3.8}{0.476} = 7.98$$

        $F_{0.05}(2, 95) = 3.09$ $\Rightarrow$ \textbf{Reject $H_0$}: Money Granger-causes output!
    \end{block}
    \end{cminipage}
    }
\end{frame}

\begin{frame}{The Toda-Yamamoto Procedure}
    \vspace{-0.2cm}
    {\small
    \begin{cminipage}{0.95\textwidth}
    \begin{alertblock}{Problem with Non-Stationary Data}
        Standard Granger test has \textbf{non-standard distributions} when:
        \begin{itemize}\setlength{\itemsep}{0pt}
            \item Variables have unit roots
            \item Variables are cointegrated
        \end{itemize}
    \end{alertblock}

    \begin{block}{Toda-Yamamoto Solution (1995)}
        \begin{enumerate}\setlength{\itemsep}{0pt}
            \item Determine maximum order of integration $d_{max}$
            \item Estimate VAR($p + d_{max}$) in \textbf{levels}
            \item Test restrictions on first $p$ lags only
            \item Extra $d_{max}$ lags are \textbf{not} tested (just for correct distribution)
        \end{enumerate}
    \end{block}

    \begin{exampleblock}{Advantage}
        Wald test has asymptotic $\chi^2$ distribution regardless of cointegration!
    \end{exampleblock}
    \end{cminipage}
    }
\end{frame}

\begin{frame}{Instantaneous Causality}
    \begin{cminipage}{0.95\textwidth}
    \vspace{-0.25cm}
    {\scriptsize
    \begin{block}{Definition}
        \begin{itemize}\setlength{\itemsep}{0pt}
            \item $X$ \textbf{instantaneously causes} $Y$ if $\E[Y_t | \Omega_{t-1}, X_t] \neq \E[Y_t | \Omega_{t-1}]$
            \item $\Omega_{t-1}$ contains all past information
        \end{itemize}
    \end{block}

    \vspace{0.1cm}

    \begin{block}{Testing in VAR}
        \begin{itemize}\setlength{\itemsep}{0pt}
            \item Test whether $\sigma_{12} \neq 0$ in the covariance matrix:
                $\bSigma = \begin{pmatrix} \sigma_1^2 & \sigma_{12} \\ \sigma_{12} & \sigma_2^2 \end{pmatrix}$
            \item If $\sigma_{12} = 0$: no instantaneous causality
        \end{itemize}
    \end{block}

    \vspace{0.1cm}

    {\footnotesize
    \begin{alertblock}{Interpretation}
        Instantaneous causality often reflects \textbf{common shocks} or \textbf{data aggregation}, not true contemporaneous effects.
    \end{alertblock}
    }
    }
    \end{cminipage}
\end{frame}

\begin{frame}{Granger Causality in Multiple Systems}
    \begin{cminipage}{0.95\textwidth}
    \begin{block}{Block Exogeneity Test}
        \begin{itemize}\setlength{\itemsep}{2pt}
            \item In a VAR with $K > 2$ variables, test whether a \textbf{group} of variables Granger-causes another group
            \item Example: Do financial variables (interest rates, stock prices) Granger-cause real variables (GDP, unemployment)?
        \end{itemize}
    \end{block}

    \vspace{0.15cm}

    \begin{exampleblock}{Test Statistic}
        $$LR = T \left(\ln|\hat{\bSigma}_R| - \ln|\hat{\bSigma}_U|\right) \sim \chi^2_{K_1 \cdot K_2 \cdot p}$$

        where $K_1$ = number of ``caused'' variables, $K_2$ = number of ``causing'' variables
    \end{exampleblock}
    \end{cminipage}
\end{frame}

%=============================================================================
% SECTION 4: IMPULSE RESPONSE FUNCTIONS
%=============================================================================
\section{Impulse Response Functions}

\begin{frame}{What are Impulse Response Functions?}
    \begin{cminipage}{0.95\textwidth}
    \vspace{-0.2cm}
    {\small
    \begin{block}{Definition}
        An \textbf{Impulse Response Function (IRF)} traces the effect of a one-time shock to one variable on the current and future values of all variables.
    \end{block}

    \vspace{0.15cm}

    \begin{exampleblock}{Question IRFs Answer}
        ``If there is an unexpected 1-unit shock to $Y_1$ today, what happens to $Y_1$ and $Y_2$ over the next $h$ periods?''
    \end{exampleblock}

    \vspace{0.15cm}

    \begin{block}{MA($\infty$) Representation}
        A stable VAR(p) can be written as:
        $$\bY_t = \boldsymbol{\mu} + \sum_{i=0}^{\infty} \boldsymbol{\Phi}_i \bepsilon_{t-i}$$

        The matrices $\boldsymbol{\Phi}_i$ are the \textbf{impulse responses} at horizon $i$.
    \end{block}
    }
    \end{cminipage}
\end{frame}

\begin{frame}{Computing IRFs for VAR(1)}
    \begin{cminipage}{0.95\textwidth}
    \begin{block}{For VAR(1): $\bY_t = \mathbf{c} + \bA \bY_{t-1} + \bepsilon_t$}
        The impulse response matrices are:
        $$\boldsymbol{\Phi}_0 = \mathbf{I}_K, \quad \boldsymbol{\Phi}_1 = \bA, \quad \boldsymbol{\Phi}_2 = \bA^2, \quad \ldots, \quad \boldsymbol{\Phi}_h = \bA^h$$
    \end{block}

    \vspace{0.15cm}

    \begin{block}{Interpretation}
        \begin{itemize}\setlength{\itemsep}{2pt}
            \item $[\boldsymbol{\Phi}_h]_{ij}$ = Effect on $Y_i$ at time $t+h$ of a unit shock to $Y_j$ at time $t$
            \item For stable VAR: $\boldsymbol{\Phi}_h \rightarrow \mathbf{0}$ as $h \rightarrow \infty$ (shocks die out)
        \end{itemize}
    \end{block}
    \end{cminipage}
\end{frame}

\begin{frame}{Computing IRFs for General VAR(p)}
    \vspace{-0.1cm}
    {\small
    \begin{cminipage}{0.95\textwidth}
    \begin{block}{Recursive Formula for VAR(p)}
        For $\bY_t = \mathbf{c} + \bA_1\bY_{t-1} + \bA_2\bY_{t-2} + \cdots + \bA_p\bY_{t-p} + \bepsilon_t$:
        $$\boldsymbol{\Phi}_h = \sum_{j=1}^{\min(h,p)} \bA_j \boldsymbol{\Phi}_{h-j}, \quad h = 1, 2, 3, \ldots$$
        with $\boldsymbol{\Phi}_0 = \mathbf{I}_K$ and $\boldsymbol{\Phi}_h = \mathbf{0}$ for $h < 0$.
    \end{block}

    \begin{exampleblock}{Example: VAR(2) IRFs}
        \begin{itemize}\setlength{\itemsep}{0pt}
            \item $\boldsymbol{\Phi}_0 = \mathbf{I}_K$
            \item $\boldsymbol{\Phi}_1 = \bA_1 \boldsymbol{\Phi}_0 = \bA_1$
            \item $\boldsymbol{\Phi}_2 = \bA_1 \boldsymbol{\Phi}_1 + \bA_2 \boldsymbol{\Phi}_0 = \bA_1^2 + \bA_2$
            \item $\boldsymbol{\Phi}_3 = \bA_1 \boldsymbol{\Phi}_2 + \bA_2 \boldsymbol{\Phi}_1 = \bA_1(\bA_1^2 + \bA_2) + \bA_2\bA_1$
        \end{itemize}
    \end{exampleblock}
    \end{cminipage}
    }
\end{frame}

\begin{frame}{Impulse Response Functions: Example}
    \vspace{-0.2cm}
    {\footnotesize
    \begin{itemize}\setlength{\itemsep}{0pt}
            \item IRFs show how each variable responds to a one-unit shock over time
            \item Shaded regions represent confidence intervals (uncertainty in estimates)
            \item For stable VAR models, responses converge to zero as the horizon increases
        \end{itemize}
    }
    \begin{center}
        \includegraphics[width=0.95\textwidth, height=0.50\textheight, keepaspectratio]{ch6_irf.pdf}
    \end{center}
    \quantlet{TSA\_ch6\_irf}{https://github.com/QuantLet/TSA/tree/main/TSA_ch6/TSA_ch6_irf}
\end{frame}

\begin{frame}{Orthogonalized IRFs}
    \begin{cminipage}{0.95\textwidth}
    \vspace{-0.15cm}
    {\small
    \begin{alertblock}{Problem: Correlated Errors}
        \begin{itemize}\setlength{\itemsep}{0pt}
            \item If $\bSigma$ is not diagonal, shocks $\varepsilon_{1t}$ and $\varepsilon_{2t}$ are correlated
            \item A shock to ``$Y_1$'' also involves a shock to ``$Y_2$''
        \end{itemize}
    \end{alertblock}

    \vspace{0.15cm}

    \begin{block}{Solution: Cholesky Decomposition}
        \begin{itemize}\setlength{\itemsep}{0pt}
            \item Factor $\bSigma = \mathbf{P}\mathbf{P}'$ where $\mathbf{P}$ is lower triangular
            \item Orthogonalized shocks: $\mathbf{u}_t = \mathbf{P}^{-1}\bepsilon_t$ with $\E[\mathbf{u}_t \mathbf{u}_t'] = \mathbf{I}$
            \item Orthogonalized IRFs: $\boldsymbol{\Theta}_h = \boldsymbol{\Phi}_h \mathbf{P}$
        \end{itemize}
    \end{block}

    \vspace{0.2cm}

    {\small
    \begin{alertblock}{Ordering Matters!}
        \begin{itemize}\setlength{\itemsep}{0pt}
            \item Cholesky assumes variables ordered from ``most exogenous'' to ``most endogenous''
            \item Results depend on this ordering
        \end{itemize}
    \end{alertblock}
    }
    }
    \end{cminipage}
\end{frame}

\begin{frame}{Cholesky Decomposition: How It Works}
    \begin{cminipage}{0.95\textwidth}
    {\footnotesize
    \begin{block}{Numerical Example}
        Suppose the VAR residual covariance is
        $\bSigma = \begin{pmatrix} 4 & 2 \\ 2 & 5 \end{pmatrix}$.
        The Cholesky factor $\mathbf{P}$ (lower triangular) satisfying $\bSigma = \mathbf{P}\mathbf{P}'$ is:
        \[
        \mathbf{P} = \begin{pmatrix} 2 & 0 \\ 1 & 2 \end{pmatrix}
        \quad \Rightarrow \quad
        \mathbf{P}\mathbf{P}' = \begin{pmatrix} 2 & 0 \\ 1 & 2 \end{pmatrix}\begin{pmatrix} 2 & 1 \\ 0 & 2 \end{pmatrix} = \begin{pmatrix} 4 & 2 \\ 2 & 5 \end{pmatrix} = \bSigma \checkmark
        \]
    \end{block}
    \vspace{-0.15cm}
    \begin{exampleblock}{Interpretation: What the Ordering Implies}
        \begin{itemize}\setlength{\itemsep}{0pt}
            \item $P_{21} = 1 \neq 0$: A unit shock to $Y_1$ has an \textbf{immediate effect on $Y_2$} (impact = 1)
            \item $P_{12} = 0$: A shock to $Y_2$ has \textbf{no contemporaneous effect on $Y_1$}
            \item $Y_1$ is ``more exogenous'' --- it affects $Y_2$ instantly, but not vice versa
        \end{itemize}
    \end{exampleblock}
    \vspace{-0.15cm}
    \begin{alertblock}{Reverse the Ordering ($Y_2$ First)}
        Swapping the variables gives a different $\mathbf{P}$ and different IRFs. This is why \textbf{economic theory} must guide the ordering --- e.g., GDP before unemployment (Okun's law: output shocks affect labor markets, not the reverse on impact).
    \end{alertblock}
    }
    \end{cminipage}
\end{frame}

\begin{frame}{IRF Numerical Example}
    \vspace{-3mm}
    {\small
    \begin{cminipage}{0.95\textwidth}
    \begin{block}{For $\bA = \begin{pmatrix} 0.7 & 0.2 \\ -0.1 & 0.6 \end{pmatrix}$}
        \begin{align*}
            \boldsymbol{\Phi}_0 &= \begin{pmatrix} 1 & 0 \\ 0 & 1 \end{pmatrix} \\
            \boldsymbol{\Phi}_1 &= \bA = \begin{pmatrix} 0.7 & 0.2 \\ -0.1 & 0.6 \end{pmatrix} \\
            \boldsymbol{\Phi}_2 &= \bA^2 = \begin{pmatrix} 0.47 & 0.26 \\ -0.13 & 0.34 \end{pmatrix}
        \end{align*}
    \end{block}

    \begin{block}{Interpretation}
        \begin{itemize}\setlength{\itemsep}{0pt}
            \item $[\boldsymbol{\Phi}_2]_{12} = 0.26$: A unit shock to $Y_2$ increases $Y_1$ by 0.26 after 2 periods
            \item $[\boldsymbol{\Phi}_2]_{21} = -0.13$: A unit shock to $Y_1$ \textbf{decreases} $Y_2$ by 0.13 after 2 periods
        \end{itemize}
    \end{block}
    \end{cminipage}
    }
\end{frame}

\begin{frame}{Cumulative Impulse Responses}
    \begin{cminipage}{0.95\textwidth}
    \begin{block}{Definition}
        \begin{itemize}\setlength{\itemsep}{0pt}
            \item The \textbf{cumulative IRF} up to horizon $H$:
                $\boldsymbol{\Psi}_H = \sum_{h=0}^{H} \boldsymbol{\Phi}_h$
            \item Measures the \textbf{total accumulated effect} of a shock
        \end{itemize}
    \end{block}

    \vspace{0.15cm}

    \begin{exampleblock}{Long-Run Multiplier}
        \begin{itemize}\setlength{\itemsep}{0pt}
            \item For stable VAR: $\boldsymbol{\Psi}_\infty = (\mathbf{I}_K - \bA_1 - \bA_2 - \cdots - \bA_p)^{-1}$
            \item This gives the \textbf{permanent effect} of a one-time shock
        \end{itemize}
    \end{exampleblock}

    \vspace{0.15cm}

    {\footnotesize
    \begin{alertblock}{When to Use}
        Cumulative IRFs are useful when interested in total impact (e.g., cumulative GDP loss from a shock).
    \end{alertblock}
    }
    \end{cminipage}
\end{frame}

\begin{frame}{Confidence Intervals for IRFs}
    \vspace{-0.2cm}
    {\footnotesize
    \begin{cminipage}{0.95\textwidth}
    \begin{block}{Sources of Uncertainty}
        \begin{itemize}\setlength{\itemsep}{0pt}
            \item IRFs are functions of estimated parameters $\hat{\bA}_1, \ldots, \hat{\bA}_p$, so they have \textbf{sampling uncertainty}
        \end{itemize}
    \end{block}

    \begin{block}{Methods for Confidence Bands}
        \begin{enumerate}\setlength{\itemsep}{0pt}
            \item \textbf{Asymptotic}: Delta method for standard errors
            \item \textbf{Monte Carlo}: Simulate from asymptotic distribution of $\hat{\bA}$
            \item \textbf{Bootstrap}: Resample residuals and re-estimate VAR
        \end{enumerate}
    \end{block}

    \begin{alertblock}{Bootstrap Procedure}
        \begin{enumerate}\setlength{\itemsep}{0pt}
            \item Estimate VAR, save residuals $\{\hat{\bepsilon}_t\}$
            \item Draw with replacement to create $\{\hat{\bepsilon}_t^*\}$
            \item Generate bootstrap sample, re-estimate, compute IRFs
            \item Repeat $B$ times; use percentiles for CIs
        \end{enumerate}
    \end{alertblock}
    \end{cminipage}
    }
\end{frame}

\begin{frame}{Structural VAR (SVAR)}
    \begin{cminipage}{0.95\textwidth}
    \begin{block}{Motivation}
        \begin{itemize}\setlength{\itemsep}{2pt}
            \item Standard VAR shocks $\bepsilon_t$ are \textbf{reduced-form} innovations---linear combinations of structural shocks
            \item We want to identify economically meaningful \textbf{structural shocks}
        \end{itemize}
    \end{block}

    \vspace{0.15cm}

    \begin{block}{Structural Form}
        \begin{itemize}\setlength{\itemsep}{0pt}
            \item $\mathbf{B}_0 \bY_t = \boldsymbol{\Gamma}_0 + \mathbf{B}_1 \bY_{t-1} + \cdots + \mathbf{B}_p \bY_{t-p} + \mathbf{u}_t$
            \item $\mathbf{u}_t$ are \textbf{structural shocks} with $\E[\mathbf{u}_t \mathbf{u}_t'] = \mathbf{I}_K$
        \end{itemize}
    \end{block}

    \vspace{0.15cm}

    {\footnotesize
    \begin{exampleblock}{Relationship to Reduced Form}
        $\bepsilon_t = \mathbf{B}_0^{-1} \mathbf{u}_t \quad \Rightarrow \quad \bSigma = \mathbf{B}_0^{-1} (\mathbf{B}_0^{-1})'$
    \end{exampleblock}
    }
    \end{cminipage}
\end{frame}

\begin{frame}{Structural IRF Example}
    \vspace{-0.2cm}
    {\footnotesize
    \begin{itemize}\setlength{\itemsep}{0pt}
            \item Structural IRFs based on Cholesky identification
            \item Order of variables affects interpretation of shocks
            \item First variable responds only to own shocks contemporaneously
        \end{itemize}
    }
    \begin{center}
        \includegraphics[width=0.95\textwidth, height=0.50\textheight, keepaspectratio]{ch6_structural_irf.pdf}
    \end{center}
    \quantlet{TSA\_ch6\_structural\_irf}{https://github.com/QuantLet/TSA/tree/main/TSA_ch6/TSA_ch6_structural_irf}
\end{frame}

\begin{frame}{Identification in SVAR}
    \vspace{-0.2cm}
    {\small
    \begin{cminipage}{0.95\textwidth}
    \begin{alertblock}{The Identification Problem}
        \begin{itemize}\setlength{\itemsep}{2pt}
            \item $\bSigma$ has $K(K+1)/2$ unique elements, but $\mathbf{B}_0^{-1}$ has $K^2$ elements
            \item Need $K(K-1)/2$ additional restrictions!
        \end{itemize}
    \end{alertblock}

    \begin{block}{Common Identification Schemes}
        \begin{enumerate}\setlength{\itemsep}{0pt}
            \item \textbf{Short-run restrictions}: Zero impact effects (Cholesky)
            \item \textbf{Long-run restrictions}: Zero long-run effects (Blanchard-Quah)
            \item \textbf{Sign restrictions}: Inequality constraints on IRFs
            \item \textbf{External instruments}: Use outside information
        \end{enumerate}
    \end{block}

    \begin{exampleblock}{Example: Cholesky (Recursive) Ordering}
        \begin{itemize}\setlength{\itemsep}{0pt}
            \item For $K=2$: $\mathbf{B}_0^{-1} = \begin{pmatrix} b_{11} & 0 \\ b_{21} & b_{22} \end{pmatrix}$
            \item Variable 1 doesn't respond to shock 2 contemporaneously
        \end{itemize}
    \end{exampleblock}
    \end{cminipage}
    }
\end{frame}

%=============================================================================
% SECTION 5: FORECAST ERROR VARIANCE DECOMPOSITION
%=============================================================================
\section{Forecast Error Variance Decomposition}

\begin{frame}{FEVD: Example}
    \vspace{-0.2cm}
    {\footnotesize
    \begin{itemize}\setlength{\itemsep}{0pt}
            \item FEVD shows the proportion of forecast variance attributable to each shock
            \item At short horizons, own shocks dominate; cross-variable effects grow over time
            \item Useful for understanding the relative importance of different shocks in the system
        \end{itemize}
    }
    \begin{center}
        \includegraphics[width=0.95\textwidth, height=0.50\textheight, keepaspectratio]{ch6_fevd.pdf}
    \end{center}
    \quantlet{TSA\_ch6\_fevd}{https://github.com/QuantLet/TSA/tree/main/TSA_ch6/TSA_ch6_fevd}
\end{frame}

\begin{frame}{Variance Decomposition}
    \begin{cminipage}{0.95\textwidth}
    \begin{alertblock}{Question}
        What proportion of the forecast error variance of $Y_i$ at horizon $h$ is due to shocks to $Y_j$?
    \end{alertblock}

    \vspace{0.1cm}

    \begin{block}{FEVD Formula}
        $$\text{FEVD}_{ij}(h) = \frac{\sum_{s=0}^{h-1} [\boldsymbol{\Theta}_s]_{ij}^2}{\sum_{s=0}^{h-1} \sum_{k=1}^{K} [\boldsymbol{\Theta}_s]_{ik}^2}$$
    \end{block}

    \vspace{0.1cm}

    {\footnotesize
    \begin{exampleblock}{Properties}
        \begin{itemize}\setlength{\itemsep}{0pt}
            \item $0 \leq \text{FEVD}_{ij}(h) \leq 1$ and $\sum_{j=1}^{K} \text{FEVD}_{ij}(h) = 1$ (sums to 100\%)
            \item At $h=1$: own shocks dominate (by Cholesky construction)
        \end{itemize}
    \end{exampleblock}
    }
    \end{cminipage}
\end{frame}

\begin{frame}{FEVD: Numerical Example}
    \vspace{-0.15cm}
    {\small
    \begin{cminipage}{0.95\textwidth}
    \begin{block}{Computing FEVD for Bivariate VAR}
        Using orthogonalized IRFs $\boldsymbol{\Theta}_h$, FEVD at horizon $H$:
        $$\text{FEVD}_{11}(H) = \frac{\sum_{h=0}^{H-1} \theta_{11}^2(h)}{\sum_{h=0}^{H-1} [\theta_{11}^2(h) + \theta_{12}^2(h)]}$$
    \end{block}

    \begin{exampleblock}{Example Calculation}
        \begin{center}
        \begin{tabular}{c|cc|cc}
            \toprule
            $h$ & $\theta_{11}(h)$ & $\theta_{12}(h)$ & $\theta_{11}^2(h)$ & $\theta_{12}^2(h)$ \\
            \midrule
            0 & 1.00 & 0.00 & 1.00 & 0.00 \\
            1 & 0.70 & 0.20 & 0.49 & 0.04 \\
            2 & 0.47 & 0.26 & 0.22 & 0.07 \\
            \bottomrule
        \end{tabular}
        \end{center}

        $\text{FEVD}_{11}(3) = \frac{1.00 + 0.49 + 0.22}{1.00 + 0.49 + 0.22 + 0.00 + 0.04 + 0.07} = \frac{1.71}{1.82} = 94\%$
    \end{exampleblock}
    \end{cminipage}
    }
\end{frame}

\begin{frame}{Historical Decomposition: Example}
    \vspace{-0.2cm}
    {\scriptsize
    \begin{block}{Observations}
        \begin{itemize}\setlength{\itemsep}{0pt}
            \item Structural contributions: each color = a different shock, stacked to sum the deviation from mean
            \item Useful for identifying shocks behind historical episodes
        \end{itemize}
    \end{block}
    }
    \begin{center}
        \includegraphics[width=0.95\textwidth, height=0.50\textheight, keepaspectratio]{ch6_historical_decomp.pdf}
    \end{center}
    \quantlet{TSA\_ch6\_historical\_decomp}{https://github.com/QuantLet/TSA/tree/main/TSA_ch6/TSA_ch6_historical_decomp}
\end{frame}

\begin{frame}{Historical Decomposition}
    \begin{cminipage}{0.95\textwidth}
    \begin{block}{Definition}
        \begin{itemize}\setlength{\itemsep}{0pt}
            \item \textbf{Historical decomposition} breaks down each observed value into contributions from each structural shock:
                $$Y_{it} - \bar{Y}_i = \sum_{j=1}^{K} \sum_{s=0}^{t-1} \theta_{ij}(s) \cdot u_{j,t-s}$$
                \vspace{-0.15cm}
        \end{itemize}
    \end{block}

    \vspace{0.15cm}

    \begin{exampleblock}{Application}
        \begin{itemize}\setlength{\itemsep}{0pt}
            \item ``How much of the 2008 GDP decline was due to financial shocks vs. oil shocks?''
                \begin{itemize}\setlength{\itemsep}{0pt}
                    \item Attributes historical movements to specific identified shocks
                    \item Useful for policy analysis and narrative interpretation
                \end{itemize}
        \end{itemize}
    \end{exampleblock}
    \end{cminipage}
\end{frame}

%=============================================================================
% SECTION 6: VAR DIAGNOSTICS
%=============================================================================
\section{VAR Diagnostics}

\begin{frame}{Residual Diagnostics}
    \begin{cminipage}{0.95\textwidth}
    \begin{block}{What to Check}
        After estimating VAR, verify that residuals $\hat{\bepsilon}_t$ behave like white noise:
        \begin{enumerate}\setlength{\itemsep}{0pt}
            \item No serial correlation
            \item Constant variance (homoskedasticity)
            \item Normality (for inference)
        \end{enumerate}
    \end{block}

    \vspace{0.15cm}

    \begin{alertblock}{Why It Matters}
        \begin{itemize}\setlength{\itemsep}{0pt}
            \item Autocorrelated residuals $\Rightarrow$ inefficient estimates
            \item Heteroskedasticity $\Rightarrow$ invalid standard errors
            \item Non-normality $\Rightarrow$ inference may be unreliable
        \end{itemize}
    \end{alertblock}
    \end{cminipage}
\end{frame}

\begin{frame}{Testing for Serial Correlation}
    \vspace{-0.25cm}
    {\small
    \begin{cminipage}{0.95\textwidth}
    \begin{block}{Portmanteau Test (Ljung-Box)}
        \begin{itemize}\setlength{\itemsep}{0pt}
            \item $Q_h = T(T+2) \sum_{j=1}^{h} \frac{1}{T-j} \text{tr}(\hat{\mathbf{C}}_j' \hat{\mathbf{C}}_0^{-1} \hat{\mathbf{C}}_j \hat{\mathbf{C}}_0^{-1})$
            \item $\hat{\mathbf{C}}_j = \frac{1}{T}\sum_{t=j+1}^{T} \hat{\bepsilon}_t \hat{\bepsilon}_{t-j}'$
            \item Under $H_0$ (no autocorrelation): $Q_h \sim \chi^2_{K^2(h-p)}$
        \end{itemize}
    \end{block}

    \begin{block}{Breusch-Godfrey LM Test}
        \begin{enumerate}\setlength{\itemsep}{0pt}
            \item Regress $\hat{\bepsilon}_t$ on $\hat{\bepsilon}_{t-1}, \ldots, \hat{\bepsilon}_{t-h}$ and original regressors
            \item $LM = T \cdot R^2 \sim \chi^2_{K^2 h}$ under $H_0$
        \end{enumerate}
    \end{block}

    \begin{alertblock}{If Rejected}
        \begin{itemize}\setlength{\itemsep}{0pt}
            \item Consider increasing lag order $p$ or adding additional variables
        \end{itemize}
    \end{alertblock}
    \end{cminipage}
    }
\end{frame}

\begin{frame}{Testing for Heteroskedasticity}
    \begin{cminipage}{0.95\textwidth}
    \begin{block}{ARCH-LM Test}
        \begin{itemize}\setlength{\itemsep}{0pt}
            \item Test for ARCH in residuals:
                $\hat{\varepsilon}_{it}^2 = \alpha_0 + \alpha_1 \hat{\varepsilon}_{i,t-1}^2 + \cdots + \alpha_q \hat{\varepsilon}_{i,t-q}^2 + v_t$
            \item $H_0$: $\alpha_1 = \cdots = \alpha_q = 0$ (homoskedasticity)
            \item $LM = TR^2 \sim \chi^2_q$
        \end{itemize}
    \end{block}

    \vspace{0.15cm}

    \begin{exampleblock}{Multivariate Version}
        \begin{itemize}\setlength{\itemsep}{0pt}
            \item Test all equations jointly using:
                $$\text{vech}(\hat{\bepsilon}_t \hat{\bepsilon}_t') = \mathbf{c} + \sum_{j=1}^{q} \mathbf{B}_j \text{vech}(\hat{\bepsilon}_{t-j} \hat{\bepsilon}_{t-j}') + \mathbf{v}_t$$
                \vspace{-0.2cm}
        \end{itemize}
    \end{exampleblock}
    \end{cminipage}
\end{frame}

\begin{frame}{Normality Testing}
    \begin{cminipage}{0.95\textwidth}
    \vspace{-0.15cm}
    \begin{block}{Jarque-Bera Test (Univariate)}
        $$JB = \frac{T}{6}\left(S^2 + \frac{(\kappa-3)^2}{4}\right) \sim \chi^2_2$$

        where $S$ = skewness, $\kappa$ = kurtosis
    \end{block}

    \vspace{0.15cm}

    \begin{block}{Multivariate Normality (Doornik-Hansen)}
        \begin{itemize}\setlength{\itemsep}{0pt}
            \item Transform residuals and test joint skewness and kurtosis:
                $$DH = s_1'(\boldsymbol{\Omega}^{-1/2})'(\boldsymbol{\Omega}^{-1/2})s_1 + s_2'(\boldsymbol{\Omega}^{-1/2})'(\boldsymbol{\Omega}^{-1/2})s_2 \sim \chi^2_{2K}$$
                \vspace{-0.2cm}
        \end{itemize}
    \end{block}

    \vspace{0.15cm}

    {\footnotesize
    \begin{alertblock}{Note}
        Normality is often rejected in financial data. Consider robust standard errors if non-normality is severe.
    \end{alertblock}
    }
    \end{cminipage}
\end{frame}

\begin{frame}{Diagnostic Summary Plot}
    \vspace{-0.15cm}
    \vspace{-0.2cm}
    {\footnotesize
    \begin{itemize}\setlength{\itemsep}{0pt}
        \item Residual ACF should show no significant autocorrelation
        \item Histogram should approximate normal distribution
        \item Q-Q plot should follow 45-degree line
    \end{itemize}
    }
\end{frame}

\begin{frame}{Diagnostic Summary Plot}
    \begin{center}
        \includegraphics[width=0.95\textwidth, height=0.78\textheight, keepaspectratio]{ch6_diagnostics.pdf}
    \end{center}
    \quantlet{TSA\_ch6\_diagnostics}{https://github.com/QuantLet/TSA/tree/main/TSA_ch6/TSA_ch6_diagnostics}
\end{frame}

%=============================================================================
% SECTION 7: VAR FORECASTING
%=============================================================================
\section{VAR Forecasting}

\begin{frame}{Point Forecasts from VAR}
    \begin{cminipage}{0.95\textwidth}
    \begin{block}{Iterative Forecasting}
        For VAR(1): $\bY_t = \mathbf{c} + \bA \bY_{t-1} + \bepsilon_t$
        \begin{itemize}\setlength{\itemsep}{0pt}
            \item \textbf{1-step forecast}: $\hat{\bY}_{T+1|T} = \mathbf{c} + \bA \bY_T$
            \item \textbf{2-step forecast}: $\hat{\bY}_{T+2|T} = \mathbf{c} + \bA \hat{\bY}_{T+1|T}$
            \item \textbf{$h$-step forecast}: $\hat{\bY}_{T+h|T} = \mathbf{c} + \bA \hat{\bY}_{T+h-1|T}$
        \end{itemize}
    \end{block}

    \vspace{0.15cm}

    \begin{exampleblock}{Direct Formula}
        \begin{itemize}\setlength{\itemsep}{0pt}
            \item $\hat{\bY}_{T+h|T} = (\mathbf{I} + \bA + \bA^2 + \cdots + \bA^{h-1})\mathbf{c} + \bA^h \bY_T$
            \item For stable VAR: converges to $\boldsymbol{\mu} = (\mathbf{I} - \bA)^{-1}\mathbf{c}$ as $h \to \infty$
        \end{itemize}
    \end{exampleblock}
    \end{cminipage}
\end{frame}

\begin{frame}{Forecast Error and MSE}
    \begin{cminipage}{0.95\textwidth}
    \vspace{-2mm}
    {\footnotesize
    \begin{block}{$h$-Step Forecast Error}
        $\mathbf{e}_{T+h|T} = \bY_{T+h} - \hat{\bY}_{T+h|T} = \sum_{j=0}^{h-1} \bA^j \bepsilon_{T+h-j}$
    \end{block}

    \begin{block}{Mean Squared Error Matrix}
        $\text{MSE}(\hat{\bY}_{T+h|T}) = \E[\mathbf{e}_{T+h|T} \mathbf{e}_{T+h|T}'] = \sum_{j=0}^{h-1} \bA^j \bSigma (\bA^j)'$
    \end{block}

    \begin{exampleblock}{Key Insight}
        \begin{itemize}\setlength{\itemsep}{0pt}
            \item MSE increases with $h$; converges to $\boldsymbol{\Gamma}(0)$ for stable VAR
            \item Long-horizon forecasts $\to$ unconditional mean
        \end{itemize}
    \end{exampleblock}
    }
    \end{cminipage}
\end{frame}

\begin{frame}{VAR Forecasts: Example}
    \vspace{-0.2cm}
    {\footnotesize
    \begin{itemize}\setlength{\itemsep}{0pt}
            \item Point forecasts shown as solid line beyond observed data
            \item Confidence bands widen as forecast horizon increases
            \item Forecasts converge to unconditional mean for long horizons
        \end{itemize}
    }
    \begin{center}
        \includegraphics[width=0.95\textwidth, height=0.50\textheight, keepaspectratio]{ch6_var_forecast.pdf}
    \end{center}
    \quantlet{TSA\_ch6\_var\_forecast}{https://github.com/QuantLet/TSA/tree/main/TSA_ch6/TSA_ch6_var_forecast}
\end{frame}

\begin{frame}{Forecast Confidence Intervals}
    \begin{cminipage}{0.95\textwidth}
    \vspace{-0.15cm}
    {\small
    \begin{block}{Constructing Intervals}
        \begin{itemize}\setlength{\itemsep}{0pt}
            \item For normally distributed errors, $(1-\alpha)$ confidence interval:
                $\hat{Y}_{i,T+h|T} \pm z_{\alpha/2} \sqrt{[\text{MSE}(\hat{\bY}_{T+h|T})]_{ii}}$
        \end{itemize}
    \end{block}

    \begin{alertblock}{Joint Confidence Regions}
        \begin{itemize}\setlength{\itemsep}{0pt}
            \item For multiple variables, use ellipsoids:
                $(\bY_{T+h} - \hat{\bY}_{T+h|T})' [\text{MSE}(\hat{\bY}_{T+h|T})]^{-1} (\bY_{T+h} - \hat{\bY}_{T+h|T}) \leq \chi^2_{K,\alpha}$
        \end{itemize}
    \end{alertblock}
    }

    {\footnotesize
    \begin{block}{Note}
        \begin{itemize}\setlength{\itemsep}{0pt}
            \item These assume known parameters. Bootstrap methods account for parameter uncertainty.
        \end{itemize}
    \end{block}
    }
    \end{cminipage}
\end{frame}

\begin{frame}{Out-of-Sample Evaluation: VAR vs.\ AR}
    \vspace{-0.2cm}
    {\footnotesize
        \begin{block}{General Methodology}
            \begin{itemize}\setlength{\itemsep}{0pt}
                \item Train / Test Split: split data into training + test; estimate on train, evaluate on test
                \item Why VAR vs.\ AR? AR ignores other variables; VAR exploits interdependencies
                \item Metric: $\text{RMSE} = \sqrt{\tfrac{1}{h}\sum e_i^2}$
            \end{itemize}
        \end{block}
        }
    \begin{center}
        \includegraphics[width=0.95\textwidth, height=0.50\textheight, keepaspectratio]{ch6_var_rolling_forecast.pdf}
    \end{center}
    \quantlet{TSA\_ch6\_var\_rolling\_forecast}{https://github.com/QuantLet/TSA/tree/main/TSA_ch6/TSA_ch6_var_rolling_forecast}
\end{frame}

\begin{frame}{Forecast Evaluation}
    \vspace{-2mm}
    {\footnotesize
    \begin{cminipage}{0.95\textwidth}
    \begin{block}{Out-of-Sample Evaluation}
        Split data: estimation sample (1 to $T_1$) and test sample ($T_1+1$ to $T$).
        Compute forecast errors: $e_{t+h} = Y_{t+h} - \hat{Y}_{t+h|t}$
    \end{block}
    \vspace{-0.1cm}
    \begin{block}{Common Metrics}
        \begin{itemize}\setlength{\itemsep}{0pt}
            \item \textbf{RMSE}: $\sqrt{\frac{1}{n}\sum e_{t+h}^2}$ \quad \textbf{MAE}: $\frac{1}{n}\sum |e_{t+h}|$ \quad \textbf{MAPE}: $\frac{100}{n}\sum \left|\frac{e_{t+h}}{Y_{t+h}}\right|$
        \end{itemize}
    \end{block}
    \vspace{-0.1cm}
    \begin{exampleblock}{Diebold-Mariano Test}
        Test whether VAR forecasts are significantly better than alternative:
        $DM = \frac{\bar{d}}{\sqrt{\hat{\sigma}_d^2/n}} \sim N(0,1)$ where $d_t = L(e_{1t}) - L(e_{2t})$ is the loss differential.
    \end{exampleblock}
    \end{cminipage}
    }
\end{frame}

%=============================================================================
% SECTION 8: PRACTICAL EXAMPLE
%=============================================================================
\section{Practical Example}

\begin{frame}{GDP and Unemployment: Quarterly Data}
    \vspace{-0.2cm}
    {\scriptsize
    \begin{block}{Observations}
        \begin{itemize}\setlength{\itemsep}{1pt}
            \item GDP growth and unemployment rate: Okun's Law, common cyclical patterns
            \item Bivariate system ideal for VAR analysis + Granger causality
        \end{itemize}
    \end{block}
    }
    \begin{center}
        \includegraphics[width=0.95\textwidth, height=0.50\textheight, keepaspectratio]{ch6_gdp_unemployment.pdf}
    \end{center}
    \hfill\quantlet{TSA\_ch6\_gdp\_unemployment}{https://github.com/QuantLet/TSA/tree/main/TSA_ch6/TSA_ch6_gdp_unemployment}
\end{frame}

\begin{frame}{Example: GDP and Unemployment}
    \begin{cminipage}{0.95\textwidth}
    \begin{block}{Okun's Law}
        \begin{itemize}\setlength{\itemsep}{0pt}
            \item Negative relationship between GDP growth and unemployment:
                $$\Delta U_t \approx -\beta (\Delta Y_t - \bar{g})$$
                \vspace{-0.15cm}
            \item $\bar{g}$ = trend GDP growth, $\beta \approx 0.4$
        \end{itemize}
    \end{block}

    \vspace{0.15cm}

    \begin{exampleblock}{VAR Analysis Questions}
        \begin{enumerate}\setlength{\itemsep}{0pt}
            \item Does GDP growth Granger-cause unemployment changes?
            \item Does unemployment Granger-cause GDP growth?
            \item How do shocks propagate between variables?
        \end{enumerate}
    \end{exampleblock}
    \end{cminipage}
\end{frame}

\begin{frame}{Estimated VAR Results}
    \vspace{-0.2cm}
    {\footnotesize
    \begin{itemize}\setlength{\itemsep}{0pt}
            \item Estimated coefficients with standard errors and t-statistics
            \item Information criteria values for model comparison
            \item Model diagnostics summary (residual tests)
        \end{itemize}
    }
    \begin{center}
        \includegraphics[width=0.95\textwidth, height=0.50\textheight, keepaspectratio]{ch6_var_results.pdf}
    \end{center}
    \quantlet{TSA\_ch6\_var\_results}{https://github.com/QuantLet/TSA/tree/main/TSA_ch6/TSA_ch6_var_results}
\end{frame}

\begin{frame}{VAR Workflow}
    \begin{cminipage}{0.95\textwidth}
    \vspace{-0.2cm}
    {\small
    \begin{enumerate}\setlength{\itemsep}{0pt}
        \item \textbf{Data preparation}
            \begin{itemize}\setlength{\itemsep}{0pt}
                \item Check for stationarity (unit root tests)
                \item Transform if necessary (differences, logs)
            \end{itemize}

        \vspace{0.2cm}
        \item \textbf{Lag selection}
            \begin{itemize}\setlength{\itemsep}{0pt}
                \item Use AIC, BIC, HQ criteria
                \item Check residual autocorrelation
            \end{itemize}

        \vspace{0.2cm}
        \item \textbf{Estimation}
            \begin{itemize}\setlength{\itemsep}{0pt}
                \item OLS equation by equation
                \item Check stability (eigenvalues)
            \end{itemize}

        \vspace{0.2cm}
        \item \textbf{Analysis}
            \begin{itemize}\setlength{\itemsep}{0pt}
                \item Granger causality tests
                \item Impulse response functions
                \item Variance decomposition
            \end{itemize}

        \vspace{0.2cm}
        \item \textbf{Forecasting}
    \end{enumerate}
    }
    \end{cminipage}
\end{frame}

\begin{frame}{Granger Causality Results}
    {\small
    \begin{cminipage}{0.95\textwidth}
    \begin{block}{Test Results: GDP and Unemployment}
        \begin{center}
        \begin{tabular}{lcccc}
            \toprule
            Null Hypothesis & F-statistic & df & p-value & Decision \\
            \midrule
            GDP $\not\to$ Unemployment & 8.42 & (2, 95) & 0.0004 & Reject \\
            Unemployment $\not\to$ GDP & 2.15 & (2, 95) & 0.1220 & Fail to Reject \\
            \bottomrule
        \end{tabular}
        \end{center}
    \end{block}

    \begin{block}{Interpretation}
        \begin{itemize}\setlength{\itemsep}{0pt}
            \item GDP growth Granger-causes unemployment (consistent with Okun's Law)
            \item Unemployment does not significantly Granger-cause GDP
            \item Evidence of \textbf{unidirectional} causality: GDP $\to$ Unemployment
        \end{itemize}
    \end{block}
    \end{cminipage}
    }
\end{frame}

\begin{frame}{Monetary Policy VAR: IRFs}
    \vspace{-0.2cm}
    {\footnotesize
    \begin{itemize}\setlength{\itemsep}{0pt}
            \item Contractionary monetary policy shock (interest rate increase)
            \item Output decreases with peak effect after 4-6 quarters (``long and variable lags'')
            \item Inflation responds more slowly, decreasing after output
        \end{itemize}
    }
    \begin{center}
        \includegraphics[width=0.95\textwidth, height=0.50\textheight, keepaspectratio]{ch6_monetary_irf.pdf}
    \end{center}
    \quantlet{TSA\_ch6\_monetary\_irf}{https://github.com/QuantLet/TSA/tree/main/TSA_ch6/TSA_ch6_monetary_irf}
\end{frame}

\begin{frame}{Example: Monetary Policy Analysis}
    \begin{cminipage}{0.95\textwidth}
    \begin{block}{Three-Variable VAR}
        Study the monetary transmission mechanism with:
        \begin{itemize}\setlength{\itemsep}{0pt}
            \item $Y_1$: Output gap (GDP deviation from trend)
            \item $Y_2$: Inflation rate
            \item $Y_3$: Interest rate (policy instrument)
        \end{itemize}
    \end{block}

    \vspace{0.15cm}

    \begin{exampleblock}{Key Questions}
        \begin{enumerate}\setlength{\itemsep}{0pt}
            \item How does an interest rate shock affect output and inflation?
            \item How long until the maximum effect is felt?
            \item What fraction of output variance is due to monetary shocks?
        \end{enumerate}
    \end{exampleblock}
    \end{cminipage}
\end{frame}

%=============================================================================
% CASE STUDY: GDP AND UNEMPLOYMENT
%=============================================================================
\section{Case Study: GDP and Unemployment}

\begin{frame}{Case Study: The Relationship between GDP and Unemployment}
    \vspace{-0.2cm}
    {\scriptsize
    \begin{block}{Data}
        \begin{itemize}\setlength{\itemsep}{0pt}
            \item Real US data (FRED, 1990--2024): GDP Growth and Unemployment Rate ($T = 140$ quarters)
            \item Visible negative correlation between series (Okun's Law); bidirectional dynamics
        \end{itemize}
    \end{block}
    }
    \begin{center}
        \includegraphics[width=0.95\textwidth, height=0.50\textheight, keepaspectratio]{ch6_case_raw_data.pdf}
    \end{center}
    \quantlet{TSA\_ch6\_case\_raw\_data}{https://github.com/QuantLet/TSA/tree/main/TSA_ch6/TSA_ch6_case_raw_data}
\end{frame}

\begin{frame}{Step 1: Preliminary Analysis}
    \vspace{-0.2cm}
    {\scriptsize
    \begin{block}{Results}
        \begin{itemize}\setlength{\itemsep}{0pt}
            \item GDP: ACF decays quickly $\Rightarrow$ stationary; Unemployment: persistent ACF (ADF: $p = 0.02$)
            \item Negative GDP--Unemployment correlation ($\rho = -0.17$); cross-correlation suggests bidirectional relationships
        \end{itemize}
    \end{block}
    }
    \begin{center}
        \includegraphics[width=0.95\textwidth, height=0.50\textheight, keepaspectratio]{ch6_case_stationarity.pdf}
    \end{center}
    \quantlet{TSA\_ch6\_case\_stationarity}{https://github.com/QuantLet/TSA/tree/main/TSA_ch6/TSA_ch6_case_stationarity}
\end{frame}

\begin{frame}{Step 2: VAR Order Selection}
    \vspace{-0.2cm}
    {\scriptsize
    \begin{block}{Results}
        \begin{itemize}\setlength{\itemsep}{0pt}
            \item AIC and BIC criteria suggest VAR(2); trade-off between complexity and fit
        \end{itemize}
    \end{block}
    }
    \begin{center}
        \includegraphics[width=0.95\textwidth, height=0.50\textheight, keepaspectratio]{ch6_case_lag_selection.pdf}
    \end{center}
    \quantlet{TSA\_ch6\_case\_lag\_selection}{https://github.com/QuantLet/TSA/tree/main/TSA_ch6/TSA_ch6_case_lag_selection}
\end{frame}

\begin{frame}{Step 3: Granger Causality Test}
    \vspace{-0.2cm}
    {\scriptsize
    \begin{block}{Results}
        \begin{itemize}\setlength{\itemsep}{0pt}
            \item GDP $\Rightarrow$ Unemployment: $F = 17.35$, $p < 0.001$ $\Rightarrow$ GDP ``Granger-causes'' Unemployment
            \item Unemployment $\Rightarrow$ GDP: $F = 38.93$, $p < 0.001$ $\Rightarrow$ bidirectional causality (Okun's Law)
        \end{itemize}
    \end{block}
    }
    \begin{center}
        \includegraphics[width=0.95\textwidth, height=0.50\textheight, keepaspectratio]{ch6_case_granger.pdf}
    \end{center}
    \quantlet{TSA\_ch6\_case\_granger}{https://github.com/QuantLet/TSA/tree/main/TSA_ch6/TSA_ch6_case_granger}
\end{frame}

\begin{frame}{Step 4: Impulse Response Functions (IRF)}
    \vspace{-0.2cm}
    {\scriptsize
    \begin{block}{IRF Results}
        \begin{itemize}\setlength{\itemsep}{0pt}
            \item GDP shock $\Rightarrow$ persistent negative effect on unemployment (Okun's Law, $>$20 quarters)
            \item Unemployment shock $\Rightarrow$ short-lived positive effect on GDP (recovery, 2--3 quarters)
        \end{itemize}
    \end{block}
    }
    \begin{center}
        \includegraphics[width=0.95\textwidth, height=0.50\textheight, keepaspectratio]{ch6_case_irf.pdf}
    \end{center}
    \quantlet{TSA\_ch6\_case\_irf}{https://github.com/QuantLet/TSA/tree/main/TSA_ch6/TSA_ch6_case_irf}
\end{frame}

\begin{frame}{Step 5: Variance Decomposition (FEVD)}
    \vspace{-0.2cm}
    {\scriptsize
    \begin{block}{FEVD Results}
        \begin{itemize}\setlength{\itemsep}{0pt}
            \item GDP: $\sim$65\% of variance explained by own shocks, $\sim$35\% by Unemployment shocks
            \item Unemployment: dominated by GDP shocks ($\sim$65\% at $h=1$, increasing to $\sim$92\% at $h=20$)
        \end{itemize}
    \end{block}
    }
    \begin{center}
        \includegraphics[width=0.95\textwidth, height=0.50\textheight, keepaspectratio]{ch6_case_fevd.pdf}
    \end{center}
    \quantlet{TSA\_ch6\_case\_fevd}{https://github.com/QuantLet/TSA/tree/main/TSA_ch6/TSA_ch6_case_fevd}
\end{frame}

\begin{frame}{Step 6: Residual Diagnostics}
    \vspace{-0.2cm}
    {\scriptsize
    \begin{block}{Diagnostics}
        \begin{itemize}\setlength{\itemsep}{0pt}
            \item Residuals show no significant autocorrelation (ACF within bounds)
            \item Significant non-normality (JB rejected) $\Rightarrow$ due to extreme COVID-19 values
        \end{itemize}
    \end{block}
    }
    \begin{center}
        \includegraphics[width=0.95\textwidth, height=0.50\textheight, keepaspectratio]{ch6_case_diagnostics.pdf}
    \end{center}
    \quantlet{TSA\_ch6\_case\_diagnostics}{https://github.com/QuantLet/TSA/tree/main/TSA_ch6/TSA_ch6_case_diagnostics}
\end{frame}

\begin{frame}{Step 7: VAR Forecasting}
    \vspace{-0.2cm}
    {\scriptsize
    \begin{block}{Forecast Results}
        \begin{itemize}\setlength{\itemsep}{0pt}
            \item 12-quarter forecast; VAR captures interdependencies between series
            \item Forecasts converge to long-run equilibrium values
        \end{itemize}
    \end{block}
    }
    \begin{center}
        \includegraphics[width=0.95\textwidth, height=0.50\textheight, keepaspectratio]{ch6_case_forecast.pdf}
    \end{center}
    \quantlet{TSA\_ch6\_case\_forecast}{https://github.com/QuantLet/TSA/tree/main/TSA_ch6/TSA_ch6_case_forecast}
\end{frame}

\begin{frame}{Step 8: Rolling Forecast -- VAR vs AR}
    \begin{cminipage}{0.95\textwidth}
    {\small
    \begin{block}{Procedure}
        \begin{itemize}\setlength{\itemsep}{0pt}
            \item Recursive Train/Test: $W = 80$, re-estimation at each step; AR(2) vs VAR(2) models
            \item Expanding RMSE: $\text{RMSE}_t = \sqrt{\frac{1}{t}\sum_{s=1}^{t} e_s^2}$; 95\% confidence intervals
            \item Mixed results:
                \begin{itemize}\setlength{\itemsep}{0pt}
                    \item Unemployment --- VAR $-$10\% RMSE vs AR (information from GDP helps)
                    \item GDP --- VAR $+$6\% RMSE vs AR (information from unemployment does not help)
                \end{itemize}
        \end{itemize}
    \end{block}
    }
    \end{cminipage}
\end{frame}

\begin{frame}{Step 8: Rolling Forecast -- VAR vs AR}
    \begin{center}
        \includegraphics[width=0.95\textwidth, height=0.78\textheight, keepaspectratio]{ch6_case_rolling_forecast.pdf}
    \end{center}
    \quantlet{TSA\_ch6\_case\_rolling\_forecast}{https://github.com/QuantLet/TSA/tree/main/TSA_ch6/TSA_ch6_case_rolling_forecast}
\end{frame}

\begin{frame}{Step 9: Out-of-Sample Comparison -- AR vs VAR}
    \vspace{-0.2cm}
    {\footnotesize
    \begin{block}{Methodology}
            \begin{itemize}\setlength{\itemsep}{0pt}
                \item Recursive forecast: estimate on $[1, \ldots, t]$, forecast $\hat{y}_{t+1|t}$; AR(2) vs VAR(2) models
                \item Expanding RMSE: $\text{RMSE}_t = \sqrt{\frac{1}{t}\sum_{s=1}^{t} e_s^2}$
                \item VAR reduces RMSE for Unemployment ($\sim$10\%), not for GDP
            \end{itemize}
        \end{block}
    }
    \begin{center}
        \includegraphics[width=0.95\textwidth, height=0.50\textheight, keepaspectratio]{ch6_case_oos_comparison.pdf}
    \end{center}
    \quantlet{TSA\_ch6\_case\_oos\_comparison}{https://github.com/QuantLet/TSA/tree/main/TSA_ch6/TSA_ch6_case_oos_comparison}
\end{frame}

%=============================================================================
\section{AI Use Case}
%=============================================================================

\begin{frame}{AI Exercise: Critical Thinking}
    \begin{cminipage}{0.95\textwidth}
    \vspace{-3mm}
    \begin{block}{\footnotesize Prompt to test in ChatGPT / Claude / Copilot}
        {\footnotesize
        ``Download from FRED: quarterly US Real GDP growth rate (A191RL1Q225SBEA) and monthly unemployment rate (UNRATE, aggregated to quarterly) for 2000-Q1 to 2024-Q4 (100 observations). Test Granger causality in both directions, estimate a VAR model, and compute orthogonalized impulse response functions. Give me complete Python code.''
        }
    \end{block}
    \vspace{-2mm}
    {\footnotesize
    \textbf{Exercise}:
    \begin{enumerate}\setlength{\itemsep}{0pt}
        \item Run the prompt in an LLM of your choice and critically analyze the response.
        \item Does it test stationarity of each variable before estimating the VAR?
        \item How does it select the lag order? Does it compare AIC, BIC, HQIC?
        \item Are impulse response functions orthogonalized? Does it discuss Cholesky ordering?
        \item Does it check the stability condition (eigenvalues inside the unit circle)?
    \end{enumerate}
    }
    \vspace{-2mm}
    \begin{alertblock}{}
        {\footnotesize \textbf{Warning}: AI-generated code may run without errors and look professional. \textit{That does not mean it is correct.}}
    \end{alertblock}
    \end{cminipage}
\end{frame}

%=============================================================================
% SECTION 9: SUMMARY
%=============================================================================
\section{Summary}

\begin{frame}{Key Takeaways}
    \vspace{-0.2cm}
    {\footnotesize
    \begin{cminipage}{0.95\textwidth}
    \begin{block}{VAR Models}
        \begin{itemize}\setlength{\itemsep}{0pt}
            \item Model \textbf{multiple} time series jointly
            \item Each variable depends on its own lags AND lags of other variables
            \item Estimated by OLS equation by equation; requires stationarity
        \end{itemize}
    \end{block}

    \begin{block}{Granger Causality}
        \begin{itemize}\setlength{\itemsep}{0pt}
            \item Tests whether $X$ helps predict $Y$ beyond $Y$'s own history
            \item \textbf{Not} the same as true causality; F-test on coefficient restrictions
        \end{itemize}
    \end{block}

    \begin{block}{IRF and FEVD}
        \begin{itemize}\setlength{\itemsep}{0pt}
            \item IRF: How shocks propagate through the system
            \item FEVD: What proportion of variance is due to each shock
            \item Both depend on variable ordering (Cholesky decomposition)
        \end{itemize}
    \end{block}
    \end{cminipage}
    }
\end{frame}

\begin{frame}{VAR Model Selection Checklist}
    \begin{cminipage}{0.95\textwidth}
    \vspace{-0.2cm}
    {\footnotesize
    \begin{columns}[T]
    \begin{column}{0.48\textwidth}
    \begin{block}{Before Estimation}
        \begin{itemize}\setlength{\itemsep}{0pt}
            \item[$\square$] Test for unit roots
            \item[$\square$] Transform if needed
            \item[$\square$] Check for breaks
        \end{itemize}
    \end{block}

    \begin{block}{Model Specification}
        \begin{itemize}\setlength{\itemsep}{0pt}
            \item[$\square$] Select lag order (AIC/BIC)
            \item[$\square$] Estimate VAR by OLS
            \item[$\square$] Check stability
        \end{itemize}
    \end{block}
    \end{column}
    \begin{column}{0.48\textwidth}
    \begin{block}{Post-Estimation}
        \begin{itemize}\setlength{\itemsep}{0pt}
            \item[$\square$] Test autocorrelation
            \item[$\square$] Test ARCH effects
            \item[$\square$] Test normality
            \item[$\square$] Compute IRFs, FEVDs
        \end{itemize}
    \end{block}
    \end{column}
    \end{columns}
    }
    \end{cminipage}
\end{frame}

\begin{frame}{Common Mistakes to Avoid}
    \begin{cminipage}{0.95\textwidth}
    \begin{alertblock}{Pitfalls in VAR Analysis}
        \begin{enumerate}\setlength{\itemsep}{0pt}
            \item \textbf{Ignoring non-stationarity}: Always test for unit roots first
            \item \textbf{Overfitting}: Too many lags $\Rightarrow$ poor forecasts
            \item \textbf{Wrong ordering}: Cholesky results depend on variable order
            \item \textbf{Confusing correlation with causation}: Granger causality $\neq$ true causality
            \item \textbf{Ignoring parameter uncertainty}: Use bootstrap CIs for IRFs
            \item \textbf{Short samples}: VAR requires many observations ($T > 50$)
        \end{enumerate}
    \end{alertblock}
    \end{cminipage}
\end{frame}

\begin{frame}{What's Next?}
    \begin{cminipage}{0.95\textwidth}
    \begin{alertblock}{Topics for Further Study}
        \begin{itemize}\setlength{\itemsep}{0pt}
            \item \textbf{Cointegration}: Long-run relationships between non-stationary variables
            \item \textbf{VECM}: Error correction models for cointegrated systems
            \item \textbf{Structural VAR}: Imposing economic theory restrictions
            \item \textbf{Panel VAR}: VAR for panel data
            \item \textbf{Bayesian VAR}:
                \begin{itemize}\setlength{\itemsep}{0pt}
                    \item Shrinkage priors for high-dimensional systems
                \end{itemize}
            \end{itemize}
    \end{alertblock}

    \vspace{0.25cm}

    \begin{center}
        \Large\textcolor{MainBlue}{Questions?}
    \end{center}
    \end{cminipage}
\end{frame}

%=============================================================================
% SECTION 10: QUIZ
%=============================================================================
\section{Quiz}

\begin{frame}{Question 1}
    \begin{cminipage}{0.95\textwidth}
    \begin{alertblock}{Question}
        \begin{itemize}\setlength{\itemsep}{0pt}
            \item For a VAR(1) model with coefficient matrix $\bA = \begin{pmatrix} 0.8 & 0.3 \\ 0.1 & 0.5 \end{pmatrix}$, is the model stable?
        \end{itemize}
    \end{alertblock}

    \vspace{0.3cm}

    \begin{block}{Answer Choices}

        \textcolor{MainBlue}{\textbf{(A)}} Yes, because all diagonal elements are less than 1\\[3pt]

        \textcolor{MainBlue}{\textbf{(B)}} Yes, because all eigenvalues are inside the unit circle\\[3pt]

        \textcolor{MainBlue}{\textbf{(C)}} No, because the sum of coefficients exceeds 1\\[3pt]

        \textcolor{MainBlue}{\textbf{(D)}} Cannot be determined without knowing $\bSigma$

    \end{block}
    \end{cminipage}
\end{frame}

\begin{frame}{Question 1: Answer}
    \begin{cminipage}{0.95\textwidth}
    \vspace{-0.2cm}
    \begin{center}
        \includegraphics[width=0.98\textwidth, height=0.58\textheight, keepaspectratio]{ch6_quiz1_var_stability.pdf}
    \end{center}
    \vspace{-3mm}
    {\small
    \begin{exampleblock}{Answer: (B)}
    \begin{itemize}\setlength{\itemsep}{0pt}
        \item $\lambda_1 = 0.879$, $\lambda_2 = 0.421$ --- both $|\lambda| < 1$ $\Rightarrow$ Stable!
    \end{itemize}
    \end{exampleblock}
    }
    \hfill\quantlet{TSA\_ch6\_quiz1\_var\_stability}{https://github.com/QuantLet/TSA/tree/main/TSA_ch6/TSA_ch6_quiz1_var_stability}
    \end{cminipage}
\end{frame}

\begin{frame}{Question 2}
    \begin{cminipage}{0.95\textwidth}
    \begin{alertblock}{Question}
        \begin{itemize}\setlength{\itemsep}{0pt}
            \item If $X$ Granger-causes $Y$ at the 5\% significance level, which of the following statements is TRUE?
        \end{itemize}
    \end{alertblock}

    \vspace{0.3cm}

    \begin{block}{Answer Choices}

        \textcolor{MainBlue}{\textbf{(A)}} $X$ is the economic cause of $Y$\\[3pt]

        \textcolor{MainBlue}{\textbf{(B)}} Past values of $X$ contain useful information for predicting $Y$\\[3pt]

        \textcolor{MainBlue}{\textbf{(C)}} $Y$ cannot Granger-cause $X$\\[3pt]

        \textcolor{MainBlue}{\textbf{(D)}} The correlation between $X$ and $Y$ is positive

    \end{block}
    \end{cminipage}
\end{frame}

\begin{frame}{Question 2: Answer}
    \begin{cminipage}{0.95\textwidth}
    \vspace{-0.2cm}
    \begin{center}
        \includegraphics[width=0.98\textwidth, height=0.58\textheight, keepaspectratio]{ch6_quiz2_granger_causality.pdf}
    \end{center}
    \vspace{-3mm}
    {\small
    \begin{exampleblock}{Answer: (B)}
    \begin{itemize}\setlength{\itemsep}{0pt}
        \item Granger causality = predictive content, not true economic causation. Past X helps predict Y.
    \end{itemize}
    \end{exampleblock}
    }
    \hfill\quantlet{TSA\_ch6\_quiz2\_granger\_causality}{https://github.com/QuantLet/TSA/tree/main/TSA_ch6/TSA_ch6_quiz2_granger_causality}
    \end{cminipage}
\end{frame}

\begin{frame}{Question 3}
    \begin{cminipage}{0.95\textwidth}
    \begin{alertblock}{Question}
        \begin{itemize}\setlength{\itemsep}{0pt}
            \item In a VAR with Cholesky-identified IRFs, what does the ordering of variables determine?
        \end{itemize}
    \end{alertblock}

    \vspace{0.3cm}

    \begin{block}{Answer Choices}

        \textcolor{MainBlue}{\textbf{(A)}} The magnitude of the impulse responses\\[3pt]

        \textcolor{MainBlue}{\textbf{(B)}} The speed at which shocks die out\\[3pt]

        \textcolor{MainBlue}{\textbf{(C)}} Which variables can respond contemporaneously to which shocks\\[3pt]

        \textcolor{MainBlue}{\textbf{(D)}} The number of lags in the VAR

    \end{block}
    \end{cminipage}
\end{frame}

\begin{frame}{Question 3: Answer}
    \begin{cminipage}{0.95\textwidth}
    \vspace{-0.2cm}
    \begin{center}
        \includegraphics[width=0.98\textwidth, height=0.58\textheight, keepaspectratio]{ch6_quiz3_cholesky_ordering.pdf}
    \end{center}
    \vspace{-3mm}
    {\small
    \begin{exampleblock}{Answer: (C)}
    \begin{itemize}\setlength{\itemsep}{0pt}
        \item Ordering determines which variables respond immediately to which shocks.
    \end{itemize}
    \end{exampleblock}
    }
    \hfill\quantlet{TSA\_ch6\_quiz3\_cholesky\_ordering}{https://github.com/QuantLet/TSA/tree/main/TSA_ch6/TSA_ch6_quiz3_cholesky_ordering}
    \end{cminipage}
\end{frame}

\begin{frame}{Question 4}
    \begin{cminipage}{0.95\textwidth}
    \begin{alertblock}{Question}
        \begin{itemize}\setlength{\itemsep}{0pt}
            \item For a bivariate VAR(1), how many parameters need to be estimated (excluding the error covariance matrix)?
        \end{itemize}
    \end{alertblock}

    \vspace{0.3cm}

    \begin{block}{Answer Choices}

        \textcolor{MainBlue}{\textbf{(A)}} 4\\[3pt]

        \textcolor{MainBlue}{\textbf{(B)}} 6\\[3pt]

        \textcolor{MainBlue}{\textbf{(C)}} 8\\[3pt]

        \textcolor{MainBlue}{\textbf{(D)}} 10

    \end{block}
    \end{cminipage}
\end{frame}

\begin{frame}{Question 4: Answer}
    \begin{cminipage}{0.95\textwidth}
    \vspace{-0.3cm}
    {\small
    \begin{exampleblock}{Answer: (B)}
    \begin{itemize}\setlength{\itemsep}{0pt}
        \item 6 parameters
    \end{itemize}
    \end{exampleblock}
    }
    \vspace{-2mm}
    {\small
    \begin{block}{Detailed Count}
        VAR(1) with $K=2$ variables:
        $$\begin{pmatrix} Y_{1t} \\ Y_{2t} \end{pmatrix} = \underbrace{\begin{pmatrix} c_1 \\ c_2 \end{pmatrix}}_{\text{2 params}} + \underbrace{\begin{pmatrix} a_{11} & a_{12} \\ a_{21} & a_{22} \end{pmatrix}}_{\text{4 params}} \begin{pmatrix} Y_{1,t-1} \\ Y_{2,t-1} \end{pmatrix} + \begin{pmatrix} \varepsilon_{1t} \\ \varepsilon_{2t} \end{pmatrix}$$

        \begin{itemize}\setlength{\itemsep}{0pt}
            \item Constant vector $\mathbf{c}$: $K = 2$ parameters
            \item Coefficient matrix $\bA$: $K^2 = 4$ parameters
            \item Total: $K + K^2 = 2 + 4 = 6$ parameters
        \end{itemize}
    \end{block}
    }
    {\footnotesize
    \begin{block}{General Formula}
        VAR($p$) with $K$ variables: $K + pK^2$ parameters (excluding $\bSigma$)
    \end{block}
    }
    \end{cminipage}
\end{frame}

\begin{frame}{Question 5}
    \begin{cminipage}{0.95\textwidth}
    \begin{alertblock}{Question}
        \begin{itemize}\setlength{\itemsep}{0pt}
            \item What does FEVD$_{12}(h) = 0.35$ mean?
        \end{itemize}
    \end{alertblock}

    \vspace{0.3cm}

    \begin{block}{Answer Choices}

        \textcolor{MainBlue}{\textbf{(A)}} 35\% of variable 1's total variance is explained by variable 2\\[3pt]

        \textcolor{MainBlue}{\textbf{(B)}} 35\% of variable 1's $h$-step forecast error variance is due to shocks to variable 2\\[3pt]

        \textcolor{MainBlue}{\textbf{(C)}} The correlation between variables 1 and 2 at lag $h$ is 0.35\\[3pt]

        \textcolor{MainBlue}{\textbf{(D)}} Variable 2 explains 35\% of the impulse response of variable 1

    \end{block}
    \end{cminipage}
\end{frame}

\begin{frame}{Question 5: Answer}
    \begin{cminipage}{0.95\textwidth}
    \vspace{-0.2cm}
    \begin{center}
        \includegraphics[width=0.98\textwidth, height=0.58\textheight, keepaspectratio]{ch6_quiz5_fevd.pdf}
    \end{center}
    \vspace{-3mm}
    {\small
    \begin{exampleblock}{Answer: (B)}
    \begin{itemize}\setlength{\itemsep}{0pt}
        \item 35\% of variable 1's $h$-step forecast error variance is due to shocks from variable 2.
    \end{itemize}
    \end{exampleblock}
    }
    \hfill\quantlet{TSA\_ch6\_quiz5\_fevd}{https://github.com/QuantLet/TSA/tree/main/TSA_ch6/TSA_ch6_quiz5_fevd}
    \end{cminipage}
\end{frame}

%=============================================================================
% KEY FORMULAS SUMMARY
%=============================================================================
\section{Key Formulas}

\begin{frame}{Key Formulas -- Summary}
    \vspace{-0.2cm}
    {\small
    \begin{columns}[T]
        \begin{column}{0.48\textwidth}
            \begin{block}{VAR(p) Model}
                \begin{itemize}\setlength{\itemsep}{0pt}
                    \item $\bY_t = \mathbf{c} + \sum_{i=1}^{p}\bA_i\bY_{t-i} + \bepsilon_t$
                    \item {\footnotesize $\bepsilon_t \sim \mathcal{N}(\mathbf{0}, \bSigma)$, i.i.d.}
                \end{itemize}
            \end{block}

            \begin{block}{Granger Causality}
                \begin{itemize}\setlength{\itemsep}{0pt}
                    \item $H_0$: $X$ does not Granger-cause $Y$
                    \item {\footnotesize F or Wald test on lag coefficients of $X$}
                \end{itemize}
            \end{block}

            \begin{block}{Lag Selection}
                \begin{itemize}\setlength{\itemsep}{0pt}
                    \item $\text{AIC} = \ln|\hat{\bSigma}| + \frac{2pK^2}{T}$
                    \item $\text{BIC} = \ln|\hat{\bSigma}| + \frac{pK^2\ln T}{T}$
                \end{itemize}
            \end{block}
        \end{column}

        \begin{column}{0.48\textwidth}
            \begin{block}{Impulse Response Functions}
                \begin{itemize}\setlength{\itemsep}{0pt}
                    \item $\bY_{t+h} = \sum_{i=0}^{\infty}\boldsymbol{\Phi}_i\bepsilon_{t+h-i}$
                    \item {\footnotesize $\boldsymbol{\Phi}_i$ = multipliers at horizon $i$}
                \end{itemize}
            \end{block}

            \begin{block}{FEVD}
                \begin{itemize}\setlength{\itemsep}{0pt}
                    \item $\text{FEVD}_{jk}(h) = \frac{\sum_{i=0}^{h-1}(\mathbf{e}'_j\boldsymbol{\Phi}_i\mathbf{P}\mathbf{e}_k)^2}{\sum_{i=0}^{h-1}\mathbf{e}'_j\boldsymbol{\Phi}_i\bSigma\boldsymbol{\Phi}'_i\mathbf{e}_j}$
                    \item {\footnotesize Contribution of shock $k$ to variance of $j$}
                \end{itemize}
            \end{block}

            \begin{block}{VAR Stationarity}
                \begin{itemize}\setlength{\itemsep}{0pt}
                    \item All eigenvalues of $\bA$ inside the unit circle
                \end{itemize}
            \end{block}
        \end{column}
    \end{columns}
    }
\end{frame}

%=============================================================================
% BIBLIOGRAPHY
%=============================================================================
\section{Bibliography}

\begin{frame}{Bibliography I}
    \begin{block}{Fundamental works on VAR and causality}
        {\small
        \begin{itemize}\setlength{\itemsep}{0pt}
            \item Sims, C.A. (1980). Macroeconomics and Reality, \textit{Econometrica}, 48(1), 1--48.
            \item Granger, C.W.J. (1969). Investigating Causal Relations by Econometric Models and Cross-Spectral Methods, \textit{Econometrica}, 37(3), 424--438.
            \item Toda, H.Y., \& Yamamoto, T. (1995). Statistical Inference in Vector Autoregressions with Possibly Integrated Processes, \textit{Journal of Econometrics}, 66(1-2), 225--250.
        \end{itemize}
        }
    \end{block}

    \begin{exampleblock}{VAR Textbooks}
        {\small
        \begin{itemize}\setlength{\itemsep}{0pt}
            \item L\"utkepohl, H. (2005). \textit{New Introduction to Multiple Time Series Analysis}, Springer.
            \item Kilian, L., \& L\"utkepohl, H. (2017). \textit{Structural Vector Autoregressive Analysis}, Cambridge University Press.
        \end{itemize}
        }
    \end{exampleblock}
\end{frame}

\begin{frame}{Bibliography II}
    \begin{block}{Impulse response functions and variance decomposition}
        {\small
        \begin{itemize}\setlength{\itemsep}{0pt}
            \item Pesaran, H.H., \& Shin, Y. (1998). Generalized Impulse Response Analysis in Linear Multivariate Models, \textit{Economics Letters}, 58(1), 17--29.
            \item Hamilton, J.D. (1994). \textit{Time Series Analysis}, Princeton University Press.
            \item Tsay, R.S. (2014). \textit{Multivariate Time Series Analysis with R and Financial Applications}, Wiley.
        \end{itemize}
        }
    \end{block}

    \begin{exampleblock}{Online resources and code}
        {\small
        \begin{itemize}\setlength{\itemsep}{0pt}
            \item \textbf{Quantlet}: \url{https://quantlet.com} -- Code platform for quantitative methods
            \item \textbf{Quantinar}: \url{https://quantinar.com} -- Learning platform for quantitative methods
            \item \textbf{GitHub TSA}: \url{https://github.com/QuantLet/TSA/tree/main/TSA_ch6} -- Python code for this chapter
        \end{itemize}
        }
    \end{exampleblock}
\end{frame}

\begin{frame}{}
    \centering
    \Huge\textcolor{MainBlue}{Thank You!}

    \vspace{1cm}

    \Large Questions?

    \vspace{0.8cm}

    \normalsize

    Course materials available at: \url{https://danpele.github.io/Time-Series-Analysis/}

    \vspace{0.2cm}

    \href{https://quantlet.com}{\raisebox{-0.15em}{\includegraphics[height=0.8em]{ql_logo.png}} Quantlet} \hspace{0.5cm}
    \href{https://quantinar.com}{\raisebox{-0.15em}{\includegraphics[height=0.8em]{qr_logo.png}} Quantinar}
\end{frame}

\end{document}