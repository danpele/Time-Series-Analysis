% Chapter 1: Introduction to Time Series Analysis
% Harvard-quality academic presentation
% Bachelor program, Bucharest University of Economic Studies

\documentclass[9pt, aspectratio=169, t]{beamer}
\input{preamble}
\subtitle{Chapter 1: Stochastic Processes and Stationarity}

\begin{document}

% Title page (no header/footer)
{
\setbeamertemplate{headline}{}
\setbeamertemplate{footline}{}
\begin{frame}
    \titlepage
\end{frame}
}

%=============================================================================
% COURSE OBJECTIVES
%=============================================================================
\begin{frame}{Learning Objectives}
    \begin{block}{By the end of this chapter, you will be able to:}
    \begin{enumerate}\setlength{\itemsep}{0pt}
        \item[\textcolor{MainBlue}{\textbf{1.}}] \textbf{Define} stochastic processes and understand their properties
        \item[\textcolor{MainBlue}{\textbf{2.}}] \textbf{Distinguish} between strict and weak (covariance) stationarity
        \item[\textcolor{MainBlue}{\textbf{3.}}] \textbf{Identify} white noise and random walk processes
        \item[\textcolor{MainBlue}{\textbf{4.}}] \textbf{Compute} and interpret ACF and PACF
        \item[\textcolor{MainBlue}{\textbf{5.}}] \textbf{Apply} the lag operator and differencing
        \item[\textcolor{MainBlue}{\textbf{6.}}] \textbf{Conduct} stationarity tests (ADF, KPSS)
        \item[\textcolor{MainBlue}{\textbf{7.}}] \textbf{Analyze} financial time series data
        \item[\textcolor{MainBlue}{\textbf{8.}}] \textbf{Distinguish} between unit root and trend-stationary processes
    \end{enumerate}
    \end{block}
\end{frame}

%=============================================================================
% TABLE OF CONTENTS
%=============================================================================
\begin{frame}{Outline}
    \setbeamertemplate{section in toc}{\color{MainBlue}$\boxdot$~\inserttocsection}
    \tableofcontents
\end{frame}

%=============================================================================
% SECTION: MOTIVATION
%=============================================================================
\section{Motivation}

\begin{frame}{Examples: stationary vs.\ non-stationary series}
    \vspace{-0.2cm}
    \begin{center}
        \includegraphics[width=\textwidth, height=0.55\textheight, keepaspectratio]{ch1_stationary_nonstationary_examples.pdf}
    \end{center}
    \vspace{-3mm}
    {\footnotesize
    \begin{block}{Observations}
    \begin{itemize}\setlength{\itemsep}{0pt}
        \item \textbf{Prices} (left) are non-stationary: trend, the mean changes over time
        \item \textbf{Returns} (right) are stationary: mean $\approx 0$, approximately constant variance
        \item Log returns: $r_t = \ln P_t - \ln P_{t-1}$ $\Rightarrow$ non-stationary $\to$ stationary
    \end{itemize}
    \end{block}
    }
    \hfill\quantlet{TSA\_ch1\_stationarity}{https://github.com/QuantLet/TSA/tree/main/TSA_ch1/TSA_ch1_stationarity}
\end{frame}


%=============================================================================
% SECTION 1: STOCHASTIC PROCESSES
%=============================================================================
\section{Stochastic Processes}

\begin{frame}{Stochastic process: visual illustration}
    \vspace{-0.2cm}
    \begin{center}
        \includegraphics[width=0.98\textwidth, height=0.60\textheight, keepaspectratio]{ch1_def_stochastic.pdf}
    \end{center}
    \vspace{-2mm}
    {\footnotesize
    \begin{block}{Interpretation}
    \begin{itemize}\setlength{\itemsep}{0pt}
        \item Each line is a \textbf{different realization} from the same underlying stochastic process
        \item We observe only \textbf{one realization}, yet aim to understand the properties of the process
    \end{itemize}
    \end{block}
    }
    \hfill\quantlet{TSA\_ch1\_stationarity}{https://github.com/QuantLet/TSA/tree/main/TSA_ch1/TSA_ch1_stationarity}
\end{frame}

\begin{frame}{Stochastic process: definition}
    \begin{defn}[Stochastic Process]
        \begin{itemize}\setlength{\itemsep}{0pt}
            \item A \textbf{stochastic process} is a collection of random variables indexed by time
            \begin{itemize}
                \item $\{X_t(\omega) : t \in \mathcal{T}, \omega \in \Omega\}$
                \item $\Omega$ is the sample space of possible outcomes
            \end{itemize}
        \end{itemize}
    \end{defn}

    \vspace{0.1cm}

    \begin{columns}[T]
        \column{0.48\textwidth}
        \begin{block}{Two Perspectives}
            \begin{itemize}\setlength{\itemsep}{0pt}
                \item \textbf{Fixed $\omega$}: A \textit{realization} $\{X_t(\omega)\}_{t \in \mathcal{T}}$
                \item \textbf{Fixed $t$}: A \textit{random variable} $X_t$
            \end{itemize}
        \end{block}
        \column{0.5\textwidth}
        \begin{exampleblock}{Key Insight}
            \begin{itemize}\setlength{\itemsep}{0pt}
                \item A time series we observe is \textbf{one realization} of the underlying stochastic process
            \end{itemize}
        \end{exampleblock}
    \end{columns}
\end{frame}

\begin{frame}{Moments of a stochastic process}
    \begin{block}{The First Two Moments Characterize the Process}
        \begin{itemize}\setlength{\itemsep}{0pt}
            \item \textbf{Mean Function}: $\mu_t = \E[X_t]$
            \item \textbf{Autocovariance (ACVF)}: $\gamma(t, s) = \Cov(X_t, X_s)$
            \begin{itemize}
                \item $\gamma(t, s) = \E[(X_t - \mu_t)(X_s - \mu_s)]$
            \end{itemize}
            \item \textbf{Autocorrelation (ACF)}:
            \begin{itemize}
                \item $\rho(t, s) = \gamma(t, s) / \sqrt{\Var(X_t) \cdot \Var(X_s)}$
            \end{itemize}
        \end{itemize}
    \end{block}

    \begin{columns}[T]
        \column{0.5\textwidth}
        \begin{exampleblock}{ACF Properties}
            \begin{itemize}\setlength{\itemsep}{0pt}
                \item \textbf{Range}: $\rho(t, s) \in [-1, 1]$
                \item \textbf{Normalization}: $\rho(t, t) = 1$ (perfect correlation with itself)
            \end{itemize}
        \end{exampleblock}
        \column{0.48\textwidth}
        \begin{alertblock}{Key Point}
            \begin{itemize}\setlength{\itemsep}{0pt}
                \item \textbf{General}: $\mu_t$ and $\gamma(t,s)$ may depend on $t$
                \item \textbf{Stationary}: Removes this dependence
            \end{itemize}
        \end{alertblock}
    \end{columns}
\end{frame}

%=============================================================================
% SECTION 4: STATIONARITY
%=============================================================================
\section{Stationarity}

\begin{frame}{Why stationarity matters}
    \vspace{-0.2cm}
    \begin{columns}[T]
        \begin{column}{0.48\textwidth}
            \begin{alertblock}{Without Stationarity}
            \begin{itemize}\setlength{\itemsep}{0pt}
                \item Mean, variance change over time
                \begin{itemize}
                    \item Estimates are inconsistent
                \end{itemize}
                \item Past may not predict the future
                \item Standard methods fail
                \item Spurious correlations
            \end{itemize}
            \end{alertblock}
        \end{column}
        \begin{column}{0.48\textwidth}
            \begin{exampleblock}{With Stationarity}
            \begin{itemize}\setlength{\itemsep}{0pt}
                \item Statistical properties constant
                \begin{itemize}
                    \item Ergodicity justified
                \end{itemize}
                \item Can estimate from a single realization
                \item Valid inference possible
                \item Models are meaningful
            \end{itemize}
            \end{exampleblock}
        \end{column}
    \end{columns}

    \begin{alertblock}{Key Principle}
        \begin{itemize}\setlength{\itemsep}{0pt}
            \item Most time series models (ARMA, ARIMA, etc.) require stationarity
            \item Non-stationary series must be transformed (e.g., differencing) before modeling
        \end{itemize}
    \end{alertblock}
\end{frame}

\begin{frame}{Strict stationarity: visual illustration}
    \vspace{-0.2cm}
    \begin{center}
        \includegraphics[width=0.95\textwidth, height=0.52\textheight, keepaspectratio]{ch1_def_strict_stationarity.pdf}
    \end{center}
    \vspace{-2mm}
    {\footnotesize
    \begin{block}{Interpretation}
    \begin{itemize}\setlength{\itemsep}{0pt}
        \item Time translation does not change the joint distribution of the variables
        \item Any two time windows have the same statistical properties
        \item In practice: we only check the first moments (weak stationarity)
    \end{itemize}
    \end{block}
    }
    \hfill\quantlet{TSA\_ch1\_stationarity}{https://github.com/QuantLet/TSA/tree/main/TSA_ch1/TSA_ch1_stationarity}
\end{frame}

\begin{frame}{Strict stationarity}
    \vspace{-0.2cm}
    \begin{defn}[Strict (Strong) Stationarity]
        \begin{itemize}\setlength{\itemsep}{0pt}
            \item A process $\{X_t\}$ is \textbf{strictly stationary} if for all $k$, all $t_1, \ldots, t_k$, and all $h$:
            \begin{itemize}
                \item $(X_{t_1}, \ldots, X_{t_k}) \stackrel{d}{=} (X_{t_1+h}, \ldots, X_{t_k+h})$
            \end{itemize}
            \item \textbf{Notation}: $X \stackrel{d}{=} Y$ means \textit{equality in distribution}
            \begin{itemize}
                \item $P(X \leq x) = P(Y \leq x)$
            \end{itemize}
        \end{itemize}
    \end{defn}
    \vspace{-0.1cm}
    \begin{block}{Implications}
            \begin{itemize}\setlength{\itemsep}{0pt}
                \item \textbf{Identical distributions}: $F_{X_t}(x)$ does not depend on $t$
                \begin{itemize}
                    \item $\E[X_t] = \mu$ (constant mean, if it exists)
                    \item $\Var(X_t) = \sigma^2$ (constant variance, if it exists)
                \end{itemize}
                \item \textbf{Lag dependence}: Joint distributions depend only on lag
            \end{itemize}
        \end{block}
    
    \vspace{-0.15cm}

    \begin{alertblock}{Note}
            \begin{itemize}\setlength{\itemsep}{0pt}
                \item Strict stationarity is a strong condition, often impossible to verify in practice
            \end{itemize}
        \end{alertblock}
\end{frame}

\begin{frame}{Weak stationarity: visual illustration}
    \vspace{-0.2cm}
    \begin{center}
        \includegraphics[width=0.95\textwidth, height=0.52\textheight, keepaspectratio]{ch1_def_weak_stationarity.pdf}
    \end{center}
    \vspace{-2mm}
    {\footnotesize
    \begin{exampleblock}{The Three Conditions}
    \begin{itemize}\setlength{\itemsep}{0pt}
        \item $\E[X_t] = \mu$ constant $\Rightarrow$ mean does not depend on time
        \item $\Var(X_t) = \sigma^2$ constant $\Rightarrow$ variance does not depend on time
        \item $\Cov(X_t, X_{t+h}) = \gamma(h)$ $\Rightarrow$ autocovariance depends only on lag $h$
    \end{itemize}
    \end{exampleblock}
    }
    \hfill\quantlet{TSA\_ch1\_stationarity}{https://github.com/QuantLet/TSA/tree/main/TSA_ch1/TSA_ch1_stationarity}
\end{frame}

\begin{frame}{Weak (covariance) stationarity}
    \small
    \begin{defn}[Weak Stationarity]
        \begin{itemize}\setlength{\itemsep}{0pt}
            \item A process $\{X_t\}$ is \textbf{weakly stationary} (or covariance stationary) if:
            \begin{itemize}\setlength{\itemsep}{0pt}
                \item $\E[X_t^2] < \infty$ for all $t$ — finite second-order moments
                \item $\E[X_t] = \mu$ for all $t$ — constant mean
                \item $\Cov(X_t, X_{t+h}) = \gamma(h)$ — covariance depends only on lag $h$, not on $t$
            \end{itemize}
        \end{itemize}
    \end{defn}

    \vspace{0.1cm}

    \begin{block}{Key Properties}
    \begin{itemize}\setlength{\itemsep}{0pt}
        \item \textbf{Autocovariance}: $\gamma(h) = \Cov(X_t, X_{t+h}) = \E[(X_t - \mu)(X_{t+h} - \mu)]$
        \item \textbf{Autocorrelation}: $\rho(h) = \gamma(h)/\gamma(0) = \Cov(X_t, X_{t+h})/\Var(X_t)$
        \item \textbf{Note}: $\rho(0) = 1$, $|\rho(h)| \leq 1$, $\rho(h) = \rho(-h)$ (symmetry)
    \end{itemize}
    \end{block}
\end{frame}

\begin{frame}{Relationship between strict and weak stationarity}
    \begin{thm}[Fundamental Implication]
        If $\{X_t\}$ is \textbf{strictly stationary} and $\E[X_t^2] < \infty$, then $\{X_t\}$ is also \textbf{weakly stationary}.
    \end{thm}

    \begin{proof}[Proof]
        \begin{itemize}\setlength{\itemsep}{0pt}
            \item Let $t_1, t_2$ be arbitrary and $h$ any time shift
            \item From joint distribution invariance: $(X_{t_1}, X_{t_2}) \stackrel{d}{=} (X_{t_1+h}, X_{t_2+h})$
            \item $\E[X_{t_1}] = \E[X_{t_1+h}] = \mu$ (constant mean)
            \item $\Cov(X_{t_1}, X_{t_2}) = \Cov(X_{t_1+h}, X_{t_2+h})$
            \item Thus autocovariance depends only on the difference $t_2 - t_1 = h$, not on $t_1$
        \end{itemize}
    \end{proof}

    \begin{alertblock}{Warning: The Converse is NOT True!}
        \begin{itemize}\setlength{\itemsep}{0pt}
            \item There exist weakly stationary processes that are \textbf{not} strictly stationary
        \end{itemize}
    \end{alertblock}
\end{frame}

\begin{frame}{Example: AR(1) is weakly stationary}
    \vspace{-0.2cm}
    {\footnotesize
    \begin{exampleblock}{Model}
        \begin{itemize}\setlength{\itemsep}{0pt}
            \item $X_t = \phi X_{t-1} + \varepsilon_t$, \quad $\varepsilon_t \sim WN(0, \sigma^2)$, \quad $|\phi| < 1$
        \end{itemize}
    \end{exampleblock}
    \vspace{-2mm}
    \begin{block}{Verification of the three conditions}
        \begin{enumerate}\setlength{\itemsep}{1pt}
            \item \textbf{Constant mean}: $\E[X_t] = \phi\,\E[X_{t-1}] + 0 = \phi\,\E[X_t]$ \;\; $\Rightarrow$ \;\; $\E[X_t] = 0$
            \item \textbf{Constant variance}: $\Var(X_t) = \phi^2\,\Var(X_t) + \sigma^2$ \;\; $\Rightarrow$ \;\; $\Var(X_t) = \dfrac{\sigma^2}{1 - \phi^2}$
            \item \textbf{Autocovariance depends only on lag}:
                $\gamma(h) = \phi^{|h|} \cdot \dfrac{\sigma^2}{1 - \phi^2}$, \quad
                $\rho(h) = \phi^{|h|}$
        \end{enumerate}
    \end{block}
    \vspace{-2mm}
    \begin{exampleblock}{Numerical example: $\phi = 0.8$, $\sigma^2 = 1$}
        \begin{itemize}\setlength{\itemsep}{0pt}
            \item $\E[X_t] = 0$, \quad $\Var(X_t) = \frac{1}{1-0.64} = 2.78$, \quad $\rho(1) = 0.8$, \; $\rho(2) = 0.64$, \; $\rho(5) = 0.33$
        \end{itemize}
    \end{exampleblock}
    }
\end{frame}

\begin{frame}{Counterexample: weakly stationary but NOT strictly stationary}
    \vspace{-0.3cm}
    {\footnotesize
    \begin{exampleblock}{Construction}
        \begin{itemize}\setlength{\itemsep}{0pt}
            \item Let $\{X_t\}$ be \textbf{independent} random variables with:
            $t$ even: $X_t \sim N(0, 1)$; \quad $t$ odd: $X_t \sim \frac{\chi^2(5) - 5}{\sqrt{10}}$
        \end{itemize}
    \end{exampleblock}
    }
    \vspace{-3mm}
    \begin{center}
        \includegraphics[width=0.95\textwidth, height=0.40\textheight, keepaspectratio]{ch1_counterexample_stationarity.pdf}
    \end{center}
    \vspace{-3mm}
    {\footnotesize
    \begin{columns}[T]
        \column{0.48\textwidth}
        \begin{exampleblock}{Weakly stationary \checkmark}
        \begin{itemize}\setlength{\itemsep}{0pt}
            \item $\E[X_t]=0$, $\Var(X_t)=1$, $\Cov(X_t,X_{t+h})=0$
        \end{itemize}
        \end{exampleblock}
        \column{0.48\textwidth}
        \begin{alertblock}{NOT strictly stationary \texttimes}
        \begin{itemize}\setlength{\itemsep}{0pt}
            \item Skewness differs ($0$ vs $>0$) $\Rightarrow$ $X_1 \stackrel{d}{\neq} X_2$
        \end{itemize}
        \end{alertblock}
    \end{columns}
    }
    \hfill\quantlet{TSA\_ch1\_stationarity}{https://github.com/QuantLet/TSA/tree/main/TSA_ch1/TSA_ch1_stationarity}
\end{frame}

\begin{frame}{Properties of the autocovariance function}
    \small
    \begin{prop}
    For a weakly stationary process, the ACVF $\gamma(h)$ satisfies:
    \begin{itemize}\setlength{\itemsep}{0pt}
        \item \textbf{Symmetry:} $\gamma(h) = \gamma(-h)$
        \item \textbf{Maximum at zero:} $|\gamma(h)| \leq \gamma(0) = \Var(X_t)$
        \item \textbf{Non-negative definiteness:} $\sum_{i,j} a_i a_j \gamma(i - j) \geq 0$ for any $a_1, \ldots, a_n$
    \end{itemize}
    \end{prop}

    \begin{alertblock}{Proof (property 3)}
    \begin{itemize}\setlength{\itemsep}{0pt}
        \item {\footnotesize $\Var\left(\sum_{i=1}^n a_i X_{t+i}\right) = \sum_{i,j} a_i a_j \gamma(i-j) \geq 0$ \quad (variance $\geq 0$)}
    \end{itemize}
    \end{alertblock}

    \begin{exampleblock}{Implication}
    \begin{itemize}\setlength{\itemsep}{0pt}
        \item Not every function can be a valid autocovariance function
    \end{itemize}
    \end{exampleblock}
\end{frame}

\begin{frame}{Ergodicity: visual illustration}
    \vspace{-0.2cm}
    \begin{center}
        \includegraphics[width=\textwidth, height=0.72\textheight, keepaspectratio]{ch1_ergodicity.pdf}
    \end{center}
    \vspace{-3mm}
    {\footnotesize
    \begin{itemize}\setlength{\itemsep}{0pt}
        \item \textbf{Time average} (single realization) and \textbf{ensemble average} (multiple realizations) both converge to $\mu$
        \item Ergodicity guarantees that we can estimate $\mu$ from a \textbf{single sufficiently long time series}
    \end{itemize}
    }
    \hfill\quantlet{TSA\_ch1\_stationarity}{https://github.com/QuantLet/TSA/tree/main/TSA_ch1/TSA_ch1_stationarity}
\end{frame}

\begin{frame}{Ergodicity: the foundation of inference from data}
    \footnotesize
    \vspace{-1mm}
    \begin{defn}[Ergodicity for Mean]
        \begin{itemize}\setlength{\itemsep}{0pt}
            \item A stationary process $\{X_t\}$ is \textbf{ergodic for the mean} if
            $\bar{X}_T = \frac{1}{T}\sum_{t=1}^{T} X_t \xrightarrow{P} \E[X_t] = \mu$ as $T \to \infty$
        \end{itemize}
    \end{defn}
    \vspace{-2mm}
    \begin{alertblock}{Why does ergodicity matter?}
        \begin{itemize}\setlength{\itemsep}{0pt}
            \item \textbf{Problem}: We have only \textbf{one realization} of the stochastic process
            \item \textbf{Solution}: Ergodicity allows estimating $\mu$ from $\bar{X}_T$ --- the time average converges to the population mean. Without ergodicity, inference is not possible!
        \end{itemize}
    \end{alertblock}
    \vspace{-2mm}
    \begin{thm}[Sufficient Condition]
        If $\sum_{h=0}^{\infty} |\gamma(h)| < \infty$ (absolutely summable autocovariances), the process is ergodic.
    \end{thm}
    \vspace{-2mm}
    {\scriptsize
    \begin{alertblock}{Counterexample: stationary but non-ergodic}
        \begin{itemize}\setlength{\itemsep}{0pt}
            \item Let $Z \sim N(0,1)$, define $X_t = Z\ \forall\, t$. Strictly stationary, but $\bar{X}_T = Z\ \forall\, T$ $\Rightarrow$ time average does \textbf{not} converge to $\mu = 0$
            \item \textbf{Conclusion}: ergodicity is an \textbf{additional} assumption, stronger than stationarity
        \end{itemize}
    \end{alertblock}
    }
\end{frame}

\begin{frame}{Spectral Density: The Frequency Domain}
    \begin{cminipage}{0.95\textwidth}
        \footnotesize
        \begin{defn}[Power Spectral Density]
            \begin{itemize}\setlength{\itemsep}{0pt}
                \item For a stationary process with $\sum_{h=-\infty}^{\infty} |\gamma(h)| < \infty$, the \textbf{spectral density} is:
                \vspace{-1mm}
                \[
                    S(\omega) = \frac{1}{2\pi} \sum_{h=-\infty}^{\infty} \gamma(h)\, e^{-i\omega h} = \frac{1}{2\pi}\left[\gamma(0) + 2\sum_{h=1}^{\infty} \gamma(h)\cos(\omega h)\right], \quad \omega \in [-\pi, \pi]
                \]
                \vspace{-3mm}
                \item $S(\omega)$ decomposes the variance across frequencies: $\gamma(0) = \int_{-\pi}^{\pi} S(\omega)\, d\omega$
            \end{itemize}
        \end{defn}
        \vspace{-2mm}
        \begin{columns}[T]
            \begin{column}{0.48\textwidth}
                \begin{block}{Interpretation}
                    \begin{itemize}\setlength{\itemsep}{0pt}
                        \item Large $S(\omega)$ at low $\omega$ $\Rightarrow$ dominant long cycle
                        \item White noise: $S(\omega) = \frac{\sigma^2}{2\pi}$ (flat)
                        \item AR(1) $\phi > 0$: power at low freq.
                        \item MA(1) $\theta > 0$: power at high freq.
                    \end{itemize}
                \end{block}
            \end{column}
            \begin{column}{0.48\textwidth}
                \begin{exampleblock}{Connections}
                    \begin{itemize}\setlength{\itemsep}{0pt}
                        \item \textbf{Fourier pair}: $S(\omega) \leftrightarrow \gamma(h)$ (equivalent)
                        \item Time domain (ACF) $\equiv$ freq.\ domain (spectrum)
                        \item \textbf{Periodogram}: empirical estimator of $S(\omega)$
                        \item Useful for detecting hidden seasonality
                    \end{itemize}
                \end{exampleblock}
            \end{column}
        \end{columns}
    \end{cminipage}
\end{frame}

\begin{frame}{The Wold decomposition theorem}
    \small
    \begin{thm}[Wold, 1938]
        Any \textbf{covariance stationary} process $\{X_t\}$ can be written as:
        $X_t = \sum_{j=0}^{\infty} \psi_j \varepsilon_{t-j} + \eta_t$
        \begin{itemize}
            \item $\varepsilon_t \sim WN(0, \sigma^2)$ $\Rightarrow$ white noise
            \begin{itemize}
                \item $\psi_0 = 1$, $\sum \psi_j^2 < \infty$
            \end{itemize}
            \item $\eta_t$ $\Rightarrow$ deterministic component (perfectly predictable)
        \end{itemize}
    \end{thm}

    \begin{exampleblock}{Significance of the Wold Theorem}
        \begin{itemize}\setlength{\itemsep}{0pt}
            \item \textbf{Decomposition}: Any stationary process = \textbf{MA($\infty$)} + deterministic component
            \begin{itemize}
                \item Theoretically justifies MA($q$) and ARMA($p,q$) models
                \item Coefficients $\psi_j$ measure the impact of past shocks
            \end{itemize}
        \end{itemize}
    \end{exampleblock}
\end{frame}

\begin{frame}{Proof of the Wold theorem (sketch)}
    {\footnotesize
    \begin{proof}[\footnotesize Proof sketch]
    \vspace{-1mm}
    \begin{enumerate}\setlength{\itemsep}{1pt}
        \item \textbf{Hilbert space of the past}: Define $\mathcal{H}_t = \overline{\mathrm{sp}}\{X_s : s \leq t\}$ --- the closed linear span of past and present values, with inner product $\langle X, Y \rangle = \mathrm{Cov}(X,Y)$.

        \item \textbf{Innovation}: Define $\varepsilon_t = X_t - \hat{X}_t$, where $\hat{X}_t = \mathrm{Proj}_{\mathcal{H}_{t-1}}(X_t)$ is the orthogonal projection. By construction, $\varepsilon_t \perp \mathcal{H}_{t-1}$, so $\varepsilon_t \perp \varepsilon_s$ for $t \neq s$ $\Rightarrow$ $\{\varepsilon_t\}$ is white noise.

        \item \textbf{Iterative representation}: Applying the projection recursively:
        \vspace{-1mm}
        \[
            X_t = \varepsilon_t + \psi_1 \varepsilon_{t-1} + \psi_2 \varepsilon_{t-2} + \cdots + \eta_t = \sum_{j=0}^{\infty} \psi_j \varepsilon_{t-j} + \eta_t
        \]
        \vspace{-3mm}

        where $\psi_j$ arise from successive projections, and $\eta_t \in \mathcal{H}_{-\infty} = \bigcap_{t} \mathcal{H}_t$.

        \item \textbf{Convergence}: $\sum_{j=0}^{\infty} \psi_j^2 < \infty$ because $\mathrm{Var}(X_t) < \infty$ (stationarity).

        \item \textbf{Deterministic component}: $\eta_t \in \mathcal{H}_{-\infty}$ $\Rightarrow$ $\eta_t$ is \textit{perfectly predictable} from the infinite past.\qedhere
    \end{enumerate}
    \end{proof}
    }
\end{frame}

\begin{frame}{The Wold theorem: visual illustration}
    \begin{center}
        \includegraphics[height=4.0cm]{../../charts/ch1_wold_decomposition.pdf}
    \end{center}
    \vspace{-2mm}
    {\footnotesize
    \begin{block}{Interpretation}
    \begin{itemize}\setlength{\itemsep}{0pt}
        \item $X_t$ decomposes into a \textbf{stochastic} component (MA($\infty$)) and a \textbf{deterministic} component ($\eta_t$)
        \item Coefficients $\psi_j$ decay $\Rightarrow$ recent shocks have greater impact than distant ones
    \end{itemize}
    \end{block}
    }
    \hfill\quantlet{TSA\_ch1\_operators}{https://github.com/QuantLet/TSA/tree/main/TSA_ch1/TSA_ch1_operators}
\end{frame}

%=============================================================================
% SECTION 5: LAG OPERATOR AND DIFFERENCING
%=============================================================================
\section{Lag Operator and Differencing}

\begin{frame}{Lag operator: visual illustration}
    \vspace{-0.3cm}
    \begin{center}
        \includegraphics[width=0.98\textwidth, height=0.55\textheight, keepaspectratio]{ch1_def_lag_operator.pdf}
    \end{center}
    \vspace{-3mm}
    {\footnotesize
    \begin{exampleblock}{Properties}
    \begin{itemize}\setlength{\itemsep}{0pt}
        \item $LX_t = X_{t-1}$ $\Rightarrow$ the lag operator shifts the series back by one period
        \item $L^k X_t = X_{t-k}$ $\Rightarrow$ shift by $k$ periods; $L^0 = I$ (identity)
        \item \textbf{Difference operator}: $\Delta = (1-L)$, so $\Delta X_t = X_t - X_{t-1}$
    \end{itemize}
    \end{exampleblock}
    }
    \hfill\quantlet{TSA\_ch1\_operators}{https://github.com/QuantLet/TSA/tree/main/TSA_ch1/TSA_ch1_operators}
\end{frame}

\begin{frame}{The lag operator}
    \begin{defn}[Lag Operator]
        \begin{itemize}\setlength{\itemsep}{0pt}
            \item The \textbf{lag operator} (or backshift operator) $L$ is defined by: $LX_t = X_{t-1}$
        \end{itemize}
    \end{defn}

    \begin{columns}[T]
        \column{0.48\textwidth}
        \begin{block}{Properties}
            \begin{itemize}\setlength{\itemsep}{0pt}
                \item \textbf{Powers}: $L^k X_t = X_{t-k}$ (lag by $k$ periods)
                \begin{itemize}
                    \item Compact notation for models
                \end{itemize}
                \item \textbf{Identity}: $L^0 = I$
                \item \textbf{Polynomial}: $(1 - \phi L)X_t = X_t - \phi X_{t-1}$
            \end{itemize}
        \end{block}
        \column{0.5\textwidth}
        \begin{exampleblock}{Examples}
            \begin{itemize}\setlength{\itemsep}{0pt}
                \item \textbf{First difference}: $(1-L)X_t = X_t - X_{t-1}$
                \item \textbf{Second difference}: $(1-L)^2 X_t = \Delta^2 X_t$
                \item \textbf{Seasonal}: $(1-L^{12})X_t$
            \end{itemize}
        \end{exampleblock}
    \end{columns}
\end{frame}

\begin{frame}{Effect of differencing: S\&P 500}
    \vspace{-0.2cm}
    \begin{center}
        \includegraphics[width=0.98\textwidth, height=0.60\textheight, keepaspectratio]{differencing_effect.pdf}
    \end{center}
    \vspace{-2mm}
    {\footnotesize
    \begin{block}{Interpretation}
    \begin{itemize}\setlength{\itemsep}{0pt}
        \item \textbf{Top}: S\&P 500 prices $\Rightarrow$ clear trend, non-stationary ($I(1)$)
        \item \textbf{Bottom}: Log returns $r_t = \ln P_t - \ln P_{t-1}$ $\Rightarrow$ fluctuates around mean $\approx 0$, stationary
    \end{itemize}
    \end{block}
    }
    \hfill\quantlet{TSA\_ch1\_operators}{https://github.com/QuantLet/TSA/tree/main/TSA_ch1/TSA_ch1_operators}
\end{frame}

\begin{frame}{Differencing}
    \vspace{-0.3cm}
    {\small
    \begin{block}{Why Do We Difference?}
    \begin{itemize}\setlength{\itemsep}{0pt}
        \item \textbf{First Difference}: $\Delta X_t = X_t - X_{t-1} = (1 - L)X_t$
        \begin{itemize}
            \item Removes trend and unit root
            \item Random walk: $\Delta X_t = \varepsilon_t$
        \end{itemize}
    \end{itemize}
    \end{block}

    \begin{defn}[Integrated Process of Order $d$]
        \begin{itemize}\setlength{\itemsep}{0pt}
            \item A process $\{X_t\}$ is \textbf{integrated of order $d$}, denoted $X_t \sim I(d)$, if:
            \begin{itemize}\setlength{\itemsep}{0pt}
                \item $\Delta^d X_t = (1-L)^d X_t$ is stationary ($I(0)$ process)
                \item $\Delta^{d-1} X_t$ is \textbf{not} stationary
            \end{itemize}
        \end{itemize}
    \end{defn}

    \begin{exampleblock}{Examples}
    \begin{itemize}\setlength{\itemsep}{0pt}
        \item $I(0)$: Stationary process (white noise, stationary AR)
        \item $I(1)$: Random walk $\Rightarrow$ $\Delta X_t = \varepsilon_t$ is stationary
        \item $I(2)$: Requires two differences for stationarity
    \end{itemize}
    \end{exampleblock}
    }
\end{frame}

%=============================================================================
% SECTION 6: WHITE NOISE AND RANDOM WALK
%=============================================================================
\section{White Noise and Random Walk}

\begin{frame}{White noise: visual illustration}
    \begin{center}
        \includegraphics[width=\textwidth, height=0.82\textheight, keepaspectratio]{ch1_def_white_noise.pdf}
    \end{center}
    \vspace{-3mm}
    \hfill\quantlet{TSA\_ch1\_white\_noise}{https://github.com/QuantLet/TSA/tree/main/TSA_ch1/TSA_ch1_white_noise}
\end{frame}

\begin{frame}{White noise process}
    \vspace{-0.3cm}
    {\footnotesize
    \begin{defn}[White Noise]
        \begin{itemize}\setlength{\itemsep}{0pt}
            \item A process $\{\varepsilon_t\}$ is \textbf{white noise}, denoted $\varepsilon_t \sim WN(0, \sigma^2)$, if:
            \begin{itemize}\setlength{\itemsep}{0pt}
                \item $\E[\varepsilon_t] = 0$ for all $t$ (zero mean)
                \item $\Var(\varepsilon_t) = \sigma^2$ for all $t$ (constant variance)
                \item $\Cov(\varepsilon_t, \varepsilon_s) = 0$ for $t \neq s$ (uncorrelated)
            \end{itemize}
        \end{itemize}
    \end{defn}
    \vspace{-2mm}
    \begin{block}{ACF of White Noise}
        \begin{itemize}\setlength{\itemsep}{0pt}
            \item By definition: $\gamma(0) = \sigma^2$ and $\gamma(h) = 0$ for $h \neq 0$;
            $\rho(h) = \begin{cases} 1 & h = 0 \\ 0 & h \neq 0 \end{cases}$
        \end{itemize}
    \end{block}
    \vspace{-2mm}
    \begin{exampleblock}{Types of white noise (in order of increasing restrictions)}
    \begin{itemize}\setlength{\itemsep}{0pt}
        \item \textbf{Weak}: uncorrelated, but nonlinear dependencies may exist
        \item \textbf{Strong}: $\varepsilon_t$ are \textit{independent} and identically distributed (i.i.d.)
        \item \textbf{Gaussian}: $\varepsilon_t \stackrel{iid}{\sim} N(0, \sigma^2)$
        \begin{itemize}
            \item Uncorrelated $\Rightarrow$ independent
        \end{itemize}
    \end{itemize}
    \end{exampleblock}
    }
\end{frame}

\begin{frame}{The three types of white noise}
    \vspace{-2mm}
    \begin{center}
        \includegraphics[width=\textwidth, height=0.48\textheight, keepaspectratio]{ch1_white_noise_types.pdf}
    \end{center}
    \vspace{-3mm}
    {\scriptsize
    \begin{exampleblock}{Inclusion relationship: \normalfont Gaussian $\subset$ Strong (i.i.d.) $\subset$ Weak (uncorrelated)}
    \begin{itemize}\setlength{\itemsep}{0pt}
        \item \textbf{Weak}: $\Cov(\varepsilon_t, \varepsilon_s) = 0$, but nonlinear dependencies may exist (e.g.\ GARCH)
        \item \textbf{Strong}: $\varepsilon_t$ are i.i.d.\ --- any distribution (e.g.\ Student-$t$)
        \item \textbf{Gaussian}: $\varepsilon_t \stackrel{iid}{\sim} N(0, \sigma^2)$ --- uncorrelated $\Leftrightarrow$ independent
    \end{itemize}
    \end{exampleblock}
    }
    \hfill\quantlet{TSA\_ch1\_white\_noise}{https://github.com/QuantLet/TSA/tree/main/TSA_ch1/TSA_ch1_white_noise}
\end{frame}

\begin{frame}{Random walk: visualization}
    \vspace{-0.2cm}
    \begin{center}
        \includegraphics[width=\textwidth, height=0.58\textheight, keepaspectratio]{random_walk.pdf}
    \end{center}
    \vspace{-3mm}
    {\footnotesize
    \begin{block}{Observations}
    \begin{itemize}\setlength{\itemsep}{0pt}
        \item Each shock has a \textbf{permanent effect}; $\Var(X_t) = t\sigma^2$ grows linearly with time
        \item \textbf{Solution} --- differencing transforms into white noise, $\Delta X_t = \varepsilon_t$
    \end{itemize}
    \end{block}
    }
    \hfill\quantlet{TSA\_ch1\_random\_walk}{https://github.com/QuantLet/TSA/tree/main/TSA_ch1/TSA_ch1_random_walk}
\end{frame}

\begin{frame}{Random walk process}
    \small
    \vspace{-3mm}
    \begin{defn}[Random Walk]
        $X_t = X_{t-1} + \varepsilon_t$, \; $\varepsilon_t \sim WN(0, \sigma^2)$, $X_0 = 0$ \;\;
        $\Rightarrow$ \textbf{Explicit form}: $X_t = \sum_{i=1}^{t} \varepsilon_i$
    \end{defn}
    \vspace{-2mm}
    {\footnotesize
    \begin{prop}[Properties]
        \begin{itemize}\setlength{\itemsep}{0pt}
            \item $\E[X_t] = 0$
            \item $\Var(X_t) = t\sigma^2$ (grows with time!)
            \item $\Cov(X_t, X_s) = \min(t, s) \cdot \sigma^2$
        \end{itemize}
    \end{prop}
    }
    \vspace{-2mm}
    {\scriptsize
    \begin{proof}[Proofs]
    \begin{itemize}\setlength{\itemsep}{0pt}
        \item $\E[X_t] = \E\!\bigl[\sum_{i=1}^{t} \varepsilon_i\bigr] = 0$
        \item $\Var(X_t) = \Var\!\bigl(\sum_{i=1}^{t} \varepsilon_i\bigr) = \sum_{i=1}^{t} \Var(\varepsilon_i) = t\sigma^2$ \quad (independence)
        \item $\Cov(X_t, X_s) = \min(t,s)\,\sigma^2$ \; (for $s \leq t$)
    \end{itemize}
    \end{proof}
    }
    \vspace{-2mm}
    \begin{alertblock}{Non-Stationary!}
    {\footnotesize $\Var(X_t) = t\sigma^2$ depends on $t$ $\Rightarrow$ random walk is \textbf{not stationary}}
    \end{alertblock}
\end{frame}

\begin{frame}{Random walk with drift}
    \vspace{-0.3cm}
    {\footnotesize
    \begin{defn}[Random Walk with Drift]
        $X_t = c + X_{t-1} + \varepsilon_t$, \; $c \neq 0$ is the \textbf{drift} \;\;
        $\Rightarrow$ \textbf{Explicit form}: $X_t = ct + \sum_{i=1}^{t} \varepsilon_i$
    \end{defn}
    \vspace{-2mm}
    \begin{prop}[Properties]
        \begin{itemize}\setlength{\itemsep}{0pt}
            \item $\E[X_t] = ct$ (linear trend)
            \item $\Var(X_t) = t\sigma^2$ (grows with time)
        \end{itemize}
    \end{prop}
    \vspace{-2mm}
    \begin{alertblock}{Differencing}
        $\Delta X_t = c + \varepsilon_t$ \; --- constant plus white noise $\Rightarrow$ the differenced series is stationary
    \end{alertblock}
    \vspace{-2mm}
    \begin{exampleblock}{Practical Importance}
        \begin{itemize}\setlength{\itemsep}{0pt}
            \item Nominal GDP, stock prices $\Rightarrow$ often modeled as RW with drift
            \item The ADF test includes variants: without constant, with constant, with constant and trend
        \end{itemize}
    \end{exampleblock}
    }
\end{frame}

\begin{frame}{White noise vs random walk: comparison}
    \vspace{-0.2cm}
    \begin{center}
        \includegraphics[width=0.75\textwidth, height=0.26\textheight, keepaspectratio]{ch1_wn_rw.pdf}
    \end{center}
    \vspace{-0.2cm}
    \hfill\quantlet{TSA\_ch1\_random\_walk}{https://github.com/QuantLet/TSA/tree/main/TSA_ch1/TSA_ch1_random_walk}
    {\footnotesize
    \begin{block}{White Noise}
        \begin{itemize}\setlength{\itemsep}{0pt}
            \item Stationary; $\Var = \sigma^2$ (const.); ACF $= 0$, $h \neq 0$; no memory
        \end{itemize}
    \end{block}
    \begin{alertblock}{Random Walk}
        \begin{itemize}\setlength{\itemsep}{0pt}
            \item Non-stationary; $\Var = t\sigma^2$ (grows); ACF $\approx 1$ (slow); permanent shocks
        \end{itemize}
    \end{alertblock}
    \begin{block}{Link}
    \begin{itemize}\setlength{\itemsep}{0pt}
        \item $\Delta X_t = \varepsilon_t$
    \end{itemize}
    \end{block}
    }
\end{frame}

\begin{frame}{Trend-stationary vs.\ difference-stationary}
    \vspace{-0.2cm}
    {\footnotesize
    \begin{columns}[T]
        \column{0.48\textwidth}
        \begin{block}{Trend-Stationary (TS)}
            \begin{itemize}\setlength{\itemsep}{1pt}
                \item \textbf{Model}: $Y_t = \alpha + \beta t + \varepsilon_t$
                \begin{itemize}
                    \item \textbf{Deterministic} trend
                    \item Deviations from the trend are temporary
                \end{itemize}
                \item \textbf{Solution}: regression on $t$, extract residuals
                \item \textbf{Effect}: Shocks do NOT have a permanent effect
            \end{itemize}
        \end{block}

        \column{0.48\textwidth}
        \begin{alertblock}{Difference-Stationary (DS)}
            \begin{itemize}\setlength{\itemsep}{1pt}
                \item \textbf{Model}: $Y_t = c + Y_{t-1} + \varepsilon_t$
                \begin{itemize}
                    \item \textbf{Stochastic} trend
                    \item Deviations from the trend are permanent
                \end{itemize}
                \item \textbf{Solution}: differencing ($\Delta Y_t$)
                \item \textbf{Effect}: Shocks HAVE a permanent effect
            \end{itemize}
        \end{alertblock}
    \end{columns}

    \begin{exampleblock}{Why does the distinction matter?}
        \begin{itemize}\setlength{\itemsep}{0pt}
            \item \textbf{Differencing a TS process}: introduces an artificial unit root in the MA part
            \item \textbf{Regression on a DS process}: produces residuals that are \textbf{still non-stationary}
            \item \textbf{Solution}: ADF and KPSS tests help distinguish between the two
        \end{itemize}
    \end{exampleblock}
    }
\end{frame}

\begin{frame}{ACF comparison: stationary vs random walk}
    \vspace{-0.2cm}
    \begin{center}
        \includegraphics[width=0.95\textwidth, height=0.52\textheight, keepaspectratio]{rw_vs_stationary.pdf}
    \end{center}
    \vspace{-2mm}
    {\footnotesize
    \begin{block}{Interpretation}
    \begin{itemize}\setlength{\itemsep}{0pt}
        \item \textbf{Stationary}: ACF decays rapidly (exponentially or oscillating) toward zero
        \item \textbf{Random walk}: ACF decays very slowly, stays close to 1
        \item \textbf{Rule of thumb}: Slow ACF decay $\Rightarrow$ suspect unit root $\Rightarrow$ ADF test
    \end{itemize}
    \end{block}
    }
    \hfill\quantlet{TSA\_ch1\_random\_walk}{https://github.com/QuantLet/TSA/tree/main/TSA_ch1/TSA_ch1_random_walk}
\end{frame}

%=============================================================================
% SECTION 6: ACF AND PACF
%=============================================================================
\section{Autocorrelation Functions}

\begin{frame}{ACF patterns for different processes}
    \vspace{-0.3cm}
    \begin{center}
        \includegraphics[width=0.99\textwidth, height=0.63\textheight, keepaspectratio]{ch1_acf_examples.pdf}
    \end{center}
    \vspace{-3mm}
    {\footnotesize
    \begin{block}{Interpretation}
    \begin{itemize}\setlength{\itemsep}{0pt}
        \item \textbf{White noise}: ACF = 0; \textbf{Stationary}: decays fast; \textbf{Non-stationary}: decays slowly
        \item \textbf{Seasonal}: Spikes at seasonal lags (12, 24 for monthly data)
    \end{itemize}
    \end{block}
    }
    \hfill\quantlet{TSA\_ch1\_acf\_patterns}{https://github.com/QuantLet/TSA/tree/main/TSA_ch1/TSA_ch1_acf_patterns}
\end{frame}

\begin{frame}{Sample autocorrelation function}
    \vspace{-0.2cm}
    {\small
    \begin{block}{Sample ACF at Lag $h$}
        \begin{itemize}\setlength{\itemsep}{0pt}
            \item $\hat{\rho}(h) = \frac{\sum_{t=1}^{T-h}(x_t - \bar{x})(x_{t+h} - \bar{x})}{\sum_{t=1}^{T}(x_t - \bar{x})^2}$
            \begin{itemize}
                \item Properties: $\hat{\rho}(0) = 1$, $|\hat{\rho}(h)| \leq 1$
            \end{itemize}
        \end{itemize}
    \end{block}

    \begin{thm}[Bartlett, 1946]
        Under $H_0$: white noise, for large $T$: $\hat{\rho}(h) \approx N(0, 1/T)$
    \end{thm}

    \begin{block}{95\% Confidence Interval}
    \begin{itemize}\setlength{\itemsep}{0pt}
        \item $\pm 1.96/\sqrt{T}$ (the bands in ACF plots)
    \end{itemize}
    \end{block}

    \begin{alertblock}{Caution}
        \begin{itemize}\setlength{\itemsep}{0pt}
            \item Bartlett's formula is valid \textbf{only under $H_0$: white noise}
            \item For AR/MA, the asymptotic variance differs
        \end{itemize}
    \end{alertblock}
    }
\end{frame}

\begin{frame}{ACF and PACF patterns}
    \vspace{-0.2cm}
    \begin{center}
        \includegraphics[width=0.98\textwidth, height=0.54\textheight, keepaspectratio]{acf_pacf_examples.pdf}
    \end{center}
    \vspace{-2mm}
    {\footnotesize
    \begin{exampleblock}{Identification Rules}
    \begin{itemize}\setlength{\itemsep}{0pt}
        \item \textbf{AR($p$)}: ACF decays exponentially, PACF cuts off after lag $p$
        \item \textbf{MA($q$)}: ACF cuts off after lag $q$, PACF decays exponentially
        \item \textbf{ARMA($p,q$)}: Both decay exponentially $\Rightarrow$ identification requires information criteria
    \end{itemize}
    \end{exampleblock}
    }
    \hfill\quantlet{TSA\_ch1\_acf\_patterns}{https://github.com/QuantLet/TSA/tree/main/TSA_ch1/TSA_ch1_acf_patterns}
\end{frame}

\begin{frame}{Partial autocorrelation function (PACF)}
    \begin{defn}[Partial Autocorrelation]
        \begin{itemize}\setlength{\itemsep}{0pt}
            \item \textbf{PACF} at lag $h$, denoted $\phi_{hh}$: the last coefficient in the regression:
            \begin{itemize}
                \item $X_t = \phi_{h1}X_{t-1} + \phi_{h2}X_{t-2} + \cdots + \phi_{hh}X_{t-h} + e_t$
            \end{itemize}
            \item \textbf{Alternatively}:
            \begin{itemize}
                \item $\phi_{hh} = \Corr(X_t - \hat{X}_t^{(h-1)}, X_{t-h} - \hat{X}_{t-h}^{(h-1)})$
            \end{itemize}
            \item \textbf{Interpretation}: \textit{Direct} dependence at lag $h$
            \begin{itemize}
                \item Removes the effect of intermediate lags
            \end{itemize}
        \end{itemize}
    \end{defn}

    \begin{exampleblock}{Key Application: Model Order Identification}
        \begin{itemize}\setlength{\itemsep}{0pt}
            \item \textbf{AR($p$)}: PACF \textbf{cuts off} after lag $p$
            \begin{itemize}
                \item ACF decays exponentially or oscillates
            \end{itemize}
            \item \textbf{MA($q$)}: ACF \textbf{cuts off} after lag $q$
            \begin{itemize}
                \item PACF decays exponentially or oscillates
            \end{itemize}
        \end{itemize}
    \end{exampleblock}
\end{frame}

\begin{frame}{ACF decay patterns}
    \vspace{-0.2cm}
    \begin{center}
        \includegraphics[width=0.98\textwidth, height=0.54\textheight, keepaspectratio]{acf_theoretical.pdf}
    \end{center}
    \vspace{-2mm}
    {\footnotesize
    \begin{block}{Interpretation}
    \begin{itemize}\setlength{\itemsep}{0pt}
        \item \textbf{Exponential decay}: Persistent positive dependence (AR with $\phi > 0$)
        \item \textbf{Oscillating decay}: Alternating dependence (AR with $\phi < 0$)
        \item The decay rate indicates the strength of the process memory
    \end{itemize}
    \end{block}
    }
    \hfill\quantlet{TSA\_ch1\_acf\_patterns}{https://github.com/QuantLet/TSA/tree/main/TSA_ch1/TSA_ch1_acf_patterns}
\end{frame}

%=============================================================================
% SECTION 7: TESTING FOR STATIONARITY
%=============================================================================
\section{Testing for Stationarity}

\begin{frame}{Augmented Dickey-Fuller (ADF) test}
    \vspace{-0.3cm}
    {\footnotesize
    \begin{block}{ADF Model}
    $\Delta X_t = \alpha + \gamma X_{t-1} + \sum_{i=1}^{p} \delta_i \Delta X_{t-i} + \varepsilon_t$, \;\;
    $\gamma = \rho - 1$, \; $H_0: \gamma = 0 \Leftrightarrow \rho = 1$
    \end{block}
    \vspace{-2mm}
    \begin{columns}[T]
        \begin{column}{0.48\textwidth}
            \begin{exampleblock}{Hypotheses}
                \begin{itemize}\setlength{\itemsep}{0pt}
                    \item \textbf{$H_0$}: $\gamma = 0$ (unit root)
                    \item \textbf{$H_1$}: $\gamma < 0$ (stationary)
                \end{itemize}
            \end{exampleblock}
        \end{column}
        \begin{column}{0.48\textwidth}
            \begin{block}{Test Statistic}
                \begin{itemize}\setlength{\itemsep}{0pt}
                    \item $\tau_{ADF} = \hat{\gamma}\, / \, SE(\hat{\gamma})$
                    \item $\hat{\gamma}$ = OLS coefficient of $X_{t-1}$
                    \item $SE(\hat{\gamma})$ from the OLS regression
                \end{itemize}
            \end{block}
        \end{column}
    \end{columns}
    \vspace{-1mm}
    \begin{alertblock}{Decision Rule}
        \begin{itemize}\setlength{\itemsep}{0pt}
            \item $\tau_{ADF} <$ critical value $\Rightarrow$ Reject $H_0$ $\Rightarrow$ \textcolor{Forest}{Stationary}
            \item $\tau_{ADF} \geq$ critical value $\Rightarrow$ \textcolor{Crimson}{Non-stationary (unit root)}
            \item Critical values follow the Dickey-Fuller distribution (\textbf{not} $t$-Student!)
        \end{itemize}
    \end{alertblock}
    }
\end{frame}

\begin{frame}{KPSS test}
    \begin{block}{Model}
        \begin{itemize}\setlength{\itemsep}{0pt}
            \item $X_t = \xi t + r_t + \varepsilon_t$ where $r_t = r_{t-1} + u_t$
        \end{itemize}
    \end{block}

    \vspace{0.1cm}

    \begin{columns}[T]
        \begin{column}{0.48\textwidth}
            \begin{exampleblock}{Hypotheses (opposite of ADF)}
                \begin{itemize}\setlength{\itemsep}{0pt}
                    \item \textbf{$H_0$}: $\sigma_u^2 = 0$ (stationary)
                    \item \textbf{$H_1$}: $\sigma_u^2 > 0$ (unit root)
                \end{itemize}
            \end{exampleblock}
        \end{column}
        \begin{column}{0.48\textwidth}
            \begin{block}{Test Statistic}
                \begin{itemize}\setlength{\itemsep}{0pt}
                    \item $LM = \frac{\sum_{t=1}^{T} S_t^2}{T^2 \hat{\sigma}^2_{LR}}$
                    \item {\small $S_t = \sum_{i=1}^{t} \hat{e}_i$, \; $\hat{\sigma}^2_{LR}$ = long-run variance}
                \end{itemize}
            \end{block}
        \end{column}
    \end{columns}

    \vspace{0.1cm}

    \begin{alertblock}{Decision Rule}
        \begin{itemize}\setlength{\itemsep}{0pt}
            \item $LM >$ critical value $\Rightarrow$ Reject $H_0$ $\Rightarrow$ \textcolor{Crimson}{Non-stationary}
            \item $LM \leq$ critical value $\Rightarrow$ \textcolor{Forest}{Stationary}
        \end{itemize}
    \end{alertblock}
\end{frame}

\begin{frame}{ADF test: visualization with S\&P 500}
    \vspace{-0.3cm}
    \begin{center}
        \includegraphics[width=0.95\textwidth, height=0.50\textheight, keepaspectratio]{adf_test_visualization.pdf}
    \end{center}
    \vspace{-2mm}
    \quantlet{TSA\_ch1\_unit\_root\_tests}{https://github.com/QuantLet/TSA/tree/main/TSA_ch1/TSA_ch1_unit_root_tests}
    \vspace{-0.2cm}
    {\footnotesize
    \begin{block}{Interpreting the ADF Test}
        \begin{itemize}\setlength{\itemsep}{0pt}
            \item \textbf{Hypothesis}: $H_0$: Unit root
            \begin{itemize}
                \item Critical values: $-3.43$ (1\%), $-2.86$ (5\%), $-2.57$ (10\%)
                \item $\tau <$ critical value $\Rightarrow$ reject $H_0$ $\Rightarrow$ stationary series
            \end{itemize}
            \item \textbf{S\&P 500}: Prices non-stationary; Returns stationary
        \end{itemize}
    \end{block}
    }
\end{frame}

\begin{frame}{Using ADF and KPSS together}
    \begin{block}{Confirmatory Testing}
            \begin{itemize}\setlength{\itemsep}{0pt}
                \item \textbf{ADF rejects $H_0$ + KPSS fails to reject}: \textcolor{Forest}{Stationary}
                \item \textbf{ADF fails to reject + KPSS rejects $H_0$}: \textcolor{Crimson}{Unit Root}
                \item \textbf{Both reject or both fail to reject}: Inconclusive
                \begin{itemize}
                    \item Additional tests required (PP, DF-GLS)
                \end{itemize}
            \end{itemize}
        \end{block}
    
    \vspace{0.1cm}
    
    \begin{exampleblock}{Workflow}
            \begin{itemize}\setlength{\itemsep}{0pt}
                \item \textbf{Step 1}: ADF test ($H_0$: unit root)
                \item \textbf{Step 2}: KPSS test ($H_0$: stationary)
                \item \textbf{Step 3}: Concordant results $\Rightarrow$ OK
                \begin{itemize}
                    \item Otherwise: PP, DF-GLS tests
                \end{itemize}
            \end{itemize}
        \end{exampleblock}
\end{frame}

\begin{frame}{The Phillips-Perron (PP) Test}
    \begin{cminipage}{0.95\textwidth}
        \small
        \begin{defn}[Phillips-Perron, 1988]
            \begin{itemize}\setlength{\itemsep}{0pt}
                \item Tests the same hypothesis as ADF: $H_0$: unit root ($\gamma = 0$)
                \item Base model (no augmented lags): $\Delta y_t = \alpha + \gamma\, y_{t-1} + e_t$
                \item Corrects for autocorrelation and heteroskedasticity in $e_t$ via \textbf{nonparametric correction} (Newey-West) of the $t$-statistic
            \end{itemize}
        \end{defn}
        \vspace{-0.2cm}
        \begin{columns}[T]
            \begin{column}{0.48\textwidth}
                \begin{block}{PP Test Statistic}
                    \begin{itemize}\setlength{\itemsep}{0pt}
                        \item $Z_t = t_\gamma \cdot \sqrt{\frac{s_e^2}{\hat{\lambda}^2}} - \frac{T(\hat{\lambda}^2 - s_e^2)}{2\hat{\lambda}^2 \cdot SE(\hat{\gamma})}$
                        \item $\hat{\lambda}^2$: long-run variance (Newey-West kernel)
                        \item $s_e^2$: OLS residual variance
                        \item Critical values: same as ADF (Dickey-Fuller distribution)
                    \end{itemize}
                \end{block}
            \end{column}
            \begin{column}{0.48\textwidth}
                \begin{exampleblock}{PP vs ADF}
                    \begin{itemize}\setlength{\itemsep}{0pt}
                        \item \textbf{ADF}: adds lagged $\Delta y_{t-j}$ $\Rightarrow$ parametric
                        \item \textbf{PP}: corrects $t$-statistic $\Rightarrow$ nonparametric
                        \item PP more robust to heteroskedasticity
                        \item ADF more robust to MA roots near $-1$
                    \end{itemize}
                \end{exampleblock}
                \vspace{-0.2cm}
                {\scriptsize
                \begin{alertblock}{Python}
                    \texttt{from statsmodels.tsa.stattools import PhillipsPerron}
                \end{alertblock}
                }
            \end{column}
        \end{columns}
    \end{cminipage}
\end{frame}

%=============================================================================
% SECTION 9: REAL DATA APPLICATION
%=============================================================================
\section{Financial Data Application}

\begin{frame}{S\&P 500 analysis: overview}
    \vspace{-0.2cm}
    \begin{center}
        \includegraphics[width=0.98\textwidth, height=0.58\textheight, keepaspectratio]{sp500_analysis.pdf}
    \end{center}
    \vspace{-2mm}
    {\footnotesize
    \begin{block}{Observations}
    \begin{itemize}\setlength{\itemsep}{0pt}
        \item \textbf{Prices}: Upward trend, non-stationary; \textbf{Returns}: Mean $\approx 0$, stationary
        \item \textbf{ACF returns}: $\approx 0$ (efficient); \textbf{ACF $r_t^2$}: Significant (volatility clustering)
    \end{itemize}
    \end{block}
    }
    \hfill\quantlet{TSA\_ch1\_ts\_basics}{https://github.com/QuantLet/TSA/tree/main/TSA_ch1/TSA_ch1_ts_basics}
\end{frame}

\begin{frame}{Stylized facts of financial returns}
    \begin{center}
        \includegraphics[width=0.95\textwidth, height=0.48\textheight, keepaspectratio]{returns_distribution.pdf}
    \end{center}
    \vspace{-0.1cm}
    \begin{columns}[T]
        \begin{column}{0.48\textwidth}
            {\footnotesize
            \begin{block}{Observed Properties}
            \begin{itemize}\setlength{\itemsep}{0pt}
                \item Negative skewness (left tail)
                \item Excess kurtosis ($\gg 3$)
                \item Heavy tails (fat tails)
            \end{itemize}
            \end{block}
            }
        \end{column}
        \begin{column}{0.48\textwidth}
            {\footnotesize
            \begin{alertblock}{Implications}
            \begin{itemize}\setlength{\itemsep}{0pt}
                \item Normal distribution inadequate
                \item Extreme events more likely
                \item Student-t or GED required
            \end{itemize}
            \end{alertblock}
            }
        \end{column}
    \end{columns}
    \hfill\quantlet{TSA\_ch1\_ts\_basics}{https://github.com/QuantLet/TSA/tree/main/TSA_ch1/TSA_ch1_ts_basics}
\end{frame}

\begin{frame}{Volatility clustering}
    \vspace{-0.3cm}
    \begin{center}
        \includegraphics[width=0.95\textwidth, height=0.52\textheight, keepaspectratio]{volatility_clustering.pdf}
    \end{center}
    \vspace{-2mm}
    {\footnotesize
    \begin{block}{Observations}
    \begin{itemize}\setlength{\itemsep}{0pt}
        \item Large returns (in absolute value) followed by large returns
        \item Calm periods followed by calm periods
        \item \textbf{Time-varying volatility} $\Rightarrow$ ARCH/GARCH models (Ch. 5)
    \end{itemize}
    \end{block}
    }
    \hfill\quantlet{TSA\_ch1\_ts\_basics}{https://github.com/QuantLet/TSA/tree/main/TSA_ch1/TSA_ch1_ts_basics}
\end{frame}

%=============================================================================
% CASE STUDY
%=============================================================================
\section{Case Study: Stationarity Testing}

\begin{frame}{Case study: Romanian quarterly GDP}
    \vspace{-0.3cm}
    \begin{center}
        \includegraphics[width=0.95\textwidth, height=0.50\textheight, keepaspectratio]{ch1_case_gdp_raw.pdf}
    \end{center}
    \vspace{-2mm}
    \quantlet{TSA\_ch1\_case\_gdp}{https://github.com/QuantLet/TSA/tree/main/TSA_ch1/TSA_ch1_case_gdp}
    \vspace{-0.2cm}
    {\footnotesize
    \begin{block}{Initial Analysis}
        \begin{itemize}\setlength{\itemsep}{0pt}
            \item \textbf{Data}: Romanian quarterly GDP 2010--2023 (56 obs., INS/Eurostat)
            \item \textbf{Observations}: Upward trend, possibly seasonal
            \begin{itemize}
                \item COVID-19 structural shock visible
            \end{itemize}
            \item \textbf{Hypothesis}: Non-stationary series $\Rightarrow$ test with ADF and KPSS
        \end{itemize}
    \end{block}
    }
\end{frame}

\begin{frame}{Stationarity testing: ADF and KPSS}
    \begin{columns}[T]
        \column{0.48\textwidth}
        \begin{block}{ADF Test}
            \begin{itemize}\setlength{\itemsep}{0pt}
                \item \textbf{Hypothesis}: $H_0$: Unit root
                \item \textbf{Result}: ADF stat.: $-1.23$
                \begin{itemize}
                    \item Critical value: $-2.89$
                    \item Fail to reject $H_0$
                \end{itemize}
            \end{itemize}
        \end{block}
        \column{0.48\textwidth}
        \begin{exampleblock}{KPSS Test}
            \begin{itemize}\setlength{\itemsep}{0pt}
                \item \textbf{Hypothesis}: $H_0$: Stationary
                \item \textbf{Result}: KPSS stat.: $0.89$
                \begin{itemize}
                    \item Critical value: $0.46$
                    \item Reject $H_0$
                \end{itemize}
            \end{itemize}
        \end{exampleblock}
    \end{columns}
    \vspace{0.2cm}
    \begin{alertblock}{Conclusion: Both Tests Agree}
        \begin{itemize}\setlength{\itemsep}{0pt}
            \item The GDP series is \textbf{non-stationary} $\Rightarrow$ requires differencing
        \end{itemize}
    \end{alertblock}
\end{frame}

\begin{frame}{Differencing: transformation to stationarity}
    \begin{columns}[T]
        \column{0.48\textwidth}
        \begin{alertblock}{After Differencing}
            \begin{itemize}\setlength{\itemsep}{0pt}
                \item \textbf{Tests}: Both confirm stationarity
                \begin{itemize}
                    \item ADF: $-4.56$ (p < 0.01)
                    \item KPSS: $0.21$ (p > 0.10)
                \end{itemize}
            \end{itemize}
        \end{alertblock}
        \column{0.48\textwidth}
        \begin{exampleblock}{Conclusion}
            \begin{itemize}\setlength{\itemsep}{0pt}
                \item \textbf{GDP level}: non-stationary
                \item \textbf{$\Delta$GDP}: stationary
                \begin{itemize}
                    \item Use $\Delta GDP_t$ for modeling
                \end{itemize}
            \end{itemize}
        \end{exampleblock}
    \end{columns}
    \vspace{0.2cm}
    \begin{block}{Final Result}
        \begin{itemize}\setlength{\itemsep}{0pt}
            \item GDP requires one differencing to become stationary
        \end{itemize}
    \end{block}
\end{frame}

%=============================================================================
\section{AI Use Case}
%=============================================================================

\begin{frame}{AI Exercise: Critical Thinking}
    \vspace{-2mm}
    \begin{block}{\footnotesize Prompt to test in ChatGPT / Claude / Copilot}
        {\small
        ``Using yfinance, download daily EUR/RON exchange rate data (EURRON=X) from 2020-01-01 to 2024-12-31 (approx.\ 1{,}250 observations). Test whether the series is stationary using ADF and KPSS tests. Fit an appropriate model and forecast the exchange rate for the next 5 trading days. Tell me if the forecast is reliable.''
        }
    \end{block}
    \vspace{-1mm}
    {\small
    \textbf{Exercise}:
    \begin{enumerate}\setlength{\itemsep}{2pt}
        \item Run the prompt in an LLM of your choice and critically analyze the response.
        \item Download real EUR/RON data and reproduce the analysis. Do the results match?
        \item Is the ADF test correctly specified (trend, lags)? What changes if you modify the options?
        \item Compare the AI model's forecast against a na\"ive benchmark ($\hat{X}_{t+1} = X_t$).
        \item If the series is a random walk, does fitting an ARMA model make sense?
    \end{enumerate}
    }
    \vspace{-1mm}
    \begin{alertblock}{}
        {\small \textbf{Warning}: Low RMSE and significant coefficients \textit{do not guarantee} a useful forecast.}
    \end{alertblock}
\end{frame}

%=============================================================================
% SUMMARY
%=============================================================================
\section{Summary}

\begin{frame}{Key takeaways}
    \begin{block}{Summary}
    \small
    \begin{itemize}\setlength{\itemsep}{0pt}
        \item \textbf{Stochastic process}: collection of random variables indexed by time
        \item \textbf{Weak stationarity}: constant mean, variance, autocovariance
        \item \textbf{White noise}: $\varepsilon_t \sim WN(0, \sigma^2)$
        \begin{itemize}
            \item Stationary, ACF $= 0$ for $h \neq 0$
        \end{itemize}
        \item \textbf{Random walk}: $X_t = X_{t-1} + \varepsilon_t$
        \begin{itemize}
            \item Non-stationary, $\Var(X_t) = t\sigma^2$
        \end{itemize}
        \item \textbf{ACF/PACF}: key tools for identifying structure
        \item \textbf{Differencing}: transforms non-stationary series into stationary ones
        \item \textbf{Unit root tests}:
        \begin{itemize}
            \item ADF ($H_0$: unit root) vs KPSS ($H_0$: stationary)
        \end{itemize}
    \end{itemize}
    \end{block}
\end{frame}

\begin{frame}{Important formulas}
    \begin{columns}[T]
        \column{0.48\textwidth}
        {\small
        \begin{block}{Weak Stationarity}
            \begin{itemize}\setlength{\itemsep}{0pt}
                \item \textbf{Constant moments}:
                \begin{itemize}
                    \item $\E[X_t] = \mu$ (constant mean)
                    \item $\Var(X_t) = \sigma^2$ (constant variance)
                \end{itemize}
                \item \textbf{Autocovariance}: $\gamma(h) = \Cov(X_t, X_{t+h})$
                \item \textbf{Autocorrelation}: $\rho(h) = \gamma(h)/\gamma(0)$
            \end{itemize}
        \end{block}

        \begin{block}{Lag Operator}
            \begin{itemize}\setlength{\itemsep}{0pt}
                \item \textbf{Lag}: $LX_t = X_{t-1}$
                \item \textbf{Difference}: $\Delta X_t = (1-L)X_t$
            \end{itemize}
        \end{block}
        }

        \column{0.48\textwidth}
        {\small
        \begin{exampleblock}{White Noise (WN)}
            \begin{itemize}\setlength{\itemsep}{0pt}
                \item \textbf{Model}: $\varepsilon_t \sim WN(0, \sigma^2)$
                \item \textbf{ACF}: $\rho(h) = 0$ for $h \neq 0$
            \end{itemize}
        \end{exampleblock}

        \begin{alertblock}{Random Walk (RW)}
            \begin{itemize}\setlength{\itemsep}{0pt}
                \item \textbf{Model}: $X_t = X_{t-1} + \varepsilon_t$
                \item \textbf{Variance}: $\Var(X_t) = t\sigma^2$ (grows!)
            \end{itemize}
        \end{alertblock}
        }
    \end{columns}
\end{frame}

\begin{frame}{Next chapter preview}
    \begin{columns}[T]
        \column{0.48\textwidth}
        \begin{block}{Chapter 2: ARMA Models}
            \begin{itemize}\setlength{\itemsep}{0pt}
                \item \textbf{AR($p$)}: Autoregressive Models
                \item \textbf{MA($q$)}: Moving Average Models
                \item \textbf{ARMA($p,q$)}: Combined Models
                \item \textbf{Identification}: Using ACF/PACF
            \end{itemize}
        \end{block}
        \column{0.48\textwidth}
        \begin{exampleblock}{What We Will Learn}
            \begin{itemize}\setlength{\itemsep}{0pt}
                \item \textbf{Estimation}: Model parameters
                \item \textbf{Diagnostics}: Model validation
                \item \textbf{Forecasting}: Confidence intervals
                \item \textbf{Selection}: AIC, BIC
            \end{itemize}
        \end{exampleblock}
    \end{columns}
\end{frame}

%=============================================================================
% SECTION: QUIZ
%=============================================================================
\section{Quiz}

\begin{frame}{Question 1}
    \begin{cminipage}{0.95\textwidth}
    \begin{alertblock}{Question}
        \begin{itemize}\setlength{\itemsep}{0pt}
            \item What are the three conditions for weak (covariance) stationarity?
        \end{itemize}
    \end{alertblock}

    \vspace{0.3cm}

    \begin{block}{Answer Choices}

        \textcolor{MainBlue}{\textbf{(A)}} Zero mean, infinite variance, time-dependent covariance\\[3pt]

        \textcolor{MainBlue}{\textbf{(B)}} Constant mean, constant variance, autocovariance depends only on lag\\[3pt]

        \textcolor{MainBlue}{\textbf{(C)}} Normal distribution, independence, unit variance\\[3pt]

        \textcolor{MainBlue}{\textbf{(D)}} Linear trend, constant seasonality, white residuals

    \end{block}
    \end{cminipage}
\end{frame}

\begin{frame}{Question 1: Answer}
    \begin{cminipage}{0.95\textwidth}
    \vspace{-0.2cm}
    \begin{center}
        \includegraphics[width=0.98\textwidth, height=0.58\textheight, keepaspectratio]{ch1_quiz2_stationarity.pdf}
    \end{center}
    \vspace{-3mm}
    {\small
    \begin{exampleblock}{Answer: (B)}
    \begin{itemize}\setlength{\itemsep}{0pt}
        \item $\E[X_t]=\mu$, $\Var(X_t)=\sigma^2$, $\gamma(t,s)=\gamma(|t-s|)$
    \end{itemize}
    \end{exampleblock}
    }
    \hfill\quantlet{TSA\_ch1\_stationarity}{https://github.com/QuantLet/TSA/tree/main/TSA_ch1/TSA_ch1_stationarity}
    \end{cminipage}
\end{frame}

\begin{frame}{Question 2}
    \begin{cminipage}{0.95\textwidth}
    \begin{alertblock}{Question}
        \begin{itemize}\setlength{\itemsep}{0pt}
            \item What is the null hypothesis ($H_0$) of the ADF (Augmented Dickey-Fuller) test?
        \end{itemize}
    \end{alertblock}

    \vspace{0.3cm}

    \begin{block}{Answer Choices}

        \textcolor{MainBlue}{\textbf{(A)}} The series is stationary\\[3pt]

        \textcolor{MainBlue}{\textbf{(B)}} The series has a unit root (is non-stationary)\\[3pt]

        \textcolor{MainBlue}{\textbf{(C)}} The series has no autocorrelation\\[3pt]

        \textcolor{MainBlue}{\textbf{(D)}} The series has a normal distribution

    \end{block}
    \end{cminipage}
\end{frame}

\begin{frame}{Question 2: Answer}
    \begin{cminipage}{0.95\textwidth}
    \vspace{-0.2cm}
    \begin{center}
        \includegraphics[width=0.98\textwidth, height=0.58\textheight, keepaspectratio]{adf_test_visualization.pdf}
    \end{center}
    \vspace{-3mm}
    {\small
    \begin{exampleblock}{Answer: (B)}
    \begin{itemize}\setlength{\itemsep}{0pt}
        \item $H_0$: unit root; $\tau <$ critical value $\Rightarrow$ stationary
    \end{itemize}
    \end{exampleblock}
    }
    \hfill\quantlet{TSA\_ch1\_unit\_root\_tests}{https://github.com/QuantLet/TSA/tree/main/TSA_ch1/TSA_ch1_unit_root_tests}
    \end{cminipage}
\end{frame}

\begin{frame}{Question 3}
    \begin{cminipage}{0.95\textwidth}
    \begin{alertblock}{Question}
        \begin{itemize}\setlength{\itemsep}{0pt}
            \item What is the null hypothesis ($H_0$) of the KPSS test?
        \end{itemize}
    \end{alertblock}

    \vspace{0.3cm}

    \begin{block}{Answer Choices}

        \textcolor{MainBlue}{\textbf{(A)}} The series has a unit root (non-stationary)\\[3pt]

        \textcolor{MainBlue}{\textbf{(B)}} The series is stationary\\[3pt]

        \textcolor{MainBlue}{\textbf{(C)}} The series is a random walk\\[3pt]

        \textcolor{MainBlue}{\textbf{(D)}} The series has a deterministic trend

    \end{block}
    \end{cminipage}
\end{frame}

\begin{frame}{Question 3: Answer}
    \begin{cminipage}{0.95\textwidth}
    \vspace{-0.2cm}
    \begin{center}
        \includegraphics[width=0.98\textwidth, height=0.58\textheight, keepaspectratio]{ch1_stationarity.pdf}
    \end{center}
    \vspace{-3mm}
    {\small
    \begin{exampleblock}{Answer: (B)}
    \begin{itemize}\setlength{\itemsep}{0pt}
        \item KPSS: $H_0$ stationary (opposite of ADF). Use both tests!
    \end{itemize}
    \end{exampleblock}
    }
    \hfill\quantlet{TSA\_ch1\_unit\_root\_tests}{https://github.com/QuantLet/TSA/tree/main/TSA_ch1/TSA_ch1_unit_root_tests}
    \end{cminipage}
\end{frame}

\begin{frame}{Question 4}
    \begin{cminipage}{0.95\textwidth}
    \begin{alertblock}{Question}
        \begin{itemize}\setlength{\itemsep}{0pt}
            \item What is the key property of the variance of a random walk $X_t = X_{t-1} + \varepsilon_t$?
        \end{itemize}
    \end{alertblock}

    \vspace{0.3cm}

    \begin{block}{Answer Choices}

        \textcolor{MainBlue}{\textbf{(A)}} Variance is constant: $\Var(X_t) = \sigma^2$\\[3pt]

        \textcolor{MainBlue}{\textbf{(B)}} Variance grows linearly with time: $\Var(X_t) = t\sigma^2$\\[3pt]

        \textcolor{MainBlue}{\textbf{(C)}} Variance decreases with time\\[3pt]

        \textcolor{MainBlue}{\textbf{(D)}} Variance is zero

    \end{block}
    \end{cminipage}
\end{frame}

\begin{frame}{Question 4: Answer}
    \begin{cminipage}{0.95\textwidth}
    \vspace{-0.2cm}
    \begin{center}
        \includegraphics[width=0.98\textwidth, height=0.58\textheight, keepaspectratio]{ch1_quiz4_wn_rw.pdf}
    \end{center}
    \vspace{-3mm}
    {\small
    \begin{exampleblock}{Answer: (B)}
    \begin{itemize}\setlength{\itemsep}{0pt}
        \item $\Var(X_t) = t\sigma^2$ grows linearly $\Rightarrow$ non-stationary
    \end{itemize}
    \end{exampleblock}
    }
    \hfill\quantlet{TSA\_ch1\_random\_walk}{https://github.com/QuantLet/TSA/tree/main/TSA_ch1/TSA_ch1_random_walk}
    \end{cminipage}
\end{frame}

\begin{frame}{Question 5}
    \begin{cminipage}{0.95\textwidth}
    \begin{alertblock}{Question}
        \begin{itemize}\setlength{\itemsep}{0pt}
            \item What does the ACF of a random walk (non-stationary series with unit root) look like?
        \end{itemize}
    \end{alertblock}

    \vspace{0.3cm}

    \begin{block}{Answer Choices}

        \textcolor{MainBlue}{\textbf{(A)}} All values are zero after lag 0\\[3pt]

        \textcolor{MainBlue}{\textbf{(B)}} Decays exponentially fast\\[3pt]

        \textcolor{MainBlue}{\textbf{(C)}} Decays very slowly (high persistence)\\[3pt]

        \textcolor{MainBlue}{\textbf{(D)}} Oscillates between positive and negative

    \end{block}
    \end{cminipage}
\end{frame}

\begin{frame}{Question 5: Answer}
    \begin{cminipage}{0.95\textwidth}
    \vspace{-0.2cm}
    \begin{center}
        \includegraphics[width=0.98\textwidth, height=0.58\textheight, keepaspectratio]{rw_vs_stationary.pdf}
    \end{center}
    \vspace{-3mm}
    {\small
    \begin{exampleblock}{Answer: (C)}
    \begin{itemize}\setlength{\itemsep}{0pt}
        \item ACF $\approx$ 1 for many lags, slow decay $\Rightarrow$ ADF test
    \end{itemize}
    \end{exampleblock}
    }
    \hfill\quantlet{TSA\_ch1\_random\_walk}{https://github.com/QuantLet/TSA/tree/main/TSA_ch1/TSA_ch1_random_walk}
    \end{cminipage}
\end{frame}

\begin{frame}{Question 6}
    \begin{cminipage}{0.95\textwidth}
    \begin{alertblock}{Question}
        \begin{itemize}\setlength{\itemsep}{0pt}
            \item How do we obtain stationary returns from a financial price series $P_t$?
        \end{itemize}
    \end{alertblock}

    \vspace{0.3cm}

    \begin{block}{Answer Choices}

        \textcolor{MainBlue}{\textbf{(A)}} Simple differencing: $\Delta P_t = P_t - P_{t-1}$\\[3pt]

        \textcolor{MainBlue}{\textbf{(B)}} Log then differencing: $r_t = \ln P_t - \ln P_{t-1}$\\[3pt]

        \textcolor{MainBlue}{\textbf{(C)}} Log only: $\ln P_t$\\[3pt]

        \textcolor{MainBlue}{\textbf{(D)}} Standardization: $(P_t - \bar{P})/s_P$

    \end{block}
    \end{cminipage}
\end{frame}

\begin{frame}{Question 6: Answer}
    \begin{cminipage}{0.95\textwidth}
    \vspace{-0.2cm}
    \begin{center}
        \includegraphics[width=0.98\textwidth, height=0.58\textheight, keepaspectratio]{differencing_effect.pdf}
    \end{center}
    \vspace{-3mm}
    {\small
    \begin{exampleblock}{Answer: (B)}
    \begin{itemize}\setlength{\itemsep}{0pt}
        \item Log returns: $r_t = \ln P_t - \ln P_{t-1}$
        \item First $\ln$ (stabilizes variance), then $\Delta$ (removes trend) $\Rightarrow$ stationary series
    \end{itemize}
    \end{exampleblock}
    }
    \hfill\quantlet{TSA\_ch1\_operators}{https://github.com/QuantLet/TSA/tree/main/TSA_ch1/TSA_ch1_operators}
    \end{cminipage}
\end{frame}

%=============================================================================
% REFERENCES
%=============================================================================
\section{References}

\begin{frame}{Bibliography I}
    \begin{block}{Core Textbooks}
        {\small
        \begin{itemize}\setlength{\itemsep}{0pt}
            \item Hamilton, J.D. (1994). \textit{Time Series Analysis}, Princeton University Press.
            \item Hyndman, R.J., \& Athanasopoulos, G. (2021). \textit{Forecasting: Principles and Practice}, 3rd ed., OTexts.
            \item Shumway, R.H., \& Stoffer, D.S. (2017). \textit{Time Series Analysis and Its Applications}, 4th ed., Springer.
        \end{itemize}
        }
    \end{block}

    \begin{exampleblock}{Classic References}
        {\small
        \begin{itemize}\setlength{\itemsep}{0pt}
            \item Wold, H. (1938). \textit{A Study in the Analysis of Stationary Time Series}, Almqvist \& Wiksell.
            \item Bartlett, M.S. (1946). On the Theoretical Specification and Sampling Properties of Autocorrelated Time-Series, \textit{JRSS Supplement}, 8(1), 27--41.
            \item Box, G.E.P., \& Jenkins, G.M. (1970). \textit{Time Series Analysis: Forecasting and Control}, Holden-Day.
        \end{itemize}
        }
    \end{exampleblock}
\end{frame}

\begin{frame}{Bibliography II}
    \begin{block}{Stationarity Tests}
        {\small
        \begin{itemize}\setlength{\itemsep}{0pt}
            \item Dickey, D.A., \& Fuller, W.A. (1979). Distribution of the Estimators for Autoregressive Time Series with a Unit Root, \textit{JASA}, 74(366), 427--431.
            \item Kwiatkowski, D., et al. (1992). Testing the Null Hypothesis of Stationarity, \textit{Journal of Econometrics}, 54(1--3), 159--178.
        \end{itemize}
        }
    \end{block}

    \begin{exampleblock}{Online Resources and Code}
        {\small
        \begin{itemize}\setlength{\itemsep}{0pt}
            \item \textbf{Quantlet}: \url{https://quantlet.com} -- Code platform for quantitative methods
            \item \textbf{Quantinar}: \url{https://quantinar.com} -- Learning platform for quantitative methods
            \item \textbf{GitHub TSA}: \url{https://github.com/QuantLet/TSA/tree/main/TSA_ch1} -- Python code for this chapter
        \end{itemize}
        }
    \end{exampleblock}
\end{frame}

\begin{frame}{}
    \centering
    \vspace{1cm}

    \Huge\textcolor{MainBlue}{Thank You!}

    \vspace{0.8cm}

    \Large Questions?

    \vspace{1cm}

    \normalsize
    Course materials available at: \url{https://danpele.github.io/Time-Series-Analysis/}

    \vspace{0.3cm}

    \href{https://quantlet.com}{\raisebox{-0.15em}{\includegraphics[height=0.8em]{ql_logo.png}} Quantlet} \hspace{0.5cm}
    \href{https://quantinar.com}{\raisebox{-0.15em}{\includegraphics[height=0.8em]{qr_logo.png}} Quantinar}
\end{frame}

\end{document}
