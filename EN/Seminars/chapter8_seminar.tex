% Seminar 8: Modern Extensions

\documentclass[9pt, aspectratio=169, t]{beamer}
\input{preamble}
\subtitle{Seminar 8: Modern Extensions}

\begin{document}

{
\setbeamertemplate{headline}{}
\setbeamertemplate{footline}{}
\begin{frame}
    \titlepage
\end{frame}
}


\begin{frame}{Seminar Outline}
    \begin{cminipage}{0.95\textwidth}
    \begin{itemize}
        \item \textbf{Multiple Choice Quiz} -- Knowledge check
        \vspace{0.15cm}
        \item \textbf{True/False} -- Conceptual checks
        \vspace{0.15cm}
        \item \textbf{Calculation Exercises} -- Applied practice
        \vspace{0.15cm}
        \item \textbf{AI-Assisted Exercise} -- Critical thinking
        \vspace{0.15cm}
        \item \textbf{Summary} -- Key takeaways
    \end{itemize}
    \end{cminipage}
\end{frame}

%=============================================================================
% MULTIPLE CHOICE QUIZ
%=============================================================================
\section{Multiple Choice Quiz}

\begin{frame}{Quiz 1: Hurst Exponent}
    \begin{cminipage}{0.95\textwidth}
    \begin{alertblock}{Question}
        A time series has Hurst exponent $H = 0.8$. What does this indicate?
    \end{alertblock}

    \vspace{0.4cm}

    \begin{block}{Answer choices}
        \textcolor{MainBlue}{\textbf{(A)}} The series is a pure random walk\\[3pt]
        \textcolor{MainBlue}{\textbf{(B)}} The series has long memory and is persistent (trend-following)\\[3pt]
        \textcolor{MainBlue}{\textbf{(C)}} The series is anti-persistent (mean-reverting)\\[3pt]
        \textcolor{MainBlue}{\textbf{(D)}} The series is stationary I(0)
    \end{block}

    \vspace{0.5cm}

    \begin{center}
        \textit{Answer on next slide...}
    \end{center}
    \end{cminipage}
\end{frame}

\begin{frame}{Quiz 1: Answer}
    \begin{cminipage}{0.95\textwidth}
    {\small
    \begin{exampleblock}{Answer: B -- Long memory and persistence}
    \begin{block}{Answer choices}
        \textcolor{MainBlue}{\textbf{(A)}} The series is a pure random walk \incorrect\\[2pt]
        \textcolor{MainBlue}{\textbf{(B)}} \textbf{\textcolor{Forest}{The series has long memory and is persistent (trend-following)}} \correct\\[2pt]
        \textcolor{MainBlue}{\textbf{(C)}} The series is anti-persistent (mean-reverting) \incorrect\\[2pt]
        \textcolor{MainBlue}{\textbf{(D)}} The series is stationary I(0) \incorrect
    \end{block}

        \vspace{0.1cm}

        \begin{itemize}\setlength{\itemsep}{0pt}
            \item $H = 0.5$: Random walk (no memory)
            \item $0.5 < H < 1$: \textbf{Persistence} -- trend continues
            \item $0 < H < 0.5$: Anti-persistence -- mean reversion
            \item With $H = 0.8 > 0.5$: large values tend to be followed by large values
        \end{itemize}
    \end{exampleblock}
    }
    \end{cminipage}
    \quantlet{TSA\_ch8\_hurst\_interpretation}{https://github.com/QuantLet/TSA/tree/main/TSA_ch8/TSA_ch8_hurst_interpretation}
\end{frame}

\begin{frame}{Quiz 2: Fractional Differencing Parameter}
    \begin{cminipage}{0.95\textwidth}
    \begin{alertblock}{Question}
        In the ARFIMA(p, d, q) model, the parameter $d$ can take values:
    \end{alertblock}

    \vspace{0.4cm}

    \begin{block}{Answer choices}
        \textcolor{MainBlue}{\textbf{(A)}} Only integer values (0, 1, 2, ...)\\[3pt]
        \textcolor{MainBlue}{\textbf{(B)}} Only $d = 0$ or $d = 1$\\[3pt]
        \textcolor{MainBlue}{\textbf{(C)}} Any real value, including fractional\\[3pt]
        \textcolor{MainBlue}{\textbf{(D)}} Only negative values
    \end{block}

    \vspace{0.5cm}

    \begin{center}
        \textit{Answer on next slide...}
    \end{center}
    \end{cminipage}
\end{frame}

\begin{frame}{Quiz 2: Answer}
    \begin{cminipage}{0.95\textwidth}
    {\small
    \begin{exampleblock}{Answer: C -- Any real value}
    \begin{block}{Answer choices}
        \textcolor{MainBlue}{\textbf{(A)}} Only integer values (0, 1, 2, ...) \incorrect\\[2pt]
        \textcolor{MainBlue}{\textbf{(B)}} Only $d = 0$ or $d = 1$ \incorrect\\[2pt]
        \textcolor{MainBlue}{\textbf{(C)}} \textbf{\textcolor{Forest}{Any real value, including fractional}} \correct\\[2pt]
        \textcolor{MainBlue}{\textbf{(D)}} Only negative values \incorrect
    \end{block}

        \vspace{0.1cm}

        \begin{itemize}\setlength{\itemsep}{0pt}
            \item \textbf{Fractional differencing}: $(1-L)^d$ with $d \in \mathbb{R}$
            \item $d = 0$: Stationary series (ARMA); $0 < d < 0.5$: Long memory, stationary
            \item $d = 0.5$: Stationary/non-stationary boundary; $d = 1$: Full differencing (classic ARIMA)
            \item \textbf{Relation to Hurst}: $d = H - 0.5$
        \end{itemize}
    \end{exampleblock}
    }
    \end{cminipage}
    \quantlet{TSA\_ch8\_arfima\_estimation}{https://github.com/QuantLet/TSA/tree/main/TSA_ch8/TSA_ch8_arfima_estimation}
\end{frame}

\begin{frame}{Quiz 3: Long Memory in Financial Series}
    \begin{cminipage}{0.95\textwidth}
    \begin{alertblock}{Question}
        In which financial series is long memory most commonly documented?
    \end{alertblock}

    \vspace{0.4cm}

    \begin{block}{Answer choices}
        \textcolor{MainBlue}{\textbf{(A)}} Stock prices\\[3pt]
        \textcolor{MainBlue}{\textbf{(B)}} Daily returns\\[3pt]
        \textcolor{MainBlue}{\textbf{(C)}} Volatility (squared returns)\\[3pt]
        \textcolor{MainBlue}{\textbf{(D)}} Trading volume
    \end{block}

    \vspace{0.5cm}

    \begin{center}
        \textit{Answer on next slide...}
    \end{center}
    \end{cminipage}
\end{frame}

\begin{frame}{Quiz 3: Answer}
    \begin{cminipage}{0.95\textwidth}
    {\small
    \begin{exampleblock}{Answer: C -- Volatility}
    \begin{block}{Answer choices}
        \textcolor{MainBlue}{\textbf{(A)}} Stock prices \incorrect\\[2pt]
        \textcolor{MainBlue}{\textbf{(B)}} Daily returns \incorrect\\[2pt]
        \textcolor{MainBlue}{\textbf{(C)}} \textbf{\textcolor{Forest}{Volatility (squared returns)}} \correct\\[2pt]
        \textcolor{MainBlue}{\textbf{(D)}} Trading volume \incorrect
    \end{block}

        \vspace{0.1cm}

        \begin{itemize}\setlength{\itemsep}{0pt}
            \item \textbf{Returns}: Approximately memoryless ($H \approx 0.5$)
            \item \textbf{Volatility}: Pronounced long memory ($H \approx 0.7-0.9$)
            \item Volatility clustering: turbulent periods followed by turbulent periods
            \item This stylized fact is the basis for FIGARCH and HAR-RV models
        \end{itemize}
    \end{exampleblock}
    }
    \end{cminipage}
    \quantlet{TSA\_ch8\_volatility\_long\_memory}{https://github.com/QuantLet/TSA/tree/main/TSA_ch8/TSA_ch8_volatility_long_memory}
\end{frame}

\begin{frame}{Quiz 4: Feature Engineering}
    \begin{cminipage}{0.95\textwidth}
    \begin{alertblock}{Question}
        To apply Random Forest to time series, we must create:
    \end{alertblock}

    \vspace{0.4cm}

    \begin{block}{Answer choices}
        \textcolor{MainBlue}{\textbf{(A)}} Dummy variables for each observation\\[3pt]
        \textcolor{MainBlue}{\textbf{(B)}} Lag features and rolling statistics\\[3pt]
        \textcolor{MainBlue}{\textbf{(C)}} Fourier transforms of the series\\[3pt]
        \textcolor{MainBlue}{\textbf{(D)}} Only the first difference of the series
    \end{block}

    \vspace{0.5cm}

    \begin{center}
        \textit{Answer on next slide...}
    \end{center}
    \end{cminipage}
\end{frame}

\begin{frame}{Quiz 4: Answer}
    \begin{cminipage}{0.95\textwidth}
    {\small
    \begin{exampleblock}{Answer: B -- Lag features and rolling statistics}
    \begin{block}{Answer choices}
        \textcolor{MainBlue}{\textbf{(A)}} Dummy variables for each observation \incorrect\\[2pt]
        \textcolor{MainBlue}{\textbf{(B)}} \textbf{\textcolor{Forest}{Lag features and rolling statistics}} \correct\\[2pt]
        \textcolor{MainBlue}{\textbf{(C)}} Fourier transforms of the series \incorrect\\[2pt]
        \textcolor{MainBlue}{\textbf{(D)}} Only the first difference of the series \incorrect
    \end{block}

        \vspace{0.1cm}

        \begin{itemize}\setlength{\itemsep}{0pt}
            \item \textbf{Lag features}: $y_{t-1}, y_{t-2}, \ldots, y_{t-k}$
            \item \textbf{Rolling statistics}: rolling mean, rolling std, min/max over window
            \item \textbf{Calendar features}: day of week, month, etc.
            \item Transforms the forecasting problem into a \textbf{supervised regression} problem
        \end{itemize}
    \end{exampleblock}
    }
    \end{cminipage}
    \quantlet{TSA\_ch8\_model\_predictions}{https://github.com/QuantLet/TSA/tree/main/TSA_ch8/TSA_ch8_model_predictions}
\end{frame}

\begin{frame}{Quiz 5: Time Series Cross-Validation}
    \begin{cminipage}{0.95\textwidth}
    \begin{alertblock}{Question}
        Why can't we use standard k-fold cross-validation for time series?
    \end{alertblock}

    \vspace{0.4cm}

    \begin{block}{Answer choices}
        \textcolor{MainBlue}{\textbf{(A)}} It's too slow for long series\\[3pt]
        \textcolor{MainBlue}{\textbf{(B)}} It violates temporal order and causes data leakage\\[3pt]
        \textcolor{MainBlue}{\textbf{(C)}} It only works for classification\\[3pt]
        \textcolor{MainBlue}{\textbf{(D)}} It requires too much data
    \end{block}

    \vspace{0.5cm}

    \begin{center}
        \textit{Answer on next slide...}
    \end{center}
    \end{cminipage}
\end{frame}

\begin{frame}{Quiz 5: Answer}
    \begin{cminipage}{0.95\textwidth}
    {\footnotesize
    \begin{exampleblock}{Answer: B -- Violates temporal order}
    \begin{block}{Answer choices}
        \textcolor{MainBlue}{\textbf{(A)}} It's too slow for long series \incorrect\\[1pt]
        \textcolor{MainBlue}{\textbf{(B)}} \textbf{\textcolor{Forest}{It violates temporal order and causes data leakage}} \correct\\[1pt]
        \textcolor{MainBlue}{\textbf{(C)}} It only works for classification \incorrect\\[1pt]
        \textcolor{MainBlue}{\textbf{(D)}} It requires too much data \incorrect
    \end{block}

        \vspace{0.1cm}

        \begin{itemize}\setlength{\itemsep}{0pt}
            \item Shuffles observations temporally $\Rightarrow$ trains on future, tests on past
            \item \textbf{Data leakage} $\Rightarrow$ overestimated performance
        \end{itemize}

        \vspace{0.1cm}
        \textbf{Solution: Time Series Split (Walk-Forward)}
        \begin{center}
        \vspace{-0.1cm}
        \begin{tikzpicture}[scale=0.6]
            \draw[fill=MainBlue!30] (0,0) rectangle (3,0.4) node[midway] {\tiny Train};
            \draw[fill=Crimson!30] (3,0) rectangle (4,0.4) node[midway] {\tiny Test};
            \draw[fill=MainBlue!30] (0,-0.55) rectangle (4,-0.15) node[midway] {\tiny Train};
            \draw[fill=Crimson!30] (4,-0.55) rectangle (5,-0.15) node[midway] {\tiny Test};
            \draw[fill=MainBlue!30] (0,-1.1) rectangle (5,-0.7) node[midway] {\tiny Train};
            \draw[fill=Crimson!30] (5,-1.1) rectangle (6,-0.7) node[midway] {\tiny Test};
        \end{tikzpicture}
        \end{center}
    \end{exampleblock}
    }
    \end{cminipage}
    \quantlet{TSA\_ch8\_timeseries\_cv}{https://github.com/QuantLet/TSA/tree/main/TSA_ch8/TSA_ch8_timeseries_cv}
\end{frame}

\begin{frame}{Quiz 6: Feature Importance in Random Forest}
    \begin{cminipage}{0.95\textwidth}
    \begin{alertblock}{Question}
        Feature importance in Random Forest for time series helps us:
    \end{alertblock}

    \vspace{0.4cm}

    \begin{block}{Answer choices}
        \textcolor{MainBlue}{\textbf{(A)}} Eliminate all low-importance variables\\[3pt]
        \textcolor{MainBlue}{\textbf{(B)}} Identify which lags and features are most predictive\\[3pt]
        \textcolor{MainBlue}{\textbf{(C)}} Determine Granger causality\\[3pt]
        \textcolor{MainBlue}{\textbf{(D)}} Calculate confidence intervals
    \end{block}

    \vspace{0.5cm}

    \begin{center}
        \textit{Answer on next slide...}
    \end{center}
    \end{cminipage}
\end{frame}

\begin{frame}{Quiz 6: Answer}
    \begin{cminipage}{0.95\textwidth}
    {\small
    \begin{exampleblock}{Answer: B -- Identifies predictive features}
    \begin{block}{Answer choices}
        \textcolor{MainBlue}{\textbf{(A)}} Eliminate all low-importance variables \incorrect\\[2pt]
        \textcolor{MainBlue}{\textbf{(B)}} \textbf{\textcolor{Forest}{Identify which lags and features are most predictive}} \correct\\[2pt]
        \textcolor{MainBlue}{\textbf{(C)}} Determine Granger causality \incorrect\\[2pt]
        \textcolor{MainBlue}{\textbf{(D)}} Calculate confidence intervals \incorrect
    \end{block}

        \vspace{0.1cm}

        \begin{itemize}\setlength{\itemsep}{0pt}
            \item Understanding temporal structure and selecting optimal number of lags
            \item Importance does \textbf{NOT} imply causality
            \item Correlated variables may share importance
            \item Use for interpretation, not causal inference
        \end{itemize}
    \end{exampleblock}
    }
    \end{cminipage}
    \quantlet{TSA\_ch8\_case\_feature\_importance}{https://github.com/QuantLet/TSA/tree/main/TSA_ch8/TSA_ch8_case_feature_importance}
\end{frame}

\begin{frame}{Quiz 7: LSTM Advantage}
    \begin{cminipage}{0.95\textwidth}
    \begin{alertblock}{Question}
        What is the main advantage of LSTM over simple RNNs?
    \end{alertblock}

    \vspace{0.4cm}

    \begin{block}{Answer choices}
        \textcolor{MainBlue}{\textbf{(A)}} It's faster to train\\[3pt]
        \textcolor{MainBlue}{\textbf{(B)}} It solves the vanishing/exploding gradient problem\\[3pt]
        \textcolor{MainBlue}{\textbf{(C)}} It requires less data\\[3pt]
        \textcolor{MainBlue}{\textbf{(D)}} It's easier to interpret
    \end{block}

    \vspace{0.5cm}

    \begin{center}
        \textit{Answer on next slide...}
    \end{center}
    \end{cminipage}
\end{frame}

\begin{frame}{Quiz 7: Answer}
    \begin{cminipage}{0.95\textwidth}
    {\small
    \begin{exampleblock}{Answer: B -- Solves the gradient problem}
    \begin{block}{Answer choices}
        \textcolor{MainBlue}{\textbf{(A)}} It's faster to train \incorrect\\[2pt]
        \textcolor{MainBlue}{\textbf{(B)}} \textbf{\textcolor{Forest}{It solves the vanishing/exploding gradient problem}} \correct\\[2pt]
        \textcolor{MainBlue}{\textbf{(C)}} It requires less data \incorrect\\[2pt]
        \textcolor{MainBlue}{\textbf{(D)}} It's easier to interpret \incorrect
    \end{block}

        \vspace{0.1cm}

        \begin{itemize}\setlength{\itemsep}{0pt}
            \item Simple RNN: gradients decay exponentially, cannot learn long-term dependencies
            \item LSTM gates: \textbf{Forget gate} (what to forget), \textbf{Input gate} (what to remember), \textbf{Output gate} (what to output)
            \item \textbf{Cell state}: highway for information flow -- gradients can flow without degradation
        \end{itemize}
    \end{exampleblock}
    }
    \end{cminipage}
    \quantlet{TSA\_ch8\_lstm\_architecture}{https://github.com/QuantLet/TSA/tree/main/TSA_ch8/TSA_ch8_lstm_architecture}
\end{frame}

\begin{frame}{Quiz 8: Data Preparation for LSTM}
    \begin{cminipage}{0.95\textwidth}
    \begin{alertblock}{Question}
        Before training an LSTM, data should be:
    \end{alertblock}

    \vspace{0.4cm}

    \begin{block}{Answer choices}
        \textcolor{MainBlue}{\textbf{(A)}} Log-transformed\\[3pt]
        \textcolor{MainBlue}{\textbf{(B)}} Normalized/scaled to [0,1] or [-1,1]\\[3pt]
        \textcolor{MainBlue}{\textbf{(C)}} Differenced twice\\[3pt]
        \textcolor{MainBlue}{\textbf{(D)}} Converted to integers
    \end{block}

    \vspace{0.5cm}

    \begin{center}
        \textit{Answer on next slide...}
    \end{center}
    \end{cminipage}
\end{frame}

\begin{frame}{Quiz 8: Answer}
    \begin{cminipage}{0.95\textwidth}
    {\small
    \begin{exampleblock}{Answer: B -- Normalized/scaled}
    \begin{block}{Answer choices}
        \textcolor{MainBlue}{\textbf{(A)}} Log-transformed \incorrect\\[2pt]
        \textcolor{MainBlue}{\textbf{(B)}} \textbf{\textcolor{Forest}{Normalized/scaled to [0,1] or [-1,1]}} \correct\\[2pt]
        \textcolor{MainBlue}{\textbf{(C)}} Differenced twice \incorrect\\[2pt]
        \textcolor{MainBlue}{\textbf{(D)}} Converted to integers \incorrect
    \end{block}

        \vspace{0.1cm}

        \begin{itemize}\setlength{\itemsep}{0pt}
            \item Activation functions (sigmoid, tanh) work in limited ranges
            \item \textbf{Min-Max}: $x' = \frac{x - x_{min}}{x_{max} - x_{min}}$ $\succ$ [0, 1]
            \item \textbf{Standard}: $x' = \frac{x - \mu}{\sigma}$ $\succ$ mean 0, std 1
            \item \textbf{Important}: Fit on train, transform on train+test!
        \end{itemize}
    \end{exampleblock}
    }
    \end{cminipage}
    \quantlet{TSA\_ch8\_data\_split}{https://github.com/QuantLet/TSA/tree/main/TSA_ch8/TSA_ch8_data_split}
\end{frame}

\begin{frame}{Quiz 9: LSTM Hyperparameters}
    \begin{cminipage}{0.95\textwidth}
    \begin{alertblock}{Question}
        Which is NOT a typical LSTM hyperparameter?
    \end{alertblock}

    \vspace{0.4cm}

    \begin{block}{Answer choices}
        \textcolor{MainBlue}{\textbf{(A)}} Number of units (neurons) per layer\\[3pt]
        \textcolor{MainBlue}{\textbf{(B)}} Input sequence length\\[3pt]
        \textcolor{MainBlue}{\textbf{(C)}} Learning rate\\[3pt]
        \textcolor{MainBlue}{\textbf{(D)}} Differencing parameter $d$
    \end{block}

    \vspace{0.5cm}

    \begin{center}
        \textit{Answer on next slide...}
    \end{center}
    \end{cminipage}
\end{frame}

\begin{frame}{Quiz 9: Answer}
    \begin{cminipage}{0.95\textwidth}
    {\small
    \begin{exampleblock}{Answer: D -- The $d$ parameter}
    \begin{block}{Answer choices}
        \textcolor{MainBlue}{\textbf{(A)}} Number of units (neurons) per layer \incorrect\\[2pt]
        \textcolor{MainBlue}{\textbf{(B)}} Input sequence length \incorrect\\[2pt]
        \textcolor{MainBlue}{\textbf{(C)}} Learning rate \incorrect\\[2pt]
        \textcolor{MainBlue}{\textbf{(D)}} \textbf{\textcolor{Forest}{Differencing parameter $d$}} \correct
    \end{block}

        \vspace{0.1cm}

        \begin{itemize}\setlength{\itemsep}{0pt}
            \item $d$ is specific to ARFIMA models, not LSTM!
            \item \textbf{Architecture}: number of layers, units/layer
            \item \textbf{Sequence}: lookback length
            \item \textbf{Training}: learning rate, batch size, epochs
            \item \textbf{Regularization}: dropout, early stopping
        \end{itemize}
    \end{exampleblock}
    }
    \end{cminipage}
    \quantlet{TSA\_ch8\_case\_lstm\_training}{https://github.com/QuantLet/TSA/tree/main/TSA_ch8/TSA_ch8_case_lstm_training}
\end{frame}

%=============================================================================
% TRUE/FALSE
%=============================================================================
\section{True/False}

\begin{frame}{True or False? --- Questions}
    \begin{cminipage}{0.95\textwidth}
    \footnotesize
    \begin{center}
    \begin{tabular}{p{9cm}c}
        \toprule
        \textbf{Statement} & \textbf{T/F?} \\
        \midrule
        1. ARFIMA models can capture long-term dependence. & ? \\[0.15cm]
        2. The parameter $d$ in ARFIMA must be an integer. & ? \\[0.15cm]
        3. LSTM networks are better than ARIMA in all situations. & ? \\[0.15cm]
        4. Random Forest requires manually created features. & ? \\[0.15cm]
        5. Standard cross-validation (k-fold) is suitable for time series. & ? \\[0.15cm]
        6. The Hurst exponent $H > 0.5$ indicates positive long memory. & ? \\
        \bottomrule
    \end{tabular}
    \end{center}
    \end{cminipage}
\end{frame}

\begin{frame}{True or False? --- Answers}
    \begin{cminipage}{0.95\textwidth}
    \scriptsize
    \begin{center}
    \begin{tabular}{p{7.5cm}cc}
        \toprule
        \textbf{Statement} & \textbf{T/F} & \textbf{Explanation} \\
        \midrule
        1. ARFIMA captures long-term dependence. & \textcolor{Forest}{\textbf{T}} & {\tiny Fractional $d$} \\[0.08cm]
        2. $d$ in ARFIMA must be an integer. & \textcolor{Crimson}{\textbf{F}} & {\tiny $d \in (0, 0.5)$ fractional} \\[0.08cm]
        3. LSTM better than ARIMA always. & \textcolor{Crimson}{\textbf{F}} & {\tiny Depends on data and sample} \\[0.08cm]
        4. RF requires manual features. & \textcolor{Forest}{\textbf{T}} & {\tiny Lags, calendar, etc.} \\[0.08cm]
        5. k-fold CV suitable for time series. & \textcolor{Crimson}{\textbf{F}} & {\tiny Violates temporal ordering} \\[0.08cm]
        6. $H > 0.5$ indicates positive long memory. & \textcolor{Forest}{\textbf{T}} & {\tiny $H = 0.5$: no memory} \\
        \bottomrule
    \end{tabular}
    \end{center}
    \end{cminipage}
\end{frame}

%=============================================================================
% CALCULATION EXERCISES
%=============================================================================
\section{Calculation Exercises}

\begin{frame}{Exercise 1: Hurst Exponent Estimation}
    \begin{cminipage}{0.95\textwidth}
    \begin{alertblock}{Problem}
        \begin{itemize}\setlength{\itemsep}{0pt}
            \item \textbf{Task}: Given daily Bitcoin returns, estimate the Hurst exponent using the R/S method and interpret the result.
            \item \textbf{Formula}: $\log(R/S) = H \cdot \log(n) + c$
        \end{itemize}
    \end{alertblock}

    \vspace{0.2cm}
    \begin{exampleblock}{Solution}
        \begin{enumerate}\setlength{\itemsep}{0pt}
            \item Calculate mean over subintervals of different lengths $n$
            \item For each $n$: calculate Range($R$) and Std($S$)
            \item The ratio $R/S$ grows as $n^H$
            \item Fit regression: $\log(R/S) = H \cdot \log(n) + c$
        \end{enumerate}

        \vspace{0.2cm}
        \textbf{Python code}: \texttt{nolds.hurst\_rs(returns)}
    \end{exampleblock}
    \end{cminipage}
    \quantlet{TSA\_ch8\_hurst\_interpretation}{https://github.com/QuantLet/TSA/tree/main/TSA_ch8/TSA_ch8_hurst_interpretation}
\end{frame}

\begin{frame}{Exercise 1: Solution and Interpretation}
    \begin{cminipage}{0.95\textwidth}
    \begin{exampleblock}{Typical Bitcoin Results}
        \begin{itemize}\setlength{\itemsep}{0pt}
            \item Returns: $H \approx 0.45-0.55$ (approximately random walk)
            \item Volatility (|returns|): $H \approx 0.75-0.85$ (long memory!)
        \end{itemize}
    \end{exampleblock}

    \vspace{0.3cm}
    \begin{itemize}\setlength{\itemsep}{0pt}
        \item Returns are hard to predict (EMH approximately valid)
        \item Volatility is predictable over long horizons
        \item Implications for risk management and VaR
    \end{itemize}

    \vspace{0.2cm}
    \textbf{Application}: FIGARCH models may outperform standard GARCH
    \end{cminipage}
    \quantlet{TSA\_ch8\_hurst\_interpretation}{https://github.com/QuantLet/TSA/tree/main/TSA_ch8/TSA_ch8_hurst_interpretation}
\end{frame}

\begin{frame}{Exercise 2: Random Forest for Forecasting}
    \begin{cminipage}{0.95\textwidth}
    \begin{alertblock}{Problem}
        \begin{itemize}\setlength{\itemsep}{0pt}
            \item \textbf{Task}: Build a Random Forest model for 1-day ahead Bitcoin price forecasting. Evaluate using TimeSeriesSplit.
        \end{itemize}
    \end{alertblock}

    \vspace{0.2cm}
    \begin{exampleblock}{Solution Pipeline}
        \begin{enumerate}\setlength{\itemsep}{0pt}
            \item \textbf{Feature engineering}:
                \begin{itemize}\setlength{\itemsep}{0pt}
                    \item Lags: $y_{t-1}, y_{t-2}, \ldots, y_{t-7}$
                    \item Rolling mean/std: 7, 14, 30 days
                \end{itemize}
            \item \textbf{Train/Test split}: TimeSeriesSplit(n\_splits=5)
            \item \textbf{Model}: RandomForestRegressor(n\_estimators=100)
            \item \textbf{Evaluation}: RMSE, MAE, Direction Accuracy
        \end{enumerate}
    \end{exampleblock}
    \end{cminipage}
    \quantlet{TSA\_ch8\_feature\_engineering}{https://github.com/QuantLet/TSA/tree/main/TSA_ch8/TSA_ch8_feature_engineering}
\end{frame}

\begin{frame}{Exercise 2: Code and Results}
    \begin{cminipage}{0.95\textwidth}
    \begin{exampleblock}{Python Code}
        {\footnotesize
        \texttt{from sklearn.ensemble import RandomForestRegressor}\\
        \texttt{from sklearn.model\_selection import TimeSeriesSplit}\\[0.2cm]
        \texttt{tscv = TimeSeriesSplit(n\_splits=5)}\\
        \texttt{rf = RandomForestRegressor(n\_estimators=100)}\\[0.2cm]
        \texttt{for train\_idx, test\_idx in tscv.split(X):}\\
        \texttt{~~~~rf.fit(X[train\_idx], y[train\_idx])}\\
        \texttt{~~~~pred = rf.predict(X[test\_idx])}
        }
    \end{exampleblock}

    \vspace{0.2cm}
    \begin{itemize}\setlength{\itemsep}{0pt}
        \item Direction accuracy: 52-55\% (slightly above random)
        \item Feature importance: lag-1 and rolling\_std dominate
    \end{itemize}
    \end{cminipage}
    \quantlet{TSA\_ch8\_rf\_prediction}{https://github.com/QuantLet/TSA/tree/main/TSA_ch8/TSA_ch8_rf_prediction}
\end{frame}

\begin{frame}{Exercise 3: LSTM for Time Series}
    \begin{cminipage}{0.95\textwidth}
    \begin{alertblock}{Problem}
        \begin{itemize}\setlength{\itemsep}{0pt}
            \item \textbf{Task}: Implement a simple LSTM model for Bitcoin forecasting. Compare with Random Forest.
        \end{itemize}
    \end{alertblock}

    \vspace{0.2cm}
    \begin{exampleblock}{Simple LSTM Architecture}
        \begin{enumerate}\setlength{\itemsep}{0pt}
            \item Input: 30-day sequences
            \item LSTM layer: 50 units
            \item Dense output: 1 neuron (forecast)
            \item Loss: MSE, Optimizer: Adam
        \end{enumerate}

        \vspace{0.2cm}
        \textbf{Important steps}:
        \begin{itemize}\setlength{\itemsep}{0pt}
            \item MinMaxScaler normalization
            \item Reshape to [samples, timesteps, features]
            \item Early stopping to prevent overfitting
        \end{itemize}
    \end{exampleblock}
    \end{cminipage}
    \quantlet{TSA\_ch8\_lstm\_architecture}{https://github.com/QuantLet/TSA/tree/main/TSA_ch8/TSA_ch8_lstm_architecture}
\end{frame}

\begin{frame}{Exercise 3: LSTM Code}
    \begin{cminipage}{0.95\textwidth}
    \begin{exampleblock}{Keras/TensorFlow Code}
        {\footnotesize
        \texttt{from tensorflow.keras.models import Sequential}\\
        \texttt{from tensorflow.keras.layers import LSTM, Dense}\\[0.2cm]
        \texttt{model = Sequential([}\\
        \texttt{~~~~LSTM(50, input\_shape=(30, 1)),}\\
        \texttt{~~~~Dense(1)}\\
        \texttt{])}\\[0.2cm]
        \texttt{model.compile(optimizer='adam', loss='mse')}\\
        \texttt{model.fit(X\_train, y\_train, epochs=50,}\\
        \texttt{~~~~~~~~~~validation\_split=0.1, verbose=0)}
        }
    \end{exampleblock}

    \vspace{0.2cm}
    \begin{itemize}\setlength{\itemsep}{0pt}
        \item RMSE similar (LSTM slightly better on smooth data)
        \item RF: faster, more interpretable
        \item LSTM: captures complex patterns better
    \end{itemize}
    \end{cminipage}
    \quantlet{TSA\_ch8\_lstm\_architecture}{https://github.com/QuantLet/TSA/tree/main/TSA_ch8/TSA_ch8_lstm_architecture}
\end{frame}

%=============================================================================
% AI-ASSISTED EXERCISE
%=============================================================================
\section{AI-Assisted Exercise}

\begin{frame}{AI Exercise: Critical Thinking}
    \begin{cminipage}{0.95\textwidth}
    \vspace{-0.3cm}
    \begin{block}{\footnotesize Prompt to test in ChatGPT / Claude / Copilot}
        {\footnotesize
        ``Download hourly electricity consumption data. Compare three approaches: (1) ARIMA, (2) Random Forest with lag features, (3) LSTM. Use time series cross-validation, report RMSE and directional accuracy for each.''
        }
    \end{block}
    \vspace{-2mm}
    {\footnotesize
    \textbf{Exercise}:
    \begin{enumerate}\setlength{\itemsep}{0pt}
        \item Did the AI use proper time series cross-validation (no future data leakage)?
        \item Are the lag features for Random Forest correctly constructed?
        \item Is the LSTM data properly scaled and sequenced?
        \item Does the comparison use the same test set for all three models?
        \item Which model performs best and why? Is this consistent with theory?
    \end{enumerate}
    }
    \vspace{-2mm}
    \begin{alertblock}{}
        {\footnotesize \textbf{Warning}: AI-generated code may run without errors and look professional. \textit{That does not mean it is correct.}}
    \end{alertblock}
    \end{cminipage}
    \quantlet{TSA\_ch8\_case\_comparison}{https://github.com/QuantLet/TSA/tree/main/TSA_ch8/TSA_ch8_case_comparison}
\end{frame}

%=============================================================================
% END
%=============================================================================
\section{Summary}

\begin{frame}{Summary: When to Use Each Method}
    \begin{cminipage}{0.95\textwidth}
    \vspace{-0.9cm}
    \begin{block}{ARFIMA}
        \begin{itemize}\setlength{\itemsep}{0pt}
            \item Series with long memory (volatility, hydrology)
            \item When $0 < d < 0.5$ is theoretically justified
            \item Statistical interpretability is important
        \end{itemize}
    \end{block}

    \begin{block}{Random Forest}
        \begin{itemize}\setlength{\itemsep}{0pt}
            \item Nonlinear relationships between features
            \item Feature importance for understanding
            \item Structured data, not too long series
        \end{itemize}
    \end{block}

    \begin{block}{LSTM}
        \begin{itemize}\setlength{\itemsep}{0pt}
            \item Long sequences with complex dependencies
            \item Sufficient data for deep learning
            \item Patterns difficult to capture with classical methods
        \end{itemize}
    \end{block}
    \end{cminipage}
    \quantlet{TSA\_ch8\_case\_comparison}{https://github.com/QuantLet/TSA/tree/main/TSA_ch8/TSA_ch8_case_comparison}
\end{frame}

\begin{frame}[shrink=5]{Key Formulas}
    \begin{cminipage}{0.95\textwidth}
    \vspace{-0.8cm}
    \begin{block}{ARFIMA and Long Memory}
        \begin{itemize}\setlength{\itemsep}{0pt}
            \item Fractional differencing: $(1-L)^d y_t = \varepsilon_t$
            \item Hurst exponent: $d = H - 0.5$
            \item ACF for long memory: $\rho(k) \sim k^{2d-1}$ (slow decay)
        \end{itemize}
    \end{block}

    \begin{block}{Machine Learning}
        \begin{itemize}\setlength{\itemsep}{0pt}
            \item Lag feature: $X_t = [y_{t-1}, y_{t-2}, \ldots, y_{t-k}]$
            \item RMSE: $\sqrt{\frac{1}{n}\sum(y_i - \hat{y}_i)^2}$
            \item Direction Accuracy: $\frac{1}{n}\sum \mathbf{1}[\text{sign}(\Delta y) = \text{sign}(\Delta \hat{y})]$
        \end{itemize}
    \end{block}

    \begin{block}{LSTM}
        \begin{itemize}\setlength{\itemsep}{0pt}
            \item Forget gate: $f_t = \sigma(W_f \cdot [h_{t-1}, x_t] + b_f)$
            \item Cell update: $C_t = f_t * C_{t-1} + i_t * \tilde{C}_t$
        \end{itemize}
    \end{block}
    \end{cminipage}
    \quantlet{TSA\_ch8\_arfima\_d\_effect}{https://github.com/QuantLet/TSA/tree/main/TSA_ch8/TSA_ch8_arfima_d_effect}
\end{frame}

%=============================================================================
% BIBLIOGRAPHY (same references as in the course)
%=============================================================================
\section{Bibliography}

\begin{frame}{Bibliography I}
    \begin{cminipage}{0.95\textwidth}
    \begin{block}{Fundamental Textbooks}
        {\small
        \begin{itemize}\setlength{\itemsep}{0pt}
            \item Hyndman, R.J., \& Athanasopoulos, G. (2021). \textit{Forecasting: Principles and Practice}, 3rd ed., OTexts.
            \item Shumway, R.H., \& Stoffer, D.S. (2017). \textit{Time Series Analysis and Its Applications}, 4th ed., Springer.
            \item Brockwell, P.J., \& Davis, R.A. (2016). \textit{Introduction to Time Series and Forecasting}, 3rd ed., Springer.
        \end{itemize}
        }
    \end{block}

    \begin{exampleblock}{Financial Time Series}
        {\small
        \begin{itemize}\setlength{\itemsep}{0pt}
            \item Tsay, R.S. (2010). \textit{Analysis of Financial Time Series}, 3rd ed., Wiley.
            \item Franke, J., H\"ardle, W.K., \& Hafner, C.M. (2019). \textit{Statistics of Financial Markets}, 4th ed., Springer.
        \end{itemize}
        }
    \end{exampleblock}
    \end{cminipage}
\end{frame}

\begin{frame}{Bibliography II}
    \begin{cminipage}{0.95\textwidth}
    \begin{block}{Modern Approaches and Machine Learning}
        {\small
        \begin{itemize}\setlength{\itemsep}{0pt}
            \item Nielsen, A. (2019). \textit{Practical Time Series Analysis}, O'Reilly Media.
            \item Petropoulos, F., et al. (2022). \textit{Forecasting: Theory and Practice}, International Journal of Forecasting.
            \item Makridakis, S., Spiliotis, E., \& Assimakopoulos, V. (2020). The M4 Competition, International Journal of Forecasting.
        \end{itemize}
        }
    \end{block}

    \begin{exampleblock}{Online Resources and Code}
        {\small
        \begin{itemize}\setlength{\itemsep}{0pt}
            \item \textbf{Quantlet}: \url{https://quantlet.com} --- Code repository for statistics
            \item \textbf{Quantinar}: \url{https://quantinar.com} --- Quantitative methods learning platform
            \item \textbf{GitHub TSA}: \url{https://github.com/QuantLet/TSA/tree/main/TSA_ch8} --- Python code for this seminar
        \end{itemize}
        }
    \end{exampleblock}
    \end{cminipage}
\end{frame}

\begin{frame}{}
    \begin{cminipage}{0.95\textwidth}
    \centering
    \Huge\textcolor{IDAred}{Thank You!}

    \vspace{1cm}

    \Large\textcolor{MainBlue}{Questions?}

    \vspace{0.8cm}

    \normalsize
    Seminar materials are available at: \url{https://danpele.github.io/Time-Series-Analysis/}

    \vspace{0.2cm}

    \href{https://quantlet.com}{\raisebox{-0.15em}{\includegraphics[height=0.8em]{ql_logo.png}} Quantlet} \hspace{0.5cm}
    \href{https://quantinar.com}{\raisebox{-0.15em}{\includegraphics[height=0.8em]{qr_logo.png}} Quantinar}
    \end{cminipage}
\end{frame}

\end{document}
