% Chapter 2: ARMA Models
% Harvard-quality academic presentation
% Bachelor program, Bucharest University of Economic Studies

\documentclass[9pt, aspectratio=169, t]{beamer}

% Ensure content fits on slides
\setbeamersize{text margin left=8mm, text margin right=8mm}

%=============================================================================
% THEME AND STYLE CONFIGURATION
%=============================================================================
\usetheme{Madrid}
\usecolortheme{seahorse}

% Professional Color Palette
\definecolor{MainBlue}{RGB}{26, 58, 110}
\definecolor{AccentBlue}{RGB}{42, 82, 140}
\definecolor{IDAred}{RGB}{220, 53, 69}
\definecolor{DarkGray}{RGB}{51, 51, 51}
\definecolor{MediumGray}{RGB}{128, 128, 128}
\definecolor{LightGray}{RGB}{248, 248, 248}
\definecolor{VeryLightGray}{RGB}{235, 235, 235}
\definecolor{Crimson}{RGB}{220, 53, 69}
\definecolor{Forest}{RGB}{46, 125, 50}
\definecolor{Amber}{RGB}{181, 133, 63}
\definecolor{Orange}{RGB}{230, 126, 34}
\definecolor{HarvardCrimson}{RGB}{165, 28, 48}

\setbeamercolor{palette primary}{bg=MainBlue, fg=white}
\setbeamercolor{palette secondary}{bg=MainBlue!85, fg=white}
\setbeamercolor{palette tertiary}{bg=MainBlue!70, fg=white}
\setbeamercolor{structure}{fg=MainBlue}
\setbeamercolor{title}{fg=MainBlue}
\setbeamercolor{frametitle}{fg=MainBlue, bg=white}
\setbeamercolor{block title}{bg=MainBlue, fg=white}
\setbeamercolor{block body}{bg=VeryLightGray, fg=DarkGray}
\setbeamercolor{block title alerted}{bg=Crimson, fg=white}
\setbeamercolor{block body alerted}{bg=Crimson!8, fg=DarkGray}
\setbeamercolor{block title example}{bg=Forest, fg=white}
\setbeamercolor{block body example}{bg=Forest!8, fg=DarkGray}
\setbeamercolor{item}{fg=MainBlue}

\setbeamertemplate{navigation symbols}{}

\setbeamertemplate{footline}{
    \leavevmode%
    \hbox{%
        \begin{beamercolorbox}[wd=.333333\paperwidth,ht=2.5ex,dp=1ex,center]{author in head/foot}%
            \usebeamerfont{author in head/foot}\insertshortauthor
        \end{beamercolorbox}%
        \begin{beamercolorbox}[wd=.333333\paperwidth,ht=2.5ex,dp=1ex,center]{title in head/foot}%
            \usebeamerfont{title in head/foot}\insertshorttitle
        \end{beamercolorbox}%
        \begin{beamercolorbox}[wd=.333333\paperwidth,ht=2.5ex,dp=1ex,right]{date in head/foot}%
            \usebeamerfont{date in head/foot}\insertshortdate{}\hspace*{2em}
            \insertframenumber{} / \inserttotalframenumber\hspace*{2ex}
        \end{beamercolorbox}}%
    \vskip0pt%
}

%=============================================================================
% PACKAGES
%=============================================================================
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{amsmath, amssymb, amsthm}
\usepackage{mathtools}
\usepackage{bm}
\usepackage{tikz}
\usetikzlibrary{arrows.meta, positioning, shapes, calc, decorations.pathreplacing}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{array}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{colortbl}
\hypersetup{colorlinks=false, pdfborder={0 0 0}}
\graphicspath{{../logos/}{../charts/}}

%=============================================================================
% THEOREM ENVIRONMENTS
%=============================================================================
\theoremstyle{definition}
\setbeamertemplate{theorems}[numbered]
\newtheorem{defn}{Definition}
\newtheorem{thm}{Theorem}
\newtheorem{prop}{Proposition}
\newtheorem{rmk}{Remark}

%=============================================================================
% CUSTOM COMMANDS
%=============================================================================
\newcommand{\E}{\mathbb{E}}
\newcommand{\Var}{\text{Var}}
\newcommand{\Cov}{\text{Cov}}
\newcommand{\Corr}{\text{Corr}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\N}{\mathbb{N}}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\B}{\mathbf{B}}
\newcommand{\imark}{\textcolor{MainBlue}{\textbullet}}
\newcommand{\RMSE}{\text{RMSE}}
\newcommand{\MAE}{\text{MAE}}
\newcommand{\MAPE}{\text{MAPE}}

%=============================================================================
% TITLE INFORMATION
%=============================================================================
\title[Chapter 2: ARMA Models]{Chapter 2: ARMA Models}
\subtitle{Bachelor Program, Faculty of Cybernetics, Statistics and Economic Informatics, Bucharest University of Economic Studies}
\author[Prof. Daniel Traian Pele, PhD]{Prof. Daniel Traian Pele, PhD\\[0.2cm]\footnotesize\texttt{danpele@ase.ro}}
\institute{Bucharest University of Economic Studies}
\date{Academic Year 2025--2026}

\begin{document}

%=============================================================================
% TITLE SLIDE
%=============================================================================
\begin{frame}[plain]
    \begin{tikzpicture}[remember picture, overlay]
        \fill[IDAred] (current page.north west) rectangle ([yshift=-0.15cm]current page.north east);
        \node[anchor=north west] at ([xshift=0.5cm, yshift=-0.3cm]current page.north west) {
            \href{https://www.ase.ro}{\includegraphics[height=1.1cm]{ase_logo.png}}
        };
        \node[anchor=north] at ([yshift=-0.3cm]current page.north) {
            \href{https://ai4efin.ase.ro}{\includegraphics[height=1.1cm]{ai4efin_logo.png}}
        };
        \node[anchor=north east] at ([xshift=-0.5cm, yshift=-0.3cm]current page.north east) {
            \href{https://www.digital-finance-msca.com}{\includegraphics[height=1.1cm]{msca_logo.png}}
        };
    \end{tikzpicture}
    \vfill
    \begin{center}
        {\Large\textcolor{MediumGray}{Time Series Analysis and Forecasting}}\\[0.3cm]
        {\Huge\textbf{\textcolor{MainBlue}{Chapter 2: ARMA Models}}}\\[0.5cm]
        {\Large\textcolor{IDAred}{Stationary Time Series}}
    \end{center}
    \vfill

    \begin{tikzpicture}[remember picture, overlay]
        \fill[IDAred] (current page.south west) rectangle ([yshift=0.15cm]current page.south east);
        \node[anchor=south west] at ([xshift=0.5cm, yshift=0.8cm]current page.south west) {
            \href{https://theida.net}{\includegraphics[height=0.9cm]{ida_logo.png}}
        };
        \node[anchor=south] at ([xshift=-3cm, yshift=0.8cm]current page.south) {
            \href{https://blockchain-research-center.com}{\includegraphics[height=0.9cm]{brc_logo.png}}
        };
        \node[anchor=south] at ([yshift=0.8cm]current page.south) {
            \href{https://quantinar.com}{\includegraphics[height=0.9cm]{qr_logo.png}}
        };
        \node[anchor=south] at ([xshift=3cm, yshift=0.8cm]current page.south) {
            \href{https://quantlet.com}{\includegraphics[height=0.9cm]{ql_logo.png}}
        };
        \node[anchor=south east] at ([xshift=-0.5cm, yshift=0.8cm]current page.south east) {
            \href{https://ipe.ro/new}{\includegraphics[height=0.9cm]{acad_logo.png}}
        };
    \end{tikzpicture}
\end{frame}

%=============================================================================
% TABLE OF CONTENTS
%=============================================================================
\begin{frame}{Outline}
    \tableofcontents
\end{frame}

%=============================================================================
% MOTIVATION
%=============================================================================
\begin{frame}{Motivating Example: Stationary Processes}
    \vspace{-0.3cm}
    \begin{center}
        \includegraphics[width=0.92\textwidth, height=0.62\textheight, keepaspectratio]{ch2_motivation_stationary.pdf}
    \end{center}
    \vspace{-0.2cm}
    {\footnotesize
    \begin{itemize}
        \item \textbf{AR processes}: Current value depends on past values --- mean-reverting behavior
        \item \textbf{MA processes}: Current value depends on past shocks --- short memory
        \item \textbf{ARMA}: Combines both mechanisms for flexible modeling
    \end{itemize}
    }
\end{frame}

\begin{frame}{Real-World Applications of ARMA}
    \vspace{-0.3cm}
    \begin{center}
        \includegraphics[width=0.95\textwidth, height=0.62\textheight, keepaspectratio]{ch2_motivation_realworld.pdf}
    \end{center}
    \vspace{-0.2cm}
    {\footnotesize
    \begin{alertblock}{Key Insight}
        Many economic and financial series become stationary after simple transformations (returns, growth rates, deviations from trend) --- perfect for ARMA modeling!
    \end{alertblock}
    }
\end{frame}

\begin{frame}{Model Identification via ACF Patterns}
    \vspace{-0.3cm}
    \begin{center}
        \includegraphics[width=0.95\textwidth, height=0.58\textheight, keepaspectratio]{ch2_motivation_acf.pdf}
    \end{center}
    \vspace{-0.2cm}
    {\footnotesize
    \begin{exampleblock}{The ACF Reveals Model Structure}
        Different ARMA models produce distinct ACF patterns --- we can identify the model by examining the data!
    \end{exampleblock}
    }
\end{frame}

%=============================================================================
% SECTION 1: INTRODUCTION AND LAG OPERATOR
%=============================================================================
\section{Introduction and Lag Operator}

\begin{frame}{Recap: Stationarity}
    \textbf{From Chapter 1:} A process $\{X_t\}$ is \textbf{weakly stationary} if:
    \begin{enumerate}
        \item $\E[X_t] = \mu$ (constant mean)
        \item $\Var(X_t) = \sigma^2 < \infty$ (constant, finite variance)
        \item $\Cov(X_t, X_{t+h}) = \gamma(h)$ (covariance depends only on lag $h$)
    \end{enumerate}

    \vspace{0.15cm}
    \textbf{Why stationarity matters for ARMA:}
    \begin{itemize}
        \item ARMA models assume the underlying process is stationary
        \item Non-stationary data must be differenced first (ARIMA)
        \item Stationarity ensures stable model parameters
    \end{itemize}

    \vspace{0.15cm}
    \textbf{Today:} We build models for stationary time series using past values and past errors.
\end{frame}

\begin{frame}{The Lag Operator (Backshift Operator)}
    \begin{defn}[Lag Operator]
        The \textbf{lag operator} $L$ (or backshift operator $B$) shifts a time series back by one period:
        $$L X_t = X_{t-1}$$
    \end{defn}

    \textbf{Properties:}
    \begin{itemize}
        \item $L^k X_t = X_{t-k}$ (shift back $k$ periods)
        \item $L^0 X_t = X_t$ (identity)
        \item $(1-L)X_t = X_t - X_{t-1} = \Delta X_t$ (first difference)
        \item $(1-L)^d X_t = \Delta^d X_t$ ($d$-th difference)
    \end{itemize}

    \vspace{0.15cm}
    \textbf{Lag Polynomials:}
    $$\phi(L) = 1 - \phi_1 L - \phi_2 L^2 - \cdots - \phi_p L^p$$
    $$\theta(L) = 1 + \theta_1 L + \theta_2 L^2 + \cdots + \theta_q L^q$$
\end{frame}

\begin{frame}{Lag Operator: Visual Illustration}
    \begin{center}
        \includegraphics[width=0.95\textwidth, height=0.65\textheight, keepaspectratio]{lag_operator.pdf}
    \end{center}
    \vspace{-0.2cm}
    \begin{exampleblock}{Key Insight}
        The lag operator $L$ shifts observations back in time: $LX_t = X_{t-1}$. This notation simplifies ARMA model expressions and enables algebraic manipulation of time series equations.
    \end{exampleblock}
\end{frame}

\begin{frame}{White Noise Process}
    \begin{defn}[White Noise]
        A process $\{\varepsilon_t\}$ is \textbf{white noise}, denoted $\varepsilon_t \sim WN(0, \sigma^2)$, if:
        \begin{enumerate}
            \item $\E[\varepsilon_t] = 0$ for all $t$
            \item $\Var(\varepsilon_t) = \sigma^2$ for all $t$
            \item $\Cov(\varepsilon_t, \varepsilon_s) = 0$ for all $t \neq s$
        \end{enumerate}
    \end{defn}

    \vspace{0.15cm}
    \textbf{Properties:}
    \begin{itemize}
        \item White noise is the ``building block'' of ARMA models
        \item ACF: $\rho(0) = 1$, $\rho(h) = 0$ for $h \neq 0$
        \item PACF: same pattern
        \item \textbf{Gaussian white noise:} additionally $\varepsilon_t \sim N(0, \sigma^2)$
    \end{itemize}

    \vspace{0.15cm}
    \textbf{Note:} White noise is \textit{not} predictable --- it's pure randomness.
\end{frame}

\begin{frame}{White Noise: Visual Illustration}
    \begin{center}
        \includegraphics[width=0.95\textwidth, height=0.58\textheight, keepaspectratio]{ch2_white_noise.pdf}
    \end{center}
    \vspace{-0.2cm}
    \begin{block}{Key Characteristics}
        \textbf{Left}: Series fluctuates randomly around mean zero with no patterns. \textbf{Right}: ACF shows only a spike at lag 0; all other autocorrelations fall within confidence bounds --- no structure to predict.
    \end{block}
\end{frame}

%=============================================================================
% SECTION 2: AR MODELS
%=============================================================================
\section{Autoregressive (AR) Models}

\begin{frame}{AR(1) Model: Definition}
    \begin{defn}[AR(1) Process]
        An \textbf{autoregressive process of order 1} is:
        $$X_t = c + \phi X_{t-1} + \varepsilon_t$$
        where $\varepsilon_t \sim WN(0, \sigma^2)$ and $|\phi| < 1$ for stationarity.
    \end{defn}

    \vspace{0.15cm}
    \textbf{Interpretation:}
    \begin{itemize}
        \item $c$: constant (intercept)
        \item $\phi$: autoregressive coefficient --- measures persistence
        \item $\varepsilon_t$: innovation (unpredictable shock)
    \end{itemize}

    \vspace{0.15cm}
    \textbf{Using lag operator:}
    $$(1 - \phi L)X_t = c + \varepsilon_t$$
    $$\phi(L) X_t = c + \varepsilon_t \quad \text{where } \phi(L) = 1 - \phi L$$
\end{frame}

\begin{frame}{AR(1): Visual Illustration}
    \begin{center}
        \includegraphics[width=0.95\textwidth, height=0.58\textheight, keepaspectratio]{ch2_def_ar1.pdf}
    \end{center}
    \vspace{-0.2cm}
    \begin{block}{Behavior Patterns}
        \textbf{Positive $\phi$}: Persistent, smooth fluctuations --- values tend to stay on same side of mean. \textbf{Negative $\phi$}: Oscillating behavior --- values alternate around the mean.
    \end{block}
\end{frame}

\begin{frame}{AR(1) Stationarity Condition}
    \textbf{For AR(1) to be stationary:} $|\phi| < 1$

    \vspace{0.15cm}
    \textbf{Intuition:}
    \begin{itemize}
        \item If $|\phi| < 1$: shocks decay over time $\rightarrow$ stationary
        \item If $|\phi| = 1$: random walk $\rightarrow$ non-stationary (unit root)
        \item If $|\phi| > 1$: explosive process $\rightarrow$ non-stationary
    \end{itemize}

    \vspace{0.15cm}
    \textbf{Characteristic equation:}
    $$\phi(z) = 1 - \phi z = 0 \implies z = \frac{1}{\phi}$$

    Stationarity requires the root $z = 1/\phi$ to lie \textbf{outside the unit circle}, i.e., $|z| > 1$, which means $|\phi| < 1$.
\end{frame}

\begin{frame}{AR(1) Properties}
    For a stationary AR(1) with $|\phi| < 1$:

    \vspace{0.15cm}
    \textbf{Mean:}
    $$\mu = \E[X_t] = \frac{c}{1-\phi}$$

    \textbf{Variance:}
    $$\gamma(0) = \Var(X_t) = \frac{\sigma^2}{1-\phi^2}$$

    \textbf{Autocovariance:}
    $$\gamma(h) = \phi^h \gamma(0) = \frac{\phi^h \sigma^2}{1-\phi^2}$$

    \textbf{Autocorrelation (ACF):}
    $$\rho(h) = \phi^h$$

    \textbf{Key insight:} ACF decays exponentially at rate $\phi$
\end{frame}

\begin{frame}{Proof: AR(1) Mean}
    \textbf{Claim:} For AR(1): $X_t = c + \phi X_{t-1} + \varepsilon_t$, the mean is $\mu = \frac{c}{1-\phi}$

    \vspace{0.2cm}
    \textbf{Proof:} Take expectations of both sides:
    $$\E[X_t] = \E[c + \phi X_{t-1} + \varepsilon_t] = c + \phi \E[X_{t-1}] + \E[\varepsilon_t]$$

    By stationarity, $\E[X_t] = \E[X_{t-1}] = \mu$, and $\E[\varepsilon_t] = 0$:
    $$\mu = c + \phi \mu$$

    Solving for $\mu$:
    $$\mu - \phi\mu = c \implies \mu(1-\phi) = c \implies \boxed{\mu = \frac{c}{1-\phi}}$$

    \vspace{0.1cm}
    \begin{alertblock}{Requirement}
        This requires $\phi \neq 1$. If $\phi = 1$ (unit root), the mean is undefined.
    \end{alertblock}
\end{frame}

\begin{frame}{Proof: AR(1) Variance}
    \textbf{Claim:} $\Var(X_t) = \frac{\sigma^2}{1-\phi^2}$

    \vspace{0.15cm}
    \textbf{Proof:} WLOG assume $c=0$ (centered process). Take variance of $X_t = \phi X_{t-1} + \varepsilon_t$:
    $$\Var(X_t) = \Var(\phi X_{t-1} + \varepsilon_t) = \phi^2 \Var(X_{t-1}) + \Var(\varepsilon_t) + 2\phi\Cov(X_{t-1}, \varepsilon_t)$$

    Since $\varepsilon_t$ is independent of $X_{t-1}$, $\Cov(X_{t-1}, \varepsilon_t) = 0$:
    $$\gamma(0) = \phi^2 \gamma(0) + \sigma^2$$

    By stationarity, $\Var(X_t) = \Var(X_{t-1}) = \gamma(0)$:
    $$\gamma(0) - \phi^2\gamma(0) = \sigma^2 \implies \gamma(0)(1-\phi^2) = \sigma^2 \implies \boxed{\gamma(0) = \frac{\sigma^2}{1-\phi^2}}$$

    \begin{exampleblock}{Note}
        Requires $|\phi| < 1$ for positive variance. As $|\phi| \to 1$, variance $\to \infty$.
    \end{exampleblock}
\end{frame}

\begin{frame}{Proof: AR(1) Autocorrelation Function}
    \textbf{Claim:} $\rho(h) = \phi^h$ for $h \geq 0$

    \vspace{0.15cm}
    \textbf{Proof:} First, find autocovariance $\gamma(h) = \Cov(X_t, X_{t-h})$.

    Multiply $X_t = \phi X_{t-1} + \varepsilon_t$ by $X_{t-h}$ and take expectations:
    $$\E[X_t X_{t-h}] = \phi \E[X_{t-1} X_{t-h}] + \E[\varepsilon_t X_{t-h}]$$

    For $h \geq 1$: $\E[\varepsilon_t X_{t-h}] = 0$ (future shock uncorrelated with past values)
    $$\gamma(h) = \phi \gamma(h-1)$$

    This is a recursive relation! Starting from $\gamma(0)$:
    $$\gamma(1) = \phi\gamma(0), \quad \gamma(2) = \phi\gamma(1) = \phi^2\gamma(0), \quad \ldots \quad \boxed{\gamma(h) = \phi^h\gamma(0)}$$

    The ACF is:
    $$\rho(h) = \frac{\gamma(h)}{\gamma(0)} = \frac{\phi^h\gamma(0)}{\gamma(0)} = \boxed{\phi^h}$$
\end{frame}

\begin{frame}{AR(1) Variance as Function of $\phi$}
    \begin{center}
        \includegraphics[width=0.95\textwidth, height=0.62\textheight, keepaspectratio]{ar1_variance.pdf}
    \end{center}
    \vspace{-0.2cm}
    \begin{alertblock}{Critical Insight}
        As $|\phi| \to 1$, variance $\sigma^2/(1-\phi^2) \to \infty$. This explains why unit root processes ($\phi = 1$) are non-stationary: their variance is unbounded.
    \end{alertblock}
\end{frame}

\begin{frame}{AR(1) Simulations: Effect of $\phi$}
    \vspace{-0.3cm}
    \begin{center}
        \includegraphics[width=0.82\textwidth, height=0.58\textheight, keepaspectratio]{ch2_ar1_simulations.pdf}
    \end{center}
    \vspace{-0.2cm}
    {\footnotesize
    \begin{itemize}
        \item Different $\phi$ values produce distinct behavior: higher $|\phi|$ means more persistence
        \item Positive $\phi$ creates smooth, trending patterns; negative $\phi$ creates oscillations
        \item As $|\phi| \to 1$, the process becomes more persistent and approaches non-stationarity
    \end{itemize}
    }
\end{frame}

\begin{frame}{AR(1) Theoretical ACF}
    \vspace{-0.2cm}
    \begin{center}
        \includegraphics[width=0.85\textwidth, height=0.55\textheight, keepaspectratio]{ar1_theoretical_acf.pdf}
    \end{center}
    \vspace{-0.2cm}
    \begin{exampleblock}{ACF Pattern}
        For AR(1): $\rho(h) = \phi^h$. Positive $\phi$ gives smooth exponential decay; negative $\phi$ gives alternating decay. The rate of decay reveals the persistence of the process.
    \end{exampleblock}
\end{frame}

\begin{frame}{AR(1) ACF and PACF: Theory vs Sample}
    \vspace{-0.3cm}
    \begin{center}
        \includegraphics[width=0.82\textwidth, height=0.58\textheight, keepaspectratio]{ch2_ar1_acf_pacf.pdf}
    \end{center}
    \vspace{-0.2cm}
    {\footnotesize
    \begin{itemize}
        \item \textbf{ACF}: Exponential decay at rate $\phi$ -- theoretical formula: $\rho(h) = \phi^h$
        \item \textbf{PACF}: Single spike at lag 1, then cuts off -- this identifies AR(1)
        \item Sample estimates (bars) fluctuate around theoretical values; use confidence bands
    \end{itemize}
    }
\end{frame}

\begin{frame}{AR(1) ACF and PACF Patterns}
    \textbf{ACF of AR(1):}
    \begin{itemize}
        \item Decays exponentially: $\rho(h) = \phi^h$
        \item If $\phi > 0$: all positive, gradual decay
        \item If $\phi < 0$: alternating signs, decay in magnitude
    \end{itemize}

    \vspace{0.15cm}
    \textbf{PACF of AR(1):}
    \begin{itemize}
        \item \textbf{Cuts off after lag 1}
        \item $\pi_1 = \phi$, $\pi_k = 0$ for $k > 1$
    \end{itemize}

    \vspace{0.15cm}
    \begin{center}
    \begin{tabular}{lcc}
        \toprule
        & \textbf{ACF} & \textbf{PACF} \\
        \midrule
        AR(1) & Exponential decay & Cuts off at lag 1 \\
        \bottomrule
    \end{tabular}
    \end{center}

    \vspace{0.15cm}
    \textbf{This is the key identification pattern for AR(1)!}
\end{frame}

\begin{frame}{AR(p) Model: General Form}
    \begin{defn}[AR(p) Process]
        An \textbf{autoregressive process of order p} is:
        $$X_t = c + \phi_1 X_{t-1} + \phi_2 X_{t-2} + \cdots + \phi_p X_{t-p} + \varepsilon_t$$
    \end{defn}

    \textbf{Using lag operator:}
    $$\phi(L) X_t = c + \varepsilon_t$$
    where $\phi(L) = 1 - \phi_1 L - \phi_2 L^2 - \cdots - \phi_p L^p$

    \vspace{0.15cm}
    \textbf{Stationarity condition:}
    \begin{itemize}
        \item All roots of $\phi(z) = 0$ must lie \textbf{outside} the unit circle
        \item Equivalently: all roots have modulus $> 1$
    \end{itemize}

    \vspace{0.15cm}
    \textbf{PACF pattern:}
    \begin{itemize}
        \item PACF cuts off after lag $p$
        \item ACF decays (exponentially or with damped oscillations)
    \end{itemize}
\end{frame}

\begin{frame}{AR(p): Visual Illustration}
    \begin{center}
        \includegraphics[width=0.95\textwidth, height=0.58\textheight, keepaspectratio]{ch2_def_arp.pdf}
    \end{center}
    \vspace{-0.2cm}
    \begin{exampleblock}{AR(2) Characteristics}
        AR(2) can exhibit pseudo-cyclic behavior when roots are complex conjugates. ACF shows damped sinusoidal decay; PACF cuts off after lag 2 --- the key identification pattern.
    \end{exampleblock}
\end{frame}

\begin{frame}{AR(2) Stationarity: Unit Circle Visualization}
    \begin{center}
        \includegraphics[width=0.95\textwidth, height=0.62\textheight, keepaspectratio]{unit_circle_stationarity.pdf}
    \end{center}
    \vspace{-0.2cm}
    \begin{block}{Stationarity Condition}
        All roots of the characteristic polynomial $\phi(z) = 1 - \phi_1 z - \phi_2 z^2 = 0$ must lie \textbf{outside} the unit circle. Equivalently, all roots of $1 - \phi_1 L - \phi_2 L^2 = 0$ must have modulus $> 1$.
    \end{block}
\end{frame}

\begin{frame}{AR(2) Stationarity Triangle}
    \vspace{-0.3cm}
    \begin{center}
        \includegraphics[width=0.82\textwidth, height=0.58\textheight, keepaspectratio]{ch2_ar2_stationarity.pdf}
    \end{center}
    \vspace{-0.2cm}
    {\footnotesize
    \begin{itemize}
        \item The triangular region defines all stationary AR(2) parameter combinations
        \item Boundaries: $\phi_1 + \phi_2 < 1$, $\phi_2 - \phi_1 < 1$, and $|\phi_2| < 1$
        \item Points outside this region lead to non-stationary or explosive processes
    \end{itemize}
    }
\end{frame}

\begin{frame}{Characteristic Polynomial Roots}
    \begin{center}
        \includegraphics[width=0.95\textwidth, height=0.62\textheight, keepaspectratio]{characteristic_roots.pdf}
    \end{center}
    \vspace{-0.2cm}
    \begin{exampleblock}{Interpretation}
        Complex conjugate roots produce oscillatory behavior in the ACF. The closer roots are to the unit circle, the more persistent the oscillations. Real roots produce monotonic decay.
    \end{exampleblock}
\end{frame}

\begin{frame}{AR(2) Model}
    \begin{defn}[AR(2) Process]
        $$X_t = c + \phi_1 X_{t-1} + \phi_2 X_{t-2} + \varepsilon_t$$
    \end{defn}

    \textbf{Stationarity conditions for AR(2):}
    \begin{enumerate}
        \item $\phi_1 + \phi_2 < 1$
        \item $\phi_2 - \phi_1 < 1$
        \item $|\phi_2| < 1$
    \end{enumerate}

    \vspace{0.15cm}
    \textbf{ACF behavior depends on roots:}
    \begin{itemize}
        \item \textbf{Real roots:} mixture of two exponential decays
        \item \textbf{Complex roots:} damped sinusoidal pattern (pseudo-cycles)
    \end{itemize}

    \vspace{0.15cm}
    \textbf{PACF:} Cuts off after lag 2 ($\pi_k = 0$ for $k > 2$)
\end{frame}

\begin{frame}{Quiz: AR Stationarity}
    \begin{alertblock}{Question}
        For which value of $\phi$ is the AR(1) process $X_t = c + \phi X_{t-1} + \varepsilon_t$ stationary?
    \end{alertblock}

    \vspace{0.3cm}

    \begin{enumerate}[(A)]
        \item $\phi = 1.2$
        \item $\phi = 1.0$
        \item $\phi = -0.8$
        \item $\phi = -1.5$
    \end{enumerate}
\end{frame}

\begin{frame}{Quiz: AR Stationarity -- Answer}
    \begin{exampleblock}{Correct Answer: (C) $\phi = -0.8$}
        AR(1) is stationary if and only if $|\phi| < 1$. Only $|-0.8| = 0.8 < 1$.
    \end{exampleblock}
    \vspace{0.2cm}
    \begin{center}
        \includegraphics[width=0.95\textwidth, height=0.55\textheight, keepaspectratio]{ch2_quiz_ar_stationarity.pdf}
    \end{center}
\end{frame}

%=============================================================================
% SECTION 3: MA MODELS
%=============================================================================
\section{Moving Average (MA) Models}

\begin{frame}{MA(1) Model: Definition}
    \begin{defn}[MA(1) Process]
        A \textbf{moving average process of order 1} is:
        $$X_t = \mu + \varepsilon_t + \theta \varepsilon_{t-1}$$
        where $\varepsilon_t \sim WN(0, \sigma^2)$.
    \end{defn}

    \vspace{0.15cm}
    \textbf{Interpretation:}
    \begin{itemize}
        \item $\mu$: mean of the process
        \item $\theta$: MA coefficient --- measures impact of past shock
        \item Current value depends on current and one past shock
    \end{itemize}

    \vspace{0.15cm}
    \textbf{Using lag operator:}
    $$X_t = \mu + \theta(L)\varepsilon_t$$
    where $\theta(L) = 1 + \theta L$

    \vspace{0.15cm}
    \textbf{Key property:} MA processes are \textbf{always stationary} for any finite $\theta$
\end{frame}

\begin{frame}{MA(1): Visual Illustration}
    \begin{center}
        \includegraphics[width=0.95\textwidth]{ch2_def_ma1.pdf}
    \end{center}
    \vspace{-0.2cm}
    \small MA(1) process on the left. ACF on the right shows characteristic cutoff after lag 1.
\end{frame}

\begin{frame}{MA(1) Properties}
    For MA(1): $X_t = \mu + \varepsilon_t + \theta \varepsilon_{t-1}$

    \vspace{0.15cm}
    \textbf{Mean:}
    $$\E[X_t] = \mu$$

    \textbf{Variance:}
    $$\gamma(0) = \Var(X_t) = \sigma^2(1 + \theta^2)$$

    \textbf{Autocovariance:}
    $$\gamma(1) = \theta\sigma^2, \quad \gamma(h) = 0 \text{ for } h > 1$$

    \textbf{Autocorrelation (ACF):}
    $$\rho(1) = \frac{\theta}{1+\theta^2}, \quad \rho(h) = 0 \text{ for } h > 1$$

    \vspace{0.15cm}
    \textbf{Key insight:} ACF \textbf{cuts off} after lag 1
\end{frame}

\begin{frame}{Proof: MA(1) Variance and Autocovariance}
    \textbf{Setup:} $X_t = \varepsilon_t + \theta\varepsilon_{t-1}$ (assuming $\mu = 0$)

    \vspace{0.15cm}
    \textbf{Variance:}
    \begin{align*}
    \gamma(0) &= \Var(X_t) = \Var(\varepsilon_t + \theta\varepsilon_{t-1}) \\
    &= \Var(\varepsilon_t) + \theta^2\Var(\varepsilon_{t-1}) + 2\theta\Cov(\varepsilon_t, \varepsilon_{t-1}) \\
    &= \sigma^2 + \theta^2\sigma^2 + 0 = \boxed{\sigma^2(1+\theta^2)}
    \end{align*}

    \textbf{Autocovariance at lag 1:}
    \begin{align*}
    \gamma(1) &= \Cov(X_t, X_{t-1}) = \Cov(\varepsilon_t + \theta\varepsilon_{t-1}, \varepsilon_{t-1} + \theta\varepsilon_{t-2}) \\
    &= \Cov(\varepsilon_t, \varepsilon_{t-1}) + \theta\Cov(\varepsilon_t, \varepsilon_{t-2}) + \theta\Cov(\varepsilon_{t-1}, \varepsilon_{t-1}) + \theta^2\Cov(\varepsilon_{t-1}, \varepsilon_{t-2}) \\
    &= 0 + 0 + \theta\sigma^2 + 0 = \boxed{\theta\sigma^2}
    \end{align*}

    \textbf{Autocovariance at lag $h \geq 2$:} No overlapping $\varepsilon$ terms $\Rightarrow \gamma(h) = 0$
\end{frame}

\begin{frame}{Proof: MA(1) ACF Maximum}
    \textbf{Claim:} $|\rho(1)| \leq 0.5$ for any value of $\theta$

    \vspace{0.2cm}
    \textbf{Proof:} The ACF at lag 1 is:
    $$\rho(1) = \frac{\gamma(1)}{\gamma(0)} = \frac{\theta\sigma^2}{\sigma^2(1+\theta^2)} = \frac{\theta}{1+\theta^2}$$

    To find the maximum, take derivative w.r.t. $\theta$ and set to zero:
    $$\frac{d\rho(1)}{d\theta} = \frac{(1+\theta^2) - \theta(2\theta)}{(1+\theta^2)^2} = \frac{1-\theta^2}{(1+\theta^2)^2} = 0$$

    Solution: $\theta = \pm 1$. At these values:
    $$\rho(1)\big|_{\theta=1} = \frac{1}{1+1} = \frac{1}{2}, \quad \rho(1)\big|_{\theta=-1} = \frac{-1}{1+1} = -\frac{1}{2}$$

    \begin{exampleblock}{Implication}
        If you estimate $|\hat{\rho}(1)| > 0.5$ from data, the process is \textbf{not} MA(1).
    \end{exampleblock}
\end{frame}

\begin{frame}{MA(1) Simulations: Effect of $\theta$}
    \vspace{-0.3cm}
    \begin{center}
        \includegraphics[width=0.82\textwidth, height=0.58\textheight, keepaspectratio]{ch2_ma1_simulations.pdf}
    \end{center}
    \vspace{-0.2cm}
    {\footnotesize
    \begin{itemize}
        \item MA(1) is always stationary regardless of $\theta$ -- finite memory of only one lag
        \item Positive $\theta$ smooths the series; negative $\theta$ creates more rapid fluctuations
        \item Unlike AR(1), MA(1) shocks only affect the process for one period
    \end{itemize}
    }
\end{frame}

\begin{frame}{MA(1) ACF and PACF Patterns}
    \textbf{ACF of MA(1):}
    \begin{itemize}
        \item \textbf{Cuts off after lag 1}
        \item $\rho(1) = \frac{\theta}{1+\theta^2}$, $\rho(h) = 0$ for $h > 1$
        \item Note: $|\rho(1)| \leq 0.5$ always (maximum at $\theta = \pm 1$)
    \end{itemize}

    \vspace{0.15cm}
    \textbf{PACF of MA(1):}
    \begin{itemize}
        \item Decays exponentially (or with alternating signs)
        \item Does \textit{not} cut off
    \end{itemize}

    \vspace{0.15cm}
    \begin{center}
    \begin{tabular}{lcc}
        \toprule
        & \textbf{ACF} & \textbf{PACF} \\
        \midrule
        MA(1) & Cuts off at lag 1 & Exponential decay \\
        \bottomrule
    \end{tabular}
    \end{center}

    \vspace{0.15cm}
    \textbf{This is the opposite pattern from AR(1)!}
\end{frame}

\begin{frame}{MA(1) ACF and PACF: Visual Comparison}
    \vspace{-0.3cm}
    \begin{center}
        \includegraphics[width=0.82\textwidth, height=0.52\textheight, keepaspectratio]{ch2_ma1_acf_pacf.pdf}
    \end{center}
    \vspace{-0.2cm}
    \begin{block}{Key Identification Pattern}
        \textbf{ACF}: Single spike at lag 1, then cuts off --- the MA(1) signature. \textbf{PACF}: Exponential decay --- opposite of AR(1). This ACF/PACF reversal distinguishes MA from AR.
    \end{block}
\end{frame}

\begin{frame}{Invertibility of MA Models}
    \begin{defn}[Invertibility]
        An MA process is \textbf{invertible} if it can be written as an infinite AR process:
        $$X_t = \mu + \sum_{j=1}^{\infty} \pi_j (X_{t-j} - \mu) + \varepsilon_t$$
    \end{defn}

    \vspace{0.15cm}
    \textbf{For MA(1):} Invertible if $|\theta| < 1$

    \textbf{For MA(q):} All roots of $\theta(z) = 0$ must lie outside the unit circle

    \vspace{0.15cm}
    \textbf{Why invertibility matters:}
    \begin{itemize}
        \item Ensures unique representation
        \item Required for forecasting and estimation
        \item Creates correspondence: AR($\infty$) $\leftrightarrow$ MA(q)
    \end{itemize}

    \vspace{0.15cm}
    \textbf{Note:} Stationarity is for AR, Invertibility is for MA
\end{frame}

\begin{frame}{Invertibility: Visual Illustration}
    \begin{center}
        \includegraphics[width=0.95\textwidth]{ch2_def_invertibility.pdf}
    \end{center}
    \vspace{-0.2cm}
    \small Left: invertibility requires roots outside unit circle. Right: AR($\infty$) weights decay only when $|\theta| < 1$.
\end{frame}

\begin{frame}{MA(q) Model: General Form}
    \begin{defn}[MA(q) Process]
        A \textbf{moving average process of order q} is:
        $$X_t = \mu + \varepsilon_t + \theta_1\varepsilon_{t-1} + \theta_2\varepsilon_{t-2} + \cdots + \theta_q\varepsilon_{t-q}$$
    \end{defn}

    \textbf{Using lag operator:}
    $$X_t = \mu + \theta(L)\varepsilon_t$$
    where $\theta(L) = 1 + \theta_1 L + \theta_2 L^2 + \cdots + \theta_q L^q$

    \vspace{0.15cm}
    \textbf{Properties:}
    \begin{itemize}
        \item Always stationary (finite variance)
        \item ACF cuts off after lag $q$: $\rho(h) = 0$ for $h > q$
        \item PACF decays gradually
        \item Invertible if all roots of $\theta(z) = 0$ lie outside unit circle
    \end{itemize}
\end{frame}

\begin{frame}{MA(q): Visual Illustration}
    \begin{center}
        \includegraphics[width=0.95\textwidth]{ch2_def_maq.pdf}
    \end{center}
    \vspace{-0.2cm}
    \small MA(3) process. Key signature: ACF cuts off after lag $q$ (here, lag 3).
\end{frame}

\begin{frame}{Quiz: ACF/PACF Pattern Recognition}
    \begin{alertblock}{Question}
        You observe: ACF has spike at lag 1, then cuts off. PACF decays gradually. What model?
    \end{alertblock}

    \vspace{0.3cm}

    \begin{enumerate}[(A)]
        \item AR(1)
        \item MA(1)
        \item ARMA(1,1)
        \item White noise
    \end{enumerate}
\end{frame}

\begin{frame}{Quiz: ACF/PACF Pattern Recognition -- Answer}
    \begin{exampleblock}{Correct Answer: (B) MA(1)}
        ACF cuts off $\rightarrow$ MA process; PACF decays $\rightarrow$ confirms MA(1)
    \end{exampleblock}
    \vspace{0.2cm}
    \begin{center}
        \includegraphics[width=0.95\textwidth, height=0.55\textheight, keepaspectratio]{ch2_quiz_acf_pacf_patterns.pdf}
    \end{center}
\end{frame}

\begin{frame}{Quiz: MA Invertibility}
    \begin{alertblock}{Question}
        Is MA(1) $X_t = \varepsilon_t + 1.5\varepsilon_{t-1}$ invertible?
    \end{alertblock}

    \vspace{0.3cm}

    \begin{enumerate}[(A)]
        \item Yes, MA processes are always invertible
        \item Yes, because $1.5 > 0$
        \item No, because $|\theta| = 1.5 > 1$
        \item No, MA processes are never invertible
    \end{enumerate}

    \vspace{0.3cm}
    \pause
    \begin{exampleblock}{Answer: (C)}
        Invertibility requires $|\theta| < 1$. Here $|\theta| = 1.5 > 1$, so not invertible.
    \end{exampleblock}
\end{frame}

%=============================================================================
% SECTION 4: ARMA MODELS
%=============================================================================
\section{ARMA Models}

\begin{frame}{ARMA(p,q) Model: Definition}
    \begin{defn}[ARMA(p,q) Process]
        An \textbf{autoregressive moving average process} of order (p,q) is:
        $$X_t = c + \phi_1 X_{t-1} + \cdots + \phi_p X_{t-p} + \varepsilon_t + \theta_1\varepsilon_{t-1} + \cdots + \theta_q\varepsilon_{t-q}$$
    \end{defn}

    \textbf{Compact form using lag operators:}
    $$\phi(L)(X_t - \mu) = \theta(L)\varepsilon_t$$

    or equivalently:
    $$\phi(L)X_t = c + \theta(L)\varepsilon_t$$

    where $\mu = \frac{c}{1-\phi_1-\cdots-\phi_p}$

    \vspace{0.15cm}
    \textbf{Key idea:} Combines AR and MA components for more flexible modeling
\end{frame}

\begin{frame}{ARMA: Visual Illustration}
    \begin{center}
        \includegraphics[width=0.95\textwidth]{ch2_def_arma.pdf}
    \end{center}
    \vspace{-0.2cm}
    \small ARMA(1,1) process. ACF shows decay after initial lag, combining AR and MA characteristics.
\end{frame}

\begin{frame}{ARMA Model Structure}
    \begin{center}
        \includegraphics[width=0.95\textwidth, height=0.62\textheight, keepaspectratio]{arma_structure.pdf}
    \end{center}
    \vspace{-0.2cm}
    \begin{block}{Model Components}
        ARMA combines autoregressive (past values) and moving average (past shocks) components. The AR part captures persistence; the MA part captures short-term shock effects.
    \end{block}
\end{frame}

\begin{frame}{How ARMA Simulation Works}
    \begin{center}
        \includegraphics[width=0.95\textwidth, height=0.62\textheight, keepaspectratio]{arma_simulation_steps.pdf}
    \end{center}
    \vspace{-0.2cm}
    \begin{exampleblock}{Simulation Algorithm}
        To simulate ARMA: (1) Generate white noise $\varepsilon_t$, (2) Apply MA filter to get intermediate series, (3) Apply AR recursion to get final output.
    \end{exampleblock}
\end{frame}

\begin{frame}{ARMA Examples}
    \begin{center}
        \includegraphics[width=0.95\textwidth, height=0.62\textheight, keepaspectratio]{arma_examples.pdf}
    \end{center}
    \vspace{-0.2cm}
    \begin{alertblock}{Key Observation}
        Different ARMA specifications produce visually similar series but have distinct ACF/PACF patterns. Model identification requires examining autocorrelation structure.
    \end{alertblock}
\end{frame}

\begin{frame}{ARMA(1,1) Model}
    \begin{defn}[ARMA(1,1) Process]
        $$X_t = c + \phi X_{t-1} + \varepsilon_t + \theta\varepsilon_{t-1}$$
    \end{defn}

    \textbf{Properties (assuming stationarity and invertibility):}
    \begin{itemize}
        \item Mean: $\mu = \frac{c}{1-\phi}$
        \item Variance: $\gamma(0) = \frac{(1+2\phi\theta+\theta^2)\sigma^2}{1-\phi^2}$
    \end{itemize}

    \vspace{0.15cm}
    \textbf{ACF:}
    $$\rho(1) = \frac{(1+\phi\theta)(\phi+\theta)}{1+2\phi\theta+\theta^2}$$
    $$\rho(h) = \phi \cdot \rho(h-1) \quad \text{for } h \geq 2$$

    \vspace{0.15cm}
    \textbf{Pattern:} ACF decays exponentially after lag 1 (like AR), but starting point depends on both $\phi$ and $\theta$
\end{frame}

\begin{frame}{ACF/PACF Patterns: AR vs MA vs ARMA}
    \vspace{-0.2cm}
    \begin{center}
        \includegraphics[width=0.55\textwidth, height=0.58\textheight, keepaspectratio]{acf_pacf_patterns.pdf}
    \end{center}
    \vspace{-0.2cm}
    \begin{alertblock}{Model Identification Rule}
        \textbf{PACF cuts off} $\Rightarrow$ AR (order = cutoff lag). \textbf{ACF cuts off} $\Rightarrow$ MA (order = cutoff lag). \textbf{Both decay} $\Rightarrow$ ARMA.
    \end{alertblock}
\end{frame}

\begin{frame}{ARMA ACF and PACF Patterns}
    \begin{center}
    \begin{tabular}{lcc}
        \toprule
        \textbf{Model} & \textbf{ACF} & \textbf{PACF} \\
        \midrule
        AR(p) & Decays (exp./damped) & Cuts off at lag $p$ \\
        MA(q) & Cuts off at lag $q$ & Decays (exp./damped) \\
        ARMA(p,q) & Decays after lag $q-p$ & Decays after lag $p-q$ \\
        \bottomrule
    \end{tabular}
    \end{center}

    \vspace{0.15cm}
    \textbf{Key identification rule:}
    \begin{itemize}
        \item \textbf{PACF cuts off} $\rightarrow$ AR process (order = cutoff lag)
        \item \textbf{ACF cuts off} $\rightarrow$ MA process (order = cutoff lag)
        \item \textbf{Both decay} $\rightarrow$ ARMA process
    \end{itemize}

    \vspace{0.15cm}
    \textbf{Caution:} In practice, sample ACF/PACF are noisy; use confidence bands
\end{frame}

\begin{frame}{Impulse Response Functions}
    \begin{center}
        \includegraphics[width=0.95\textwidth, height=0.62\textheight, keepaspectratio]{impulse_response.pdf}
    \end{center}
    \vspace{-0.2cm}
    \begin{exampleblock}{Interpretation}
        The impulse response function (IRF) shows how a unit shock $\varepsilon_t = 1$ propagates through the system over time. For stationary processes, the IRF decays to zero as $h \to \infty$.
    \end{exampleblock}
\end{frame}

\begin{frame}{Stationarity and Invertibility Summary}
    \textbf{For ARMA(p,q) to be well-behaved:}

    \vspace{0.15cm}
    \begin{tabular}{ll}
        \toprule
        \textbf{Condition} & \textbf{Requirement} \\
        \midrule
        Stationarity & Roots of $\phi(z) = 0$ outside unit circle \\
        Invertibility & Roots of $\theta(z) = 0$ outside unit circle \\
        \bottomrule
    \end{tabular}

    \vspace{0.15cm}
    \textbf{Implications:}
    \begin{itemize}
        \item \textbf{Stationarity:} Can write as MA($\infty$): $X_t = \mu + \sum_{j=0}^{\infty} \psi_j \varepsilon_{t-j}$
        \item \textbf{Invertibility:} Can write as AR($\infty$): $X_t = \mu + \sum_{j=1}^{\infty} \pi_j (X_{t-j}-\mu) + \varepsilon_t$
    \end{itemize}

    \vspace{0.15cm}
    \textbf{Causal representation:} $X_t$ depends only on \textit{past} shocks (not future)
\end{frame}

\begin{frame}{Wold's Decomposition Theorem}
    \begin{center}
        \includegraphics[width=0.95\textwidth, height=0.58\textheight, keepaspectratio]{wold_representation.pdf}
    \end{center}
    \vspace{-0.2cm}
    \begin{block}{Fundamental Result}
        Any stationary process can be written as MA($\infty$): $X_t = \mu + \sum_{j=0}^{\infty} \psi_j \varepsilon_{t-j}$ where $\psi_0 = 1$ and $\sum \psi_j^2 < \infty$. ARMA models are parsimonious approximations to this infinite representation.
    \end{block}
\end{frame}

\begin{frame}{Quiz: ARMA Representation}
    \begin{alertblock}{Question}
        The compact form $\phi(L)X_t = \theta(L)\varepsilon_t$ represents which model?
    \end{alertblock}

    \vspace{0.3cm}

    \begin{enumerate}[(A)]
        \item Pure AR model
        \item Pure MA model
        \item ARMA model
        \item None of the above
    \end{enumerate}

    \vspace{0.3cm}
    \pause
    \begin{exampleblock}{Answer: (C) ARMA model}
        $\phi(L)$ is the AR polynomial, $\theta(L)$ is the MA polynomial $\rightarrow$ ARMA(p,q)
    \end{exampleblock}
\end{frame}

\begin{frame}{Quiz: Lag Operator}
    \begin{alertblock}{Question}
        What is $(1-L)^2 X_t$?
    \end{alertblock}

    \vspace{0.3cm}

    \begin{enumerate}[(A)]
        \item $X_t - X_{t-1}$
        \item $X_t - 2X_{t-1} + X_{t-2}$
        \item $X_t + X_{t-1} + X_{t-2}$
        \item $X_t - X_{t-2}$
    \end{enumerate}

    \vspace{0.3cm}
    \pause
    \begin{exampleblock}{Answer: (B)}
        $(1-L)^2 = 1 - 2L + L^2$, so $(1-L)^2 X_t = X_t - 2X_{t-1} + X_{t-2}$
    \end{exampleblock}
\end{frame}

%=============================================================================
% SECTION 5: MODEL IDENTIFICATION
%=============================================================================
\section{Model Identification}

\begin{frame}{The Box-Jenkins Methodology}
    \begin{center}
        \includegraphics[width=0.95\textwidth, height=0.58\textheight, keepaspectratio]{box_jenkins_flowchart.pdf}
    \end{center}
    \vspace{-0.2cm}
    \begin{block}{Three-Stage Process}
        \textbf{1. Identification}: Use ACF/PACF to determine model orders $(p, q)$. \textbf{2. Estimation}: Fit parameters via MLE or least squares. \textbf{3. Diagnostic checking}: Verify residuals are white noise.
    \end{block}
\end{frame}

\begin{frame}{Model Identification Summary Table}
    \vspace{-0.2cm}
    \begin{center}
        \includegraphics[width=0.85\textwidth, height=0.65\textheight, keepaspectratio]{model_identification_table.pdf}
    \end{center}
    \vspace{-0.2cm}
    {\small \textbf{Practical tip:} Start simple (low $p$, $q$), increase if diagnostics fail}
\end{frame}

\begin{frame}{ACF/PACF Identification Rules}
    \textbf{Theoretical patterns for stationary processes:}

    \vspace{0.15cm}
    \begin{center}
    \begin{tabular}{lll}
        \toprule
        \textbf{Model} & \textbf{ACF Pattern} & \textbf{PACF Pattern} \\
        \midrule
        AR(1) & Exponential decay & Spike at lag 1, then 0 \\
        AR(2) & Damped exponential/sine & Spikes at lags 1-2, then 0 \\
        AR(p) & Decays gradually & Cuts off after lag $p$ \\
        \midrule
        MA(1) & Spike at lag 1, then 0 & Exponential decay \\
        MA(2) & Spikes at lags 1-2, then 0 & Damped exponential/sine \\
        MA(q) & Cuts off after lag $q$ & Decays gradually \\
        \midrule
        ARMA(p,q) & Decays & Decays \\
        \bottomrule
    \end{tabular}
    \end{center}
\end{frame}

\begin{frame}{ACF/PACF Patterns: Visual Guide}
    \vspace{-0.3cm}
    \begin{center}
        \includegraphics[width=0.82\textwidth, height=0.52\textheight, keepaspectratio]{ch2_acf_pacf_patterns.pdf}
    \end{center}
    \vspace{-0.2cm}
    \begin{exampleblock}{Practical Identification}
        \textbf{AR}: ACF decays, PACF cuts off --- use PACF for order $p$. \textbf{MA}: ACF cuts off, PACF decays --- use ACF for order $q$. \textbf{ARMA}: Both decay --- use AIC/BIC.
    \end{exampleblock}
\end{frame}

\begin{frame}{Information Criteria}
    \textbf{Purpose:} Balance goodness-of-fit against model complexity

    \vspace{0.15cm}
    \textbf{Akaike Information Criterion (AIC):}
    $$\text{AIC} = -2\ln(\hat{L}) + 2k$$

    \textbf{Bayesian Information Criterion (BIC/SBC):}
    $$\text{BIC} = -2\ln(\hat{L}) + k\ln(n)$$

    where $\hat{L}$ = maximized likelihood, $k$ = number of parameters, $n$ = sample size

    \vspace{0.15cm}
    \textbf{Usage:}
    \begin{itemize}
        \item Lower values are better
        \item BIC penalizes complexity more strongly than AIC
        \item AIC tends to choose larger models; BIC more parsimonious
        \item Compare models fit to the \textit{same data}
    \end{itemize}
\end{frame}

\begin{frame}{AIC vs BIC: Model Selection}
    \begin{center}
        \includegraphics[width=0.95\textwidth, height=0.62\textheight, keepaspectratio]{aic_bic_comparison.pdf}
    \end{center}
    \vspace{-0.2cm}
    \begin{exampleblock}{Interpretation}
        White square marks the best model; lower values (green) are better. BIC typically selects simpler models than AIC due to stronger complexity penalty.
    \end{exampleblock}
\end{frame}

\begin{frame}{Parsimony Principle: Bias-Variance Trade-off}
    \begin{center}
        \includegraphics[width=0.95\textwidth, height=0.62\textheight, keepaspectratio]{parsimony_principle.pdf}
    \end{center}
    \vspace{-0.2cm}
    \begin{alertblock}{Key Principle}
        Simple models may underfit (high bias); complex models may overfit (high variance). The optimal model balances fit and complexity --- ``as simple as possible, but no simpler.''
    \end{alertblock}
\end{frame}

\begin{frame}{Automatic Model Selection}
    \textbf{Grid search approach:}
    \begin{enumerate}
        \item Fit ARMA(p,q) for $p = 0,1,\ldots,p_{max}$ and $q = 0,1,\ldots,q_{max}$
        \item Select model with lowest AIC or BIC
        \item Verify with diagnostic checks
    \end{enumerate}

    \vspace{0.15cm}
    \textbf{In Python (statsmodels):}
    \begin{itemize}
        \item \texttt{pm.auto\_arima()} from \texttt{pmdarima} package
        \item Automatically tests stationarity, searches over orders
        \item Returns best model by AIC/BIC
    \end{itemize}

    \vspace{0.15cm}
    \textbf{Caution:}
    \begin{itemize}
        \item Automatic selection is a starting point, not final answer
        \item Always check diagnostics
        \item Consider domain knowledge
    \end{itemize}
\end{frame}

\begin{frame}{Quiz: Information Criteria}
    \begin{alertblock}{Question}
        Comparing ARMA(1,1) vs ARMA(2,1) using BIC, which is correct?
    \end{alertblock}

    \vspace{0.3cm}

    \begin{enumerate}[(A)]
        \item Lower BIC always means better forecasts
        \item BIC penalizes complexity less than AIC
        \item The model with lower BIC is preferred
        \item BIC can only compare models with same \# of parameters
    \end{enumerate}
\end{frame}

\begin{frame}{Quiz: Information Criteria -- Answer}
    \begin{exampleblock}{Correct Answer: (C) The model with lower BIC is preferred}
        Lower BIC indicates better fit-complexity trade-off. BIC penalizes complexity \textit{more} than AIC.
    \end{exampleblock}
    \vspace{0.2cm}
    \begin{center}
        \includegraphics[width=0.95\textwidth, height=0.55\textheight, keepaspectratio]{ch2_quiz_information_criteria.pdf}
    \end{center}
\end{frame}

%=============================================================================
% SECTION 6: PARAMETER ESTIMATION
%=============================================================================
\section{Parameter Estimation}

\begin{frame}{Estimation Methods Overview}
    \textbf{Three main approaches:}

    \vspace{0.15cm}
    \textbf{1. Method of Moments / Yule-Walker (AR only)}
    \begin{itemize}
        \item Match sample autocorrelations to theoretical values
        \item Simple, closed-form for AR models
        \item Not efficient for MA components
    \end{itemize}

    \vspace{0.15cm}
    \textbf{2. Maximum Likelihood Estimation (MLE)}
    \begin{itemize}
        \item Most common approach
        \item Requires distributional assumption (usually Gaussian)
        \item Efficient and consistent
    \end{itemize}

    \vspace{0.15cm}
    \textbf{3. Conditional Least Squares}
    \begin{itemize}
        \item Minimize sum of squared residuals
        \item Conditioning on initial observations
        \item Computationally simpler than exact MLE
    \end{itemize}
\end{frame}

\begin{frame}{Estimation Methods Comparison}
    \begin{center}
        \includegraphics[width=0.95\textwidth, height=0.62\textheight, keepaspectratio]{estimation_comparison.pdf}
    \end{center}
    \vspace{-0.2cm}
    \begin{block}{Method Selection}
        \textbf{Yule-Walker}: Fast, closed-form for AR. \textbf{MLE}: Most efficient, requires optimization. \textbf{CSS}: Good balance of speed and accuracy.
    \end{block}
\end{frame}

\begin{frame}{Yule-Walker Equations for AR(p)}
    \begin{center}
        \includegraphics[width=0.95\textwidth, height=0.62\textheight, keepaspectratio]{yule_walker.pdf}
    \end{center}
    \vspace{-0.2cm}
    \begin{exampleblock}{Key Property}
        Yule-Walker exploits the relationship between AR parameters and autocorrelations. Replace theoretical $\rho(k)$ with sample estimates $\hat{\rho}(k)$ to obtain parameter estimates.
    \end{exampleblock}
\end{frame}

\begin{frame}{Yule-Walker Equations: Matrix Form}
    For AR(p): $X_t = \phi_1 X_{t-1} + \cdots + \phi_p X_{t-p} + \varepsilon_t$

    \vspace{0.15cm}
    \textbf{Yule-Walker equations:}
    $$\rho(k) = \phi_1\rho(k-1) + \phi_2\rho(k-2) + \cdots + \phi_p\rho(k-p)$$

    for $k = 1, 2, \ldots, p$

    \vspace{0.15cm}
    \textbf{Matrix form:}
    $$\begin{pmatrix} \rho(0) & \rho(1) & \cdots & \rho(p-1) \\ \rho(1) & \rho(0) & \cdots & \rho(p-2) \\ \vdots & \vdots & \ddots & \vdots \\ \rho(p-1) & \rho(p-2) & \cdots & \rho(0) \end{pmatrix}
    \begin{pmatrix} \phi_1 \\ \phi_2 \\ \vdots \\ \phi_p \end{pmatrix} =
    \begin{pmatrix} \rho(1) \\ \rho(2) \\ \vdots \\ \rho(p) \end{pmatrix}$$

    \vspace{0.15cm}
    \textbf{Estimation:} Replace $\rho(k)$ with sample autocorrelations $\hat{\rho}(k)$
\end{frame}

\begin{frame}{Proof: Yule-Walker Equations}
    \textbf{Goal:} Derive the relationship $\rho(k) = \phi_1\rho(k-1) + \cdots + \phi_p\rho(k-p)$

    \vspace{0.15cm}
    \textbf{Proof:} Start with AR(p): $X_t = \phi_1 X_{t-1} + \cdots + \phi_p X_{t-p} + \varepsilon_t$

    Multiply both sides by $X_{t-k}$ and take expectations:
    $$\E[X_t X_{t-k}] = \phi_1 \E[X_{t-1} X_{t-k}] + \cdots + \phi_p \E[X_{t-p} X_{t-k}] + \E[\varepsilon_t X_{t-k}]$$

    For $k \geq 1$: $\E[\varepsilon_t X_{t-k}] = 0$ (future shock uncorrelated with past)

    Using $\gamma(k) = \E[X_t X_{t-k}]$ (assuming zero mean):
    $$\gamma(k) = \phi_1 \gamma(k-1) + \phi_2 \gamma(k-2) + \cdots + \phi_p \gamma(k-p)$$

    Dividing by $\gamma(0)$:
    $$\boxed{\rho(k) = \phi_1 \rho(k-1) + \phi_2 \rho(k-2) + \cdots + \phi_p \rho(k-p)}$$

    \begin{exampleblock}{AR(1) Special Case}
        $\rho(k) = \phi_1 \rho(k-1) = \phi_1^k$ (using $\rho(0) = 1$)
    \end{exampleblock}
\end{frame}

\begin{frame}{Maximum Likelihood Estimation}
    \textbf{Assuming Gaussian errors:} $\varepsilon_t \sim N(0, \sigma^2)$

    \vspace{0.15cm}
    \textbf{Log-likelihood for ARMA(p,q):}
    $$\ell(\bm{\phi}, \bm{\theta}, \sigma^2) = -\frac{n}{2}\ln(2\pi) - \frac{n}{2}\ln(\sigma^2) - \frac{1}{2\sigma^2}\sum_{t=1}^{n}\varepsilon_t^2$$

    where $\varepsilon_t$ are the innovations computed recursively.

    \vspace{0.15cm}
    \textbf{Estimation procedure:}
    \begin{enumerate}
        \item Initialize: use method of moments or OLS for starting values
        \item Optimize: numerical methods (e.g., BFGS, Newton-Raphson)
        \item Iterate until convergence
    \end{enumerate}

    \vspace{0.15cm}
    \textbf{In practice:} Use \texttt{statsmodels.tsa.arima.model.ARIMA}
\end{frame}

\begin{frame}{Standard Errors and Inference}
    \textbf{Asymptotic distribution of MLE:}
    $$\hat{\bm{\theta}} \xrightarrow{d} N\left(\bm{\theta}_0, \frac{1}{n}\mathbf{I}(\bm{\theta}_0)^{-1}\right)$$

    where $\mathbf{I}(\bm{\theta})$ is the Fisher information matrix.

    \vspace{0.15cm}
    \textbf{Standard errors:} Square root of diagonal of $\frac{1}{n}\hat{\mathbf{I}}^{-1}$

    \vspace{0.15cm}
    \textbf{Hypothesis testing:}
    \begin{itemize}
        \item $H_0: \phi_j = 0$ (or $\theta_j = 0$)
        \item Test statistic: $z = \frac{\hat{\phi}_j}{SE(\hat{\phi}_j)} \sim N(0,1)$ asymptotically
        \item Reject if $|z| > 1.96$ at 5\% level
    \end{itemize}

    \vspace{0.15cm}
    \textbf{Confidence interval:} $\hat{\phi}_j \pm 1.96 \cdot SE(\hat{\phi}_j)$
\end{frame}

%=============================================================================
% SECTION 7: MODEL DIAGNOSTICS
%=============================================================================
\section{Model Diagnostics}

\begin{frame}{Residual Analysis}
    \textbf{If model is correctly specified, residuals should be white noise:}

    \vspace{0.15cm}
    \textbf{1. Plot residuals over time}
    \begin{itemize}
        \item Should fluctuate around zero
        \item No obvious patterns or trends
        \item Constant variance (no heteroskedasticity)
    \end{itemize}

    \vspace{0.15cm}
    \textbf{2. Check ACF of residuals}
    \begin{itemize}
        \item All correlations should be within confidence bands
        \item No significant spikes $\rightarrow$ white noise
    \end{itemize}

    \vspace{0.15cm}
    \textbf{3. Check histogram / Q-Q plot}
    \begin{itemize}
        \item Should be approximately normal (if assuming Gaussian)
        \item Heavy tails suggest non-normal errors
    \end{itemize}
\end{frame}

\begin{frame}{Residual Diagnostics: Example}
    \vspace{-0.3cm}
    \begin{center}
        \includegraphics[width=0.82\textwidth, height=0.52\textheight, keepaspectratio]{ch2_diagnostics.pdf}
    \end{center}
    \vspace{-0.2cm}
    \begin{block}{What to Look For}
        \textbf{Residual plot}: Random scatter around zero, constant variance. \textbf{ACF}: No significant spikes (white noise). \textbf{Q-Q plot}: Points on diagonal (normality).
    \end{block}
\end{frame}

\begin{frame}{Ljung-Box Test}
    \begin{defn}[Ljung-Box Test]
        Tests whether residuals are independently distributed (no autocorrelation).
    \end{defn}

    \textbf{Test statistic:}
    $$Q(m) = n(n+2)\sum_{k=1}^{m}\frac{\hat{\rho}_k^2}{n-k}$$

    \textbf{Hypotheses:}
    \begin{itemize}
        \item $H_0$: Residuals are white noise (no autocorrelation up to lag $m$)
        \item $H_1$: Residuals are autocorrelated
    \end{itemize}

    \textbf{Distribution:} Under $H_0$, $Q(m) \sim \chi^2(m-p-q)$ approximately

    \vspace{0.15cm}
    \textbf{Decision:}
    \begin{itemize}
        \item p-value $> 0.05$ $\rightarrow$ fail to reject $H_0$ $\rightarrow$ residuals look like white noise (good!)
        \item p-value $< 0.05$ $\rightarrow$ significant autocorrelation remains $\rightarrow$ model inadequate
    \end{itemize}
\end{frame}

\begin{frame}{Ljung-Box Test: Visual Illustration}
    \begin{center}
        \includegraphics[width=0.95\textwidth]{ch2_def_ljungbox.pdf}
    \end{center}
    \vspace{-0.2cm}
    \small Left: Good model -- residuals are white noise (no significant ACF). Right: Poor model -- residuals show autocorrelation.
\end{frame}

\begin{frame}{Diagnostic Checklist}
    \textbf{A good ARMA model should satisfy:}

    \vspace{0.15cm}
    \begin{enumerate}
        \item \textbf{Stationarity:} AR roots outside unit circle \\
              \textcolor{Forest}{$\checkmark$} Check with \texttt{arroots}

        \item \textbf{Invertibility:} MA roots outside unit circle \\
              \textcolor{Forest}{$\checkmark$} Check with \texttt{maroots}

        \item \textbf{White noise residuals:} No significant ACF \\
              \textcolor{Forest}{$\checkmark$} ACF plot, Ljung-Box test

        \item \textbf{Normal residuals:} (if assumed) \\
              \textcolor{Forest}{$\checkmark$} Q-Q plot, Jarque-Bera test

        \item \textbf{No heteroskedasticity:} Constant variance \\
              \textcolor{Forest}{$\checkmark$} Plot residuals, ARCH test

        \item \textbf{Parsimonious:} Lowest AIC/BIC among adequate models
    \end{enumerate}

    \vspace{0.15cm}
    \textbf{If diagnostics fail:} Return to identification, try different orders
\end{frame}

\begin{frame}{Quiz: Ljung-Box Test}
    \begin{alertblock}{Question}
        After fitting an ARMA model, you run the Ljung-Box test on residuals and get p-value = 0.03. What does this mean?
    \end{alertblock}

    \vspace{0.3cm}

    \begin{enumerate}[(A)]
        \item Model is adequate, residuals are white noise
        \item Model is inadequate, residuals have autocorrelation
        \item Need to increase sample size
        \item Test is inconclusive
    \end{enumerate}
\end{frame}

\begin{frame}{Quiz: Ljung-Box Test -- Answer}
    \begin{exampleblock}{Correct Answer: (B) Model is inadequate}
        p-value $< 0.05$ rejects $H_0$ (white noise), indicating remaining autocorrelation.
    \end{exampleblock}
    \vspace{0.2cm}
    \begin{center}
        \includegraphics[width=0.95\textwidth, height=0.55\textheight, keepaspectratio]{ch2_quiz_ljung_box.pdf}
    \end{center}
\end{frame}

%=============================================================================
% SECTION 8: FORECASTING
%=============================================================================
\section{Forecasting with ARMA}

\begin{frame}{Point Forecasts}
    \textbf{Optimal forecast:} Conditional expectation minimizes MSE
    $$\hat{X}_{n+h|n} = \E[X_{n+h} | X_n, X_{n-1}, \ldots]$$

    \vspace{0.15cm}
    \textbf{For AR(1):} $X_t = c + \phi X_{t-1} + \varepsilon_t$
    \begin{align*}
        \hat{X}_{n+1|n} &= c + \phi X_n \\
        \hat{X}_{n+2|n} &= c + \phi \hat{X}_{n+1|n} = c(1+\phi) + \phi^2 X_n \\
        \hat{X}_{n+h|n} &= \mu + \phi^h(X_n - \mu)
    \end{align*}

    \vspace{0.15cm}
    \textbf{Key property:} Forecasts converge to mean $\mu$ as $h \to \infty$

    \vspace{0.15cm}
    \textbf{For MA(1):} $X_t = \mu + \varepsilon_t + \theta\varepsilon_{t-1}$
    \begin{align*}
        \hat{X}_{n+1|n} &= \mu + \theta\varepsilon_n \\
        \hat{X}_{n+h|n} &= \mu \quad \text{for } h > 1
    \end{align*}
\end{frame}

\begin{frame}{Forecast Uncertainty}
    \textbf{Forecast error:}
    $$e_{n+h|n} = X_{n+h} - \hat{X}_{n+h|n}$$

    \textbf{Mean squared forecast error (MSFE):}
    $$\text{MSFE}(h) = \E[e_{n+h|n}^2] = \sigma^2 \sum_{j=0}^{h-1}\psi_j^2$$

    where $\psi_j$ are the MA($\infty$) coefficients.

    \vspace{0.15cm}
    \textbf{For AR(1):} $\psi_j = \phi^j$
    $$\text{MSFE}(h) = \sigma^2 \frac{1-\phi^{2h}}{1-\phi^2} \to \frac{\sigma^2}{1-\phi^2} = \Var(X_t)$$

    \vspace{0.15cm}
    \textbf{Key insight:} Forecast uncertainty increases with horizon, eventually reaching unconditional variance
\end{frame}

\begin{frame}{ARMA Forecasting with Confidence Intervals}
    \begin{center}
        \includegraphics[width=0.95\textwidth, height=0.62\textheight, keepaspectratio]{arma_forecast.pdf}
    \end{center}
    \vspace{-0.2cm}
    \begin{exampleblock}{Forecast Properties}
        Point forecasts converge to the unconditional mean as horizon increases. Confidence intervals widen with horizon, reflecting increasing uncertainty about distant future values.
    \end{exampleblock}
\end{frame}

\begin{frame}{AR(1) Forecasting: Mean Reversion}
    \vspace{-0.3cm}
    \begin{center}
        \includegraphics[width=0.82\textwidth, height=0.58\textheight, keepaspectratio]{ch2_ar1_forecast.pdf}
    \end{center}
    \vspace{-0.2cm}
    {\footnotesize
    \begin{itemize}
        \item Forecasts converge to the unconditional mean $\mu$ as horizon increases
        \item Rate of convergence depends on $|\phi|$: higher values mean slower reversion
        \item Confidence intervals widen with horizon, eventually reaching unconditional variance
    \end{itemize}
    }
\end{frame}

\begin{frame}{Forecast Error Variance Over Horizon}
    \begin{center}
        \includegraphics[width=0.95\textwidth, height=0.62\textheight, keepaspectratio]{forecast_error_decomposition.pdf}
    \end{center}
    \vspace{-0.2cm}
    \begin{block}{Variance Decomposition}
        Forecast error variance grows with horizon $h$, accumulating contributions from future shocks. For stationary processes, it converges to the unconditional variance $\gamma(0)$ as $h \to \infty$.
    \end{block}
\end{frame}

\begin{frame}{Confidence Intervals for Forecasts}
    \textbf{Assuming Gaussian errors:}
    $$X_{n+h} | X_n, \ldots \sim N\left(\hat{X}_{n+h|n}, \text{MSFE}(h)\right)$$

    \vspace{0.15cm}
    \textbf{$(1-\alpha)$ confidence interval:}
    $$\hat{X}_{n+h|n} \pm z_{\alpha/2} \cdot \sqrt{\text{MSFE}(h)}$$

    where $z_{\alpha/2} = 1.96$ for 95\% CI.

    \vspace{0.15cm}
    \textbf{Properties:}
    \begin{itemize}
        \item Intervals widen as horizon increases
        \item Eventually converge to unconditional interval: $\mu \pm z_{\alpha/2}\sigma_X$
        \item Width depends on model parameters (AR coefficients, etc.)
    \end{itemize}

    \vspace{0.15cm}
    \textbf{In Python:} \texttt{model.get\_forecast(h).conf\_int()}
\end{frame}

\begin{frame}{Forecast Evaluation}
    \textbf{Out-of-sample testing:}
    \begin{enumerate}
        \item Split data: training set (fit model) and test set (evaluate)
        \item Generate forecasts for test period
        \item Compare forecasts to actual values
    \end{enumerate}

    \vspace{0.15cm}
    \textbf{Metrics (from Chapter 1):}
    \begin{itemize}
        \item MAE $= \frac{1}{n}\sum|e_t|$
        \item RMSE $= \sqrt{\frac{1}{n}\sum e_t^2}$
        \item MAPE $= \frac{100}{n}\sum\left|\frac{e_t}{X_t}\right|$
    \end{itemize}

    \vspace{0.15cm}
    \textbf{Rolling/expanding window:}
    \begin{itemize}
        \item Re-estimate model as new data arrives
        \item More realistic assessment of forecast performance
    \end{itemize}
\end{frame}

\begin{frame}{Train/Validation/Test Forecasting Example}
    \begin{center}
        \includegraphics[width=0.95\textwidth, height=0.62\textheight, keepaspectratio]{forecast_train_val_test.pdf}
    \end{center}
    \vspace{-0.2cm}
    \begin{alertblock}{Best Practice}
        Always evaluate forecasts on held-out data. Use training set for model fitting, validation set for model selection, and test set for final performance assessment.
    \end{alertblock}
\end{frame}

\begin{frame}{Rolling Window Forecasting}
    \begin{center}
        \includegraphics[width=0.95\textwidth, height=0.58\textheight, keepaspectratio]{ch2_rolling_forecast.pdf}
    \end{center}
    \vspace{-0.2cm}
    \begin{block}{Rolling Forecast Methodology}
        \begin{itemize}
            \item \textbf{Fixed window:} Re-estimate model using most recent $w$ observations
            \item \textbf{Expanding window:} Use all available data up to forecast origin
            \item Generate 1-step ahead forecast, move window forward, repeat
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}{Rolling vs Multi-Step Forecasting}
    \begin{center}
        \includegraphics[width=0.95\textwidth, height=0.58\textheight, keepaspectratio]{ch2_rolling_vs_multistep.pdf}
    \end{center}
    \vspace{-0.2cm}
    \begin{exampleblock}{Key Differences}
        \begin{itemize}
            \item \textbf{Rolling 1-step:} More accurate, requires frequent re-estimation
            \item \textbf{Direct multi-step:} Estimate separate model for each horizon $h$
            \item \textbf{Recursive multi-step:} Iterate 1-step forecasts (error accumulation)
        \end{itemize}
    \end{exampleblock}
\end{frame}

\begin{frame}{Real Data Application: Forecasting Comparison}
    \begin{center}
        \includegraphics[width=0.95\textwidth, height=0.58\textheight, keepaspectratio]{real_data_forecast_comparison.pdf}
    \end{center}
    \vspace{-0.2cm}
    \begin{block}{Practical Considerations}
        \begin{itemize}
            \item Real data often exhibits non-stationarity, structural breaks
            \item Compare multiple models: ARMA, exponential smoothing, naive
            \item Use cross-validation or rolling evaluation for robust assessment
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}{Quiz: Forecast Properties}
    \begin{alertblock}{Question}
        For a stationary AR(1) model, what happens to forecasts as horizon $h \to \infty$?
    \end{alertblock}

    \vspace{0.3cm}

    \begin{enumerate}[(A)]
        \item Forecasts grow without bound
        \item Forecasts oscillate forever
        \item Forecasts converge to the unconditional mean $\mu$
        \item Forecasts become more accurate
    \end{enumerate}
\end{frame}

\begin{frame}{Quiz: Forecast Properties -- Answer}
    \begin{exampleblock}{Correct Answer: (C) Forecasts converge to $\mu$}
        $\hat{X}_{n+h|n} = \mu + \phi^h(X_n - \mu) \to \mu$ as $h \to \infty$ (since $|\phi|<1$)
    \end{exampleblock}
    \vspace{0.2cm}
    \begin{center}
        \includegraphics[width=0.95\textwidth, height=0.55\textheight, keepaspectratio]{ch2_quiz_forecast_properties.pdf}
    \end{center}
\end{frame}

%=============================================================================
% SECTION 9: PRACTICAL IMPLEMENTATION
%=============================================================================
\section{Practical Implementation}

\begin{frame}[fragile]{Python Implementation: Fitting ARMA}
    \textbf{Using statsmodels:}

    \vspace{0.2cm}
    \texttt{\small from statsmodels.tsa.arima.model import ARIMA}

    \vspace{0.2cm}
    \texttt{\small \# Fit ARMA(2,1) --- note: ARIMA(p,d,q) with d=0}

    \texttt{\small model = ARIMA(data, order=(2, 0, 1))}

    \texttt{\small results = model.fit()}

    \vspace{0.2cm}
    \texttt{\small \# Summary}

    \texttt{\small print(results.summary())}

    \vspace{0.2cm}
    \texttt{\small \# Forecasting}

    \texttt{\small forecast = results.get\_forecast(steps=10)}

    \texttt{\small print(forecast.predicted\_mean)}

    \texttt{\small print(forecast.conf\_int())}

    \vspace{0.15cm}
    \textbf{Note:} ARIMA with $d=0$ is equivalent to ARMA
\end{frame}

\begin{frame}[fragile]{Python: Model Selection with pmdarima}
    \textbf{Automatic ARIMA selection:}

    \vspace{0.2cm}
    \texttt{\small import pmdarima as pm}

    \vspace{0.2cm}
    \texttt{\small \# Auto ARIMA with AIC criterion}

    \texttt{\small model = pm.auto\_arima(data,}

    \texttt{\small \hspace{2cm} start\_p=0, max\_p=5,}

    \texttt{\small \hspace{2cm} start\_q=0, max\_q=5,}

    \texttt{\small \hspace{2cm} d=0,  \# No differencing for stationary data}

    \texttt{\small \hspace{2cm} seasonal=False,}

    \texttt{\small \hspace{2cm} information\_criterion='aic',}

    \texttt{\small \hspace{2cm} trace=True)}

    \vspace{0.2cm}
    \texttt{\small print(model.summary())}

    \vspace{0.15cm}
    \textbf{Output:} Best model order and fitted parameters
\end{frame}

\begin{frame}{Workflow Summary}
    \begin{enumerate}
        \item \textbf{Data preparation}
        \begin{itemize}
            \item Check for missing values, outliers
            \item Transform if necessary (log, differencing)
        \end{itemize}

        \item \textbf{Stationarity check}
        \begin{itemize}
            \item Visual inspection: time plot, ACF
            \item Formal tests: ADF, KPSS
            \item Difference if non-stationary
        \end{itemize}

        \item \textbf{Model identification}
        \begin{itemize}
            \item ACF/PACF patterns
            \item Information criteria grid search
        \end{itemize}

        \item \textbf{Estimation and diagnostics}
        \begin{itemize}
            \item Fit model, check significance
            \item Residual analysis, Ljung-Box test
        \end{itemize}

        \item \textbf{Forecasting}
        \begin{itemize}
            \item Point forecasts with confidence intervals
            \item Out-of-sample validation
        \end{itemize}
    \end{enumerate}
\end{frame}

%=============================================================================
% CASE STUDY: REAL DATA
%=============================================================================
\section{Case Study: Real Data}

\begin{frame}{Case Study: Sunspot Numbers}
    \begin{center}
        \includegraphics[width=0.95\textwidth, height=0.58\textheight, keepaspectratio]{ch2_case_raw_data.pdf}
    \end{center}
    \vspace{-0.2cm}
    \begin{block}{Data Description}
        \textbf{Yearly sunspot numbers (1700--2008)}: Classic time series dataset. Stationary series with approximately 11-year cycles. We will apply the complete Box-Jenkins methodology.
    \end{block}
\end{frame}

\begin{frame}{Step 1: ACF/PACF Analysis}
    \begin{center}
        \includegraphics[width=0.95\textwidth, height=0.58\textheight, keepaspectratio]{ch2_case_acf_pacf.pdf}
    \end{center}
    \vspace{-0.2cm}
    \begin{exampleblock}{Identification}
        \begin{itemize}
            \item \textbf{ACF}: Slow, sinusoidal decay --- suggests AR process
            \item \textbf{PACF}: Significant spikes at lags 1, 2, 9 --- suggests AR(9) or AR(2)
            \item Series appears stationary (no differencing needed, $d=0$)
        \end{itemize}
    \end{exampleblock}
\end{frame}

\begin{frame}{Step 2: Model Comparison}
    \begin{center}
        \includegraphics[width=0.95\textwidth, height=0.55\textheight, keepaspectratio]{ch2_case_model_comparison.pdf}
    \end{center}
    \vspace{-0.2cm}
    \begin{alertblock}{Model Selection}
        We compare several candidate models using AIC criterion. The \textbf{AR(9)} model has the lowest AIC, capturing the 11-year solar cycle.
    \end{alertblock}
\end{frame}

\begin{frame}{Step 3: Diagnostic Checking}
    \begin{center}
        \includegraphics[width=0.95\textwidth, height=0.65\textheight, keepaspectratio]{ch2_case_diagnostics.pdf}
    \end{center}
    \vspace{-0.2cm}
    \begin{block}{AR(9) Diagnostics}
        Residuals resemble white noise: zero mean, constant variance, no significant ACF structure, approximately normal distribution.
    \end{block}
\end{frame}

\begin{frame}{Step 4: Forecasting}
    \begin{center}
        \includegraphics[width=0.95\textwidth, height=0.58\textheight, keepaspectratio]{ch2_case_forecast.pdf}
    \end{center}
    \vspace{-0.2cm}
    \begin{exampleblock}{Results}
        \begin{itemize}
            \item AR(9) model captures the cyclical nature of sunspots
            \item 95\% confidence intervals cover most actual values
            \item Test set RMSE: approximately 30 (reasonable for this volatile series)
        \end{itemize}
    \end{exampleblock}
\end{frame}

%=============================================================================
% SUMMARY
%=============================================================================
\section{Summary}

\begin{frame}{Key Takeaways}
    \begin{enumerate}
        \item \textbf{AR(p) models:} Current value depends on $p$ past values
        \begin{itemize}
            \item Stationarity: roots of $\phi(z)$ outside unit circle
            \item PACF cuts off at lag $p$
        \end{itemize}

        \item \textbf{MA(q) models:} Current value depends on $q$ past shocks
        \begin{itemize}
            \item Always stationary; invertibility: roots of $\theta(z)$ outside unit circle
            \item ACF cuts off at lag $q$
        \end{itemize}

        \item \textbf{ARMA(p,q):} Combines AR and MA for flexible modeling
        \begin{itemize}
            \item Both ACF and PACF decay
        \end{itemize}

        \item \textbf{Box-Jenkins:} Identify $\to$ Estimate $\to$ Diagnose $\to$ Forecast

        \item \textbf{Diagnostics:} Residuals must be white noise

        \item \textbf{Forecasts:} Converge to mean; uncertainty increases with horizon
    \end{enumerate}
\end{frame}

\begin{frame}{Next Chapter Preview}
    \textbf{Chapter 3: ARIMA and Seasonal Models}

    \vspace{0.15cm}
    \begin{itemize}
        \item ARIMA(p,d,q): Integrated models for non-stationary data
        \item Seasonal ARIMA: SARIMA(p,d,q)(P,D,Q)$_s$
        \item Seasonal differencing
        \item Real-world applications with seasonal patterns
    \end{itemize}

    \vspace{0.15cm}
    \textbf{Reading:}
    \begin{itemize}
        \item Hyndman \& Athanasopoulos, \textit{Forecasting: Principles and Practice}, Ch. 9
        \item Box, Jenkins, Reinsel \& Ljung, \textit{Time Series Analysis}, Ch. 3-4
    \end{itemize}
\end{frame}

%=============================================================================
% REFERENCES
%=============================================================================
\begin{frame}{References}
    \footnotesize
    \begin{thebibliography}{99}
        \bibitem{box2015} Box, G.E.P., Jenkins, G.M., Reinsel, G.C., \& Ljung, G.M. (2015). \textit{Time Series Analysis: Forecasting and Control}. 5th ed., Wiley.

        \bibitem{hamilton1994} Hamilton, J.D. (1994). \textit{Time Series Analysis}. Princeton University Press.

        \bibitem{hyndman2021} Hyndman, R.J., \& Athanasopoulos, G. (2021). \textit{Forecasting: Principles and Practice}. 3rd ed., OTexts.

        \bibitem{brockwell2016} Brockwell, P.J., \& Davis, R.A. (2016). \textit{Introduction to Time Series and Forecasting}. 3rd ed., Springer.

        \bibitem{shumway2017} Shumway, R.H., \& Stoffer, D.S. (2017). \textit{Time Series Analysis and Its Applications}. 4th ed., Springer.
    \end{thebibliography}
\end{frame}

%=============================================================================
% DATA SOURCES
%=============================================================================
\begin{frame}{Data Sources}
    \begin{block}{Simulated Data Used in This Chapter}
        \begin{itemize}
            \item \textbf{AR(1), AR(2) processes}: Simulated with various $\phi$ parameters
            \item \textbf{MA(1), MA(q) processes}: Simulated with various $\theta$ parameters
            \item \textbf{ARMA(p,q) processes}: Combined AR and MA simulations
        \end{itemize}
    \end{block}

    \begin{block}{Software \& Tools}
        \begin{itemize}
            \item \textbf{Python}: statsmodels (ARIMA), numpy, matplotlib
            \item \textbf{R}: forecast, tseries packages
            \item \textbf{Key functions}: ARIMA(), auto.arima(), acf(), pacf()
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}{}
    \centering
    \Huge\textcolor{MainBlue}{Thank You!}

    \vspace{1cm}

    \Large Questions?
\end{frame}

\end{document}
