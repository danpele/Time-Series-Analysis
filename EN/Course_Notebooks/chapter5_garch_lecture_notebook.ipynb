{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/danpele/Time-Series-Analysis/blob/main/EN/Course_Notebooks/chapter5_garch_lecture_notebook.ipynb)\n\n---"
  },
  {
   "cell_type": "markdown",
   "id": "cell-1",
   "metadata": {},
   "source": [
    "# Volatility Models: ARCH, GARCH and Extensions\n",
    "\n",
    "**Course:** Time Series Analysis and Forecasting  \n",
    "**Program:** Bachelor program, Faculty of Cybernetics, Statistics and Economic Informatics, Bucharest University of Economic Studies, Romania  \n",
    "**Academic Year:** 2025-2026\n",
    "\n",
    "---\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this notebook, you will be able to:\n",
    "1. Understand volatility clustering and stylized facts of financial returns\n",
    "2. Estimate and interpret ARCH and GARCH models\n",
    "3. Apply asymmetric models (EGARCH, GJR-GARCH) to capture leverage effect\n",
    "4. Perform model diagnostics and selection\n",
    "5. Forecast volatility and calculate Value at Risk (VaR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-2",
   "metadata": {},
   "source": [
    "## Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-3",
   "metadata": {},
   "outputs": [],
   "source": "# Install packages if needed (for Colab)\ntry:\n    from arch import arch_model\n    import yfinance as yf\nexcept ImportError:\n    !pip install arch yfinance --quiet\n    from arch import arch_model\n    import yfinance as yf\n\n# Core libraries\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom scipy import stats\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Statistical models\nfrom arch import arch_model\nfrom arch.univariate import GARCH, EGARCH, ConstantMean\nfrom statsmodels.stats.diagnostic import het_arch, acorr_ljungbox\nfrom statsmodels.tsa.stattools import adfuller\nfrom statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n\n# Plotting style\nplt.rcParams['figure.figsize'] = (12, 5)\nplt.rcParams['font.size'] = 11\nplt.rcParams['axes.facecolor'] = 'none'\nplt.rcParams['figure.facecolor'] = 'none'\nplt.rcParams['savefig.facecolor'] = 'none'\nplt.rcParams['savefig.transparent'] = True\nplt.rcParams['axes.grid'] = False\nplt.rcParams['axes.spines.top'] = False\nplt.rcParams['axes.spines.right'] = False\nplt.rcParams['legend.frameon'] = False\n\n# Colors (IDA scheme)\nCOLORS = {\n    'blue': '#1A3A6E',\n    'red': '#DC3545',\n    'green': '#2E7D32',\n    'orange': '#E67E22',\n    'gray': '#666666'\n}\n\nprint(\"All libraries loaded successfully!\")"
  },
  {
   "cell_type": "markdown",
   "id": "cell-4",
   "metadata": {},
   "source": [
    "## 1. Why Model Volatility?\n",
    "\n",
    "**ARIMA models assume constant variance (homoskedasticity)**\n",
    "\n",
    "Financial time series exhibit:\n",
    "- **Volatility clustering**: Large changes followed by large changes\n",
    "- **Fat tails (leptokurtosis)**: More extreme values than normal distribution\n",
    "- **Leverage effect**: Negative returns increase volatility more than positive returns\n",
    "\n",
    "These features require **conditional heteroskedasticity models**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-5",
   "metadata": {},
   "outputs": [],
   "source": "# Download real S&P 500 data to demonstrate volatility clustering\nprint(\"Downloading S&P 500 data from Yahoo Finance...\")\n\nsp500 = yf.download('^GSPC', start='2000-01-01', end='2024-12-31', progress=False)\nsp500_close = sp500['Close'].squeeze() if isinstance(sp500['Close'], pd.DataFrame) else sp500['Close']\n\n# Calculate returns (percentage)\nreturns = (sp500_close.pct_change() * 100).dropna()\nreturns = pd.Series(returns.values, index=returns.index, name='returns')\n\nprint(f\"\\nS&P 500 Data Summary:\")\nprint(f\"Period: {returns.index[0].date()} to {returns.index[-1].date()}\")\nprint(f\"Observations: {len(returns)}\")\nprint(f\"\\nBasic Statistics:\")\nprint(f\"  Mean return: {returns.mean():.4f}%\")\nprint(f\"  Std deviation: {returns.std():.4f}%\")\nprint(f\"  Min: {returns.min():.2f}%\")\nprint(f\"  Max: {returns.max():.2f}%\")\nprint(f\"  Skewness: {stats.skew(returns):.4f}\")\nprint(f\"  Kurtosis: {stats.kurtosis(returns)+3:.4f} (Normal = 3)\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-6",
   "metadata": {},
   "outputs": [],
   "source": "# Visualize volatility clustering in real S&P 500 data\nfig, axes = plt.subplots(2, 1, figsize=(14, 8), sharex=True)\n\n# Returns\naxes[0].plot(returns.index, returns.values, color=COLORS['blue'], linewidth=0.5, alpha=0.8)\naxes[0].axhline(y=0, color='black', linewidth=0.5)\naxes[0].set_ylabel('Returns (%)')\naxes[0].set_title('S&P 500 Daily Returns (2000-2024): Volatility Clustering', fontweight='bold')\n\n# Mark crisis periods\ncrisis_periods = [\n    ('2008-09-01', '2009-03-31', '2008 Financial Crisis'),\n    ('2020-02-15', '2020-04-30', 'COVID-19'),\n    ('2022-01-01', '2022-10-31', '2022 Bear Market')\n]\nfor start, end, label in crisis_periods:\n    axes[0].axvspan(pd.Timestamp(start), pd.Timestamp(end), alpha=0.2, color=COLORS['red'])\n\n# Rolling volatility (20-day)\nrolling_vol = returns.rolling(window=20).std()\naxes[1].plot(rolling_vol.index, rolling_vol.values, color=COLORS['red'], linewidth=0.8)\naxes[1].fill_between(rolling_vol.index, 0, rolling_vol.values, color=COLORS['red'], alpha=0.3)\naxes[1].set_ylabel('Rolling Volatility (%)')\naxes[1].set_xlabel('Date')\naxes[1].set_title('20-Day Rolling Volatility', fontweight='bold')\n\nfor start, end, label in crisis_periods:\n    axes[1].axvspan(pd.Timestamp(start), pd.Timestamp(end), alpha=0.2, color=COLORS['red'])\n\nplt.tight_layout()\nplt.show()\n\nprint(\"\\nKey observations:\")\nprint(\"  • Volatility clustering is clearly visible - large moves followed by large moves\")\nprint(\"  • Crisis periods (2008, 2020) show extreme volatility spikes\")\nprint(\"  • Periods of calm are followed by periods of calm\")"
  },
  {
   "cell_type": "markdown",
   "id": "cell-7",
   "metadata": {},
   "source": [
    "## 2. Stylized Facts of Financial Returns\n",
    "\n",
    "Key empirical regularities:\n",
    "1. **No autocorrelation** in returns $r_t$\n",
    "2. **Significant autocorrelation** in $r_t^2$ and $|r_t|$\n",
    "3. **Fat tails** (kurtosis > 3)\n",
    "4. **Volatility clustering**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-8",
   "metadata": {},
   "outputs": [],
   "source": "# Check stylized facts of real S&P 500 returns\nfig, axes = plt.subplots(2, 2, figsize=(14, 10))\n\n# ACF of returns\nplot_acf(returns.values, lags=30, ax=axes[0, 0], color=COLORS['blue'],\n         vlines_kwargs={'color': COLORS['blue']}, alpha=0.05)\naxes[0, 0].set_title('ACF of Returns (should be ~0)', fontweight='bold')\n\n# ACF of squared returns\nplot_acf(returns.values**2, lags=30, ax=axes[0, 1], color=COLORS['red'],\n         vlines_kwargs={'color': COLORS['red']}, alpha=0.05)\naxes[0, 1].set_title('ACF of Squared Returns (significant!)', fontweight='bold')\n\n# Distribution vs Normal\naxes[1, 0].hist(returns.values, bins=100, density=True, color=COLORS['blue'],\n                alpha=0.7, edgecolor='white', label='S&P 500')\nx = np.linspace(returns.min(), returns.max(), 100)\naxes[1, 0].plot(x, stats.norm.pdf(x, float(returns.mean()), float(returns.std())),\n                color=COLORS['red'], linewidth=2, label='Normal')\nkurtosis_val = float(stats.kurtosis(returns.values) + 3)\naxes[1, 0].set_title(f'Distribution: Kurtosis = {kurtosis_val:.2f} (Normal = 3)', fontweight='bold')\naxes[1, 0].legend(loc='upper center', bbox_to_anchor=(0.5, -0.1), ncol=2)\naxes[1, 0].set_xlim(-10, 10)\n\n# QQ-Plot\nstats.probplot(returns.values.flatten(), dist=\"norm\", plot=axes[1, 1])\naxes[1, 1].get_lines()[0].set_color(COLORS['blue'])\naxes[1, 1].get_lines()[0].set_markersize(3)\naxes[1, 1].get_lines()[1].set_color(COLORS['red'])\naxes[1, 1].set_title('QQ-Plot: Fat Tails Visible', fontweight='bold')\n\nplt.tight_layout()\nplt.show()\n\nprint(\"\\nStylized Facts Summary (Real S&P 500 Data):\")\nprint(f\"  Mean return: {float(returns.mean()):.4f}%\")\nprint(f\"  Std deviation: {float(returns.std()):.4f}%\")\nprint(f\"  Skewness: {float(stats.skew(returns.values)):.4f}\")\nprint(f\"  Kurtosis: {kurtosis_val:.4f} (Normal = 3)\")\nprint(f\"\\n  → Fat tails confirmed (kurtosis >> 3)\")\nprint(f\"  → Negative skewness (crashes more severe than rallies)\")"
  },
  {
   "cell_type": "markdown",
   "id": "cell-9",
   "metadata": {},
   "source": [
    "## 3. Testing for ARCH Effects\n",
    "\n",
    "**Engle's ARCH-LM Test:**\n",
    "1. Estimate mean model, get residuals $\\hat{\\varepsilon}_t$\n",
    "2. Regress $\\hat{\\varepsilon}_t^2$ on its lags\n",
    "3. Test statistic: $LM = T \\cdot R^2 \\sim \\chi^2(q)$\n",
    "\n",
    "- $H_0$: No ARCH effects\n",
    "- $H_1$: ARCH effects present"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-10",
   "metadata": {},
   "outputs": [],
   "source": "# ARCH-LM test on real S&P 500 data\nprint(\"ARCH-LM Test for Heteroskedasticity (S&P 500)\")\nprint(\"=\"*50)\n\n# Test with different lag orders\nreturns_demeaned = returns.values - returns.values.mean()\nfor q in [5, 10, 20]:\n    lm_stat, lm_pvalue, f_stat, f_pvalue = het_arch(returns_demeaned, nlags=q)\n    result = \"Reject H0 → ARCH present\" if lm_pvalue < 0.05 else \"Cannot reject H0\"\n    print(f\"  Lags = {q}: LM = {lm_stat:.2f}, p-value = {lm_pvalue:.6f} → {result}\")\n\nprint(\"\\n⚠️ Strong ARCH effects detected in S&P 500 returns!\")\nprint(\"   This confirms the need for GARCH modeling.\")"
  },
  {
   "cell_type": "markdown",
   "id": "cell-11",
   "metadata": {},
   "source": [
    "## 4. The ARCH(q) Model\n",
    "\n",
    "**Engle (1982) - Nobel Prize 2003**\n",
    "\n",
    "$$\\varepsilon_t = \\sigma_t z_t, \\quad z_t \\sim \\text{i.i.d.}(0, 1)$$\n",
    "$$\\sigma_t^2 = \\omega + \\alpha_1 \\varepsilon_{t-1}^2 + \\cdots + \\alpha_q \\varepsilon_{t-q}^2$$\n",
    "\n",
    "**Constraints:**\n",
    "- $\\omega > 0$\n",
    "- $\\alpha_i \\geq 0$\n",
    "- $\\sum \\alpha_i < 1$ (stationarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-12",
   "metadata": {},
   "outputs": [],
   "source": "# Estimate ARCH(5) model on real S&P 500 data\nprint(\"ARCH(5) Model Estimation - S&P 500\")\nprint(\"=\"*50)\n\nmodel_arch = arch_model(returns.values, vol='ARCH', p=5, dist='normal')\nres_arch = model_arch.fit(disp='off')\n\nprint(res_arch.summary())"
  },
  {
   "cell_type": "markdown",
   "id": "cell-13",
   "metadata": {},
   "source": [
    "## 5. The GARCH(p,q) Model\n",
    "\n",
    "**Bollerslev (1986)** - Adds lagged conditional variance for persistence:\n",
    "\n",
    "$$\\sigma_t^2 = \\omega + \\sum_{i=1}^{q} \\alpha_i \\varepsilon_{t-i}^2 + \\sum_{j=1}^{p} \\beta_j \\sigma_{t-j}^2$$\n",
    "\n",
    "**GARCH(1,1) - The workhorse model:**\n",
    "$$\\sigma_t^2 = \\omega + \\alpha \\varepsilon_{t-1}^2 + \\beta \\sigma_{t-1}^2$$\n",
    "\n",
    "- $\\alpha$ = reaction to news (ARCH effect)\n",
    "- $\\beta$ = persistence (GARCH effect)\n",
    "- $\\alpha + \\beta$ = total persistence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-14",
   "metadata": {},
   "outputs": [],
   "source": "# Estimate GARCH(1,1) model on real S&P 500 data\nprint(\"GARCH(1,1) Model Estimation - S&P 500\")\nprint(\"=\"*50)\n\nmodel_garch = arch_model(returns.values, vol='Garch', p=1, q=1, dist='normal')\nres_garch = model_garch.fit(disp='off')\n\nprint(res_garch.summary())"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-15",
   "metadata": {},
   "outputs": [],
   "source": "# Interpret GARCH(1,1) parameters for S&P 500\nomega_est = res_garch.params['omega']\nalpha_est = res_garch.params['alpha[1]']\nbeta_est = res_garch.params['beta[1]']\n\nprint(\"GARCH(1,1) Parameter Interpretation - S&P 500\")\nprint(\"=\"*50)\nprint(f\"\\nEstimated Parameters:\")\nprint(f\"  ω (omega) = {omega_est:.6f}\")\nprint(f\"  α (alpha) = {alpha_est:.4f}  ← News reaction (ARCH effect)\")\nprint(f\"  β (beta)  = {beta_est:.4f}  ← Persistence (GARCH effect)\")\n\nprint(f\"\\nDerived Quantities:\")\npersistence = alpha_est + beta_est\nprint(f\"  α + β = {persistence:.4f}  ← Total persistence (very high!)\")\n\nif persistence < 1:\n    uncond_var = omega_est / (1 - persistence)\n    uncond_vol = np.sqrt(uncond_var)\n    half_life = np.log(0.5) / np.log(persistence)\n    print(f\"  Unconditional variance: {uncond_var:.6f}\")\n    print(f\"  Unconditional volatility: {uncond_vol:.2f}% daily\")\n    print(f\"  Annualized volatility: {uncond_vol * np.sqrt(252):.2f}%\")\n    print(f\"  Half-life: {half_life:.1f} trading days\")\n    print(f\"\\n  → Shocks take ~{half_life:.0f} days to decay by half\")\nelse:\n    print(\"  ⚠️ Model is IGARCH (α + β ≥ 1), unconditional variance undefined\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-16",
   "metadata": {},
   "outputs": [],
   "source": "# Visualize GARCH(1,1) conditional volatility for S&P 500\nfig, axes = plt.subplots(2, 1, figsize=(14, 8), sharex=True)\n\n# Returns with conditional volatility bands\ncond_vol = res_garch.conditional_volatility\naxes[0].plot(returns.index, returns.values, color=COLORS['blue'], linewidth=0.5, alpha=0.7, label='Returns')\naxes[0].plot(returns.index, 2*cond_vol, color=COLORS['red'], linewidth=0.8, label='±2σ bands')\naxes[0].plot(returns.index, -2*cond_vol, color=COLORS['red'], linewidth=0.8)\naxes[0].axhline(y=0, color='black', linewidth=0.5)\naxes[0].set_ylabel('Return (%)')\naxes[0].set_title('S&P 500 Returns with GARCH(1,1) Volatility Bands', fontweight='bold')\naxes[0].legend(loc='upper center', bbox_to_anchor=(0.5, -0.05), ncol=2)\n\n# Conditional volatility\naxes[1].plot(returns.index, cond_vol, color=COLORS['red'], linewidth=0.8)\naxes[1].fill_between(returns.index, 0, cond_vol, color=COLORS['red'], alpha=0.3)\naxes[1].set_ylabel('Volatility (%)')\naxes[1].set_xlabel('Date')\naxes[1].set_title('GARCH(1,1) Conditional Volatility', fontweight='bold')\n\n# Mark crisis periods\nfor start, end, label in crisis_periods:\n    axes[1].axvspan(pd.Timestamp(start), pd.Timestamp(end), alpha=0.15, color='gray')\n\nplt.tight_layout()\nplt.show()\n\nprint(f\"\\nVolatility Statistics:\")\nprint(f\"  Mean conditional volatility: {cond_vol.mean():.2f}%\")\nprint(f\"  Max volatility: {cond_vol.max():.2f}% (during crisis)\")\nprint(f\"  Min volatility: {cond_vol.min():.2f}% (calm periods)\")"
  },
  {
   "cell_type": "markdown",
   "id": "cell-17",
   "metadata": {},
   "source": [
    "## 6. Alternative Distributions\n",
    "\n",
    "Normal distribution underestimates tail risk. Alternatives:\n",
    "- **Student-t**: Fat tails, estimated degrees of freedom $\\nu$\n",
    "- **GED**: Generalized Error Distribution\n",
    "- **Skewed Student-t**: Asymmetry + fat tails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-18",
   "metadata": {},
   "outputs": [],
   "source": "# Compare different distributions for S&P 500\nprint(\"GARCH(1,1) with Different Distributions - S&P 500\")\nprint(\"=\"*60)\n\ndistributions = ['normal', 't', 'skewt', 'ged']\nresults = {}\n\nfor dist in distributions:\n    try:\n        model = arch_model(returns.values, vol='Garch', p=1, q=1, dist=dist)\n        res = model.fit(disp='off')\n        results[dist] = res\n        print(f\"{dist:>8}: AIC = {res.aic:.2f}, BIC = {res.bic:.2f}, LogLik = {res.loglikelihood:.2f}\")\n    except:\n        print(f\"{dist:>8}: Failed to converge\")\n\n# Best model\nbest_dist = min(results, key=lambda x: results[x].aic)\nprint(f\"\\nBest distribution by AIC: {best_dist}\")\nprint(\"→ Student-t captures the fat tails observed in real financial data!\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Student-t GARCH details\n",
    "if 't' in results:\n",
    "    print(\"GARCH(1,1) with Student-t Distribution\")\n",
    "    print(\"=\"*50)\n",
    "    print(results['t'].summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-20",
   "metadata": {},
   "source": [
    "## 7. Asymmetric GARCH Models\n",
    "\n",
    "### Leverage Effect\n",
    "Negative returns increase volatility MORE than positive returns of the same magnitude.\n",
    "\n",
    "Standard GARCH uses $\\varepsilon_{t-1}^2$ → symmetric response\n",
    "\n",
    "### Solutions:\n",
    "- **EGARCH** (Nelson, 1991)\n",
    "- **GJR-GARCH** (Glosten, Jagannathan, Runkle, 1993)\n",
    "- **TGARCH** (Zakoian, 1994)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-21",
   "metadata": {},
   "outputs": [],
   "source": "# Demonstrate leverage effect with real S&P 500 data\nprint(\"Leverage Effect Analysis - S&P 500\")\nprint(\"=\"*50)\n\n# Calculate rolling volatility\nrolling_vol = returns.rolling(window=5).std()\nlagged_returns = returns.shift(1)\n\n# Create DataFrame for analysis\nleverage_df = pd.DataFrame({\n    'return': returns,\n    'lagged_return': lagged_returns,\n    'next_vol': rolling_vol.shift(-1)\n}).dropna()\n\n# Separate positive and negative returns\nneg_mask = leverage_df['lagged_return'] < 0\npos_mask = leverage_df['lagged_return'] >= 0\n\nvol_after_neg = leverage_df.loc[neg_mask, 'next_vol'].mean()\nvol_after_pos = leverage_df.loc[pos_mask, 'next_vol'].mean()\n\nprint(f\"Mean volatility after negative returns: {vol_after_neg:.3f}%\")\nprint(f\"Mean volatility after positive returns: {vol_after_pos:.3f}%\")\nprint(f\"Ratio: {vol_after_neg/vol_after_pos:.2f}x higher after negative shocks!\")\nprint(\"\\n→ This confirms the leverage effect in real market data!\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-22",
   "metadata": {},
   "outputs": [],
   "source": "# Visualize leverage effect in S&P 500\nfig, axes = plt.subplots(1, 2, figsize=(14, 5))\n\n# Scatter plot: returns vs next-period volatility\ncolors_scatter = [COLORS['red'] if r < 0 else COLORS['blue'] for r in leverage_df['lagged_return']]\naxes[0].scatter(leverage_df['lagged_return'], leverage_df['next_vol'], \n                c=colors_scatter, alpha=0.3, s=10)\naxes[0].axvline(x=0, color='gray', linestyle='--', linewidth=1)\naxes[0].set_xlabel('Return at t (%)')\naxes[0].set_ylabel('Volatility at t+1 (%)')\naxes[0].set_title('S&P 500: Leverage Effect\\nNegative Shocks → Higher Volatility', fontweight='bold')\naxes[0].set_xlim(-12, 12)\n\n# Box plot comparison\nbp = axes[1].boxplot([leverage_df.loc[neg_mask, 'next_vol'].values, \n                      leverage_df.loc[pos_mask, 'next_vol'].values],\n                     labels=['After Negative Return', 'After Positive Return'],\n                     patch_artist=True)\nbp['boxes'][0].set_facecolor(COLORS['red'])\nbp['boxes'][0].set_alpha(0.6)\nbp['boxes'][1].set_facecolor(COLORS['blue'])\nbp['boxes'][1].set_alpha(0.6)\naxes[1].set_ylabel('Next Period Volatility (%)')\naxes[1].set_title('Distribution of Volatility by Return Sign', fontweight='bold')\n\nplt.tight_layout()\nplt.show()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-23",
   "metadata": {},
   "outputs": [],
   "source": "# Estimate asymmetric models on S&P 500\nprint(\"Asymmetric GARCH Models Comparison - S&P 500\")\nprint(\"=\"*60)\n\n# Standard GARCH\nmodel_garch_t = arch_model(returns.values, vol='Garch', p=1, q=1, dist='t')\nres_garch_t = model_garch_t.fit(disp='off')\n\n# GJR-GARCH (o=1 adds asymmetry)\nmodel_gjr = arch_model(returns.values, vol='Garch', p=1, o=1, q=1, dist='t')\nres_gjr = model_gjr.fit(disp='off')\n\n# EGARCH (o=1 is needed for asymmetry term gamma)\nmodel_egarch = arch_model(returns.values, vol='EGARCH', p=1, o=1, q=1, dist='t')\nres_egarch = model_egarch.fit(disp='off')\n\nprint(f\"{'Model':<15} {'AIC':>10} {'BIC':>10} {'LogLik':>12}\")\nprint(\"-\"*50)\nprint(f\"{'GARCH(1,1)':<15} {res_garch_t.aic:>10.2f} {res_garch_t.bic:>10.2f} {res_garch_t.loglikelihood:>12.2f}\")\nprint(f\"{'GJR-GARCH':<15} {res_gjr.aic:>10.2f} {res_gjr.bic:>10.2f} {res_gjr.loglikelihood:>12.2f}\")\nprint(f\"{'EGARCH':<15} {res_egarch.aic:>10.2f} {res_egarch.bic:>10.2f} {res_egarch.loglikelihood:>12.2f}\")\n\n# Best model\nmodels_comparison = {'GARCH': res_garch_t.aic, 'GJR-GARCH': res_gjr.aic, 'EGARCH': res_egarch.aic}\nbest_model = min(models_comparison, key=models_comparison.get)\nprint(f\"\\n→ Best model by AIC: {best_model}\")\nprint(\"→ Asymmetric models fit better because they capture the leverage effect!\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-24",
   "metadata": {},
   "outputs": [],
   "source": "# GJR-GARCH details for S&P 500\nprint(\"GJR-GARCH(1,1,1) Estimation Results - S&P 500\")\nprint(\"=\"*50)\nprint(res_gjr.summary())\n\n# Interpret leverage\ngamma_gjr = res_gjr.params['gamma[1]']\nalpha_gjr = res_gjr.params['alpha[1]']\nprint(f\"\\nLeverage Effect Interpretation:\")\nprint(f\"  α = {alpha_gjr:.4f} (positive shock impact)\")\nprint(f\"  α + γ = {alpha_gjr + gamma_gjr:.4f} (negative shock impact)\")\nprint(f\"  Ratio: {(alpha_gjr + gamma_gjr)/alpha_gjr:.2f}x higher impact for negative shocks\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-25",
   "metadata": {},
   "outputs": [],
   "source": "# EGARCH details for S&P 500\nprint(\"EGARCH(1,1) Estimation Results - S&P 500\")\nprint(\"=\"*50)\nprint(res_egarch.summary())\n\ngamma_egarch = res_egarch.params['gamma[1]']\nprint(\"\\nInterpretation of EGARCH parameters:\")\nprint(f\"  gamma[1] = {gamma_egarch:.4f}\")\nif gamma_egarch < 0:\n    print(\"  → Negative gamma confirms leverage effect!\")\n    print(\"  → Negative shocks increase volatility more than positive shocks\")\n    print(\"  → This is typical for equity markets (bad news has larger impact)\")"
  },
  {
   "cell_type": "markdown",
   "id": "cell-26",
   "metadata": {},
   "source": [
    "## 8. News Impact Curve\n",
    "\n",
    "The **News Impact Curve** shows how today's volatility $\\sigma_{t+1}^2$ responds to yesterday's shock $\\varepsilon_t$, holding $\\sigma_t^2$ constant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot News Impact Curves\n",
    "epsilon_range = np.linspace(-0.04, 0.04, 200)\n",
    "sigma2_prev = 0.0004  # Fixed previous variance\n",
    "\n",
    "# GARCH (symmetric)\n",
    "omega_g = 0.0001\n",
    "alpha_g = 0.10\n",
    "beta_g = 0.85\n",
    "sigma2_garch_curve = omega_g + alpha_g * epsilon_range**2 + beta_g * sigma2_prev\n",
    "\n",
    "# GJR-GARCH\n",
    "omega_gjr = 0.0001\n",
    "alpha_gjr = 0.05\n",
    "gamma_gjr = 0.10\n",
    "beta_gjr = 0.85\n",
    "indicator = (epsilon_range < 0).astype(float)\n",
    "sigma2_gjr_curve = omega_gjr + alpha_gjr * epsilon_range**2 + gamma_gjr * epsilon_range**2 * indicator + beta_gjr * sigma2_prev\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "ax.plot(epsilon_range * 100, np.sqrt(sigma2_garch_curve) * 100,\n",
    "        color=COLORS['blue'], linewidth=2, label='GARCH (Symmetric)')\n",
    "ax.plot(epsilon_range * 100, np.sqrt(sigma2_gjr_curve) * 100,\n",
    "        color=COLORS['red'], linewidth=2, label='GJR-GARCH (Asymmetric)')\n",
    "\n",
    "ax.axvline(x=0, color='gray', linestyle=':', linewidth=1)\n",
    "ax.set_xlabel('Shock εₜ (%)', fontsize=12)\n",
    "ax.set_ylabel('Conditional Volatility σₜ₊₁ (%)', fontsize=12)\n",
    "ax.set_title('News Impact Curve: GARCH vs GJR-GARCH', fontweight='bold', fontsize=14)\n",
    "ax.legend(loc='upper center', bbox_to_anchor=(0.5, -0.12), ncol=2)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"The asymmetric curve shows higher volatility for negative shocks.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-28",
   "metadata": {},
   "source": [
    "## 9. Model Diagnostics\n",
    "\n",
    "After fitting a GARCH model, check:\n",
    "1. **Standardized residuals** $\\hat{z}_t = \\hat{\\varepsilon}_t / \\hat{\\sigma}_t$ should be i.i.d.\n",
    "2. **No remaining ARCH effects** in $\\hat{z}_t^2$\n",
    "3. **No autocorrelation** in $\\hat{z}_t$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-29",
   "metadata": {},
   "outputs": [],
   "source": "# Diagnostics for GJR-GARCH on S&P 500\nstd_resid = res_gjr.std_resid\n\nfig, axes = plt.subplots(2, 2, figsize=(14, 10))\n\n# Time series of standardized residuals\naxes[0, 0].plot(returns.index, std_resid, color=COLORS['blue'], linewidth=0.5, alpha=0.8)\naxes[0, 0].axhline(y=0, color='black', linewidth=0.5)\naxes[0, 0].axhline(y=2, color=COLORS['red'], linestyle='--', linewidth=0.8, alpha=0.5)\naxes[0, 0].axhline(y=-2, color=COLORS['red'], linestyle='--', linewidth=0.8, alpha=0.5)\naxes[0, 0].set_title('Standardized Residuals - S&P 500', fontweight='bold')\naxes[0, 0].set_ylabel('zₜ')\n\n# ACF of squared standardized residuals\nplot_acf(std_resid**2, lags=20, ax=axes[0, 1], color=COLORS['blue'],\n         vlines_kwargs={'color': COLORS['blue']}, alpha=0.05)\naxes[0, 1].set_title('ACF of z²ₜ (should be ≈ 0 if model adequate)', fontweight='bold')\n\n# Histogram\naxes[1, 0].hist(std_resid, bins=60, density=True, color=COLORS['blue'],\n                alpha=0.7, edgecolor='white')\nx = np.linspace(-6, 6, 100)\nnu_est = res_gjr.params['nu']\naxes[1, 0].plot(x, stats.norm.pdf(x), color=COLORS['red'], linewidth=2, label='N(0,1)')\naxes[1, 0].plot(x, stats.t.pdf(x, df=nu_est), color=COLORS['green'], linewidth=2, label=f't({nu_est:.1f})')\naxes[1, 0].set_title('Distribution of Standardized Residuals', fontweight='bold')\naxes[1, 0].set_xlim(-6, 6)\naxes[1, 0].legend(loc='upper center', bbox_to_anchor=(0.5, -0.12), ncol=2)\n\n# QQ-plot\nstats.probplot(std_resid, dist=\"norm\", plot=axes[1, 1])\naxes[1, 1].get_lines()[0].set_color(COLORS['blue'])\naxes[1, 1].get_lines()[0].set_markersize(3)\naxes[1, 1].get_lines()[1].set_color(COLORS['red'])\naxes[1, 1].set_title('QQ-Plot of Standardized Residuals', fontweight='bold')\n\nplt.tight_layout()\nplt.show()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-30",
   "metadata": {},
   "outputs": [],
   "source": "# Formal diagnostic tests for GJR-GARCH on S&P 500\nprint(\"Diagnostic Tests for GJR-GARCH Model - S&P 500\")\nprint(\"=\"*50)\n\n# Ljung-Box on standardized residuals\nlb_z = acorr_ljungbox(std_resid, lags=10, return_df=True)\nprint(\"\\n1. Ljung-Box Test on zₜ (no autocorrelation in mean):\")\nprint(f\"   Lag 10: Q = {lb_z['lb_stat'].iloc[-1]:.2f}, p-value = {lb_z['lb_pvalue'].iloc[-1]:.4f}\")\nprint(f\"   Result: {'✓ No autocorrelation' if lb_z['lb_pvalue'].iloc[-1] > 0.05 else '✗ Autocorrelation present'}\")\n\n# Ljung-Box on squared standardized residuals\nlb_z2 = acorr_ljungbox(std_resid**2, lags=10, return_df=True)\nprint(\"\\n2. Ljung-Box Test on z²ₜ (no remaining ARCH effects):\")\nprint(f\"   Lag 10: Q = {lb_z2['lb_stat'].iloc[-1]:.2f}, p-value = {lb_z2['lb_pvalue'].iloc[-1]:.4f}\")\nprint(f\"   Result: {'✓ No ARCH effects' if lb_z2['lb_pvalue'].iloc[-1] > 0.05 else '✗ ARCH effects remain'}\")\n\n# ARCH-LM test on standardized residuals\nlm_stat, lm_pval, _, _ = het_arch(std_resid, nlags=5)\nprint(\"\\n3. ARCH-LM Test on Standardized Residuals:\")\nprint(f\"   LM = {lm_stat:.2f}, p-value = {lm_pval:.4f}\")\nprint(f\"   Result: {'✓ No remaining ARCH' if lm_pval > 0.05 else '✗ ARCH effects remain'}\")\n\nif lm_pval > 0.05:\n    print(\"\\n→ Model diagnostics PASS - GJR-GARCH adequately captures volatility dynamics!\")"
  },
  {
   "cell_type": "markdown",
   "id": "cell-31",
   "metadata": {},
   "source": [
    "## 10. Volatility Forecasting\n",
    "\n",
    "**One-step forecast:**\n",
    "$$\\hat{\\sigma}_{T+1}^2 = \\omega + \\alpha \\varepsilon_T^2 + \\beta \\sigma_T^2$$\n",
    "\n",
    "**Multi-step forecast:** Converges to unconditional variance\n",
    "$$E_T[\\sigma_{T+h}^2] = \\bar{\\sigma}^2 + (\\alpha + \\beta)^{h-1} (\\sigma_{T+1}^2 - \\bar{\\sigma}^2)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-32",
   "metadata": {},
   "outputs": [],
   "source": "# Volatility forecasting for S&P 500\nhorizon = 20\n\n# Generate forecasts from GJR-GARCH\nforecasts = res_gjr.forecast(horizon=horizon)\nvol_forecast = np.sqrt(forecasts.variance.values[-1, :])\n\n# Get historical volatility\nhist_vol = res_gjr.conditional_volatility\n\n# Unconditional volatility\nparams = res_gjr.params\nomega_hat = params['omega']\nalpha_hat = params['alpha[1]']\ngamma_hat = params.get('gamma[1]', 0)  # Handle case where gamma might not exist\nbeta_hat = params['beta[1]']\npersistence = alpha_hat + gamma_hat/2 + beta_hat\nuncond_vol = np.sqrt(omega_hat / (1 - persistence)) if persistence < 1 else np.nan\n\n# Plot\nfig, ax = plt.subplots(figsize=(14, 5))\n\n# Historical (last 100 periods)\nax.plot(range(100), hist_vol[-100:], color=COLORS['blue'], linewidth=1, label='Historical Volatility')\n\n# Forecast - handle both numpy array and pandas Series\nlast_hist_vol = hist_vol[-1] if isinstance(hist_vol, np.ndarray) else hist_vol.iloc[-1]\nforecast_x = range(99, 99 + horizon + 1)\nforecast_values = np.concatenate([[last_hist_vol], vol_forecast])\nax.plot(forecast_x, forecast_values, color=COLORS['red'], linewidth=2, linestyle='--', label='Forecast')\n\n# Unconditional level\nif not np.isnan(uncond_vol):\n    ax.axhline(y=uncond_vol, color=COLORS['green'], linestyle=':',\n               linewidth=1.5, label=f'Unconditional: {uncond_vol:.2f}%')\n\nax.axvline(x=99, color='black', linestyle='-', alpha=0.3)\nax.set_xlabel('Days')\nax.set_ylabel('Volatility (%)')\nax.set_title('S&P 500: GJR-GARCH Volatility Forecast (20 days ahead)', fontweight='bold')\nax.legend(loc='upper center', bbox_to_anchor=(0.5, -0.12), ncol=3)\n\nplt.tight_layout()\nplt.show()\n\nprint(f\"\\nForecast Summary:\")\nprint(f\"  Last observed date: {returns.index[-1].date()}\")\nprint(f\"  1-day ahead forecast: {vol_forecast[0]:.3f}%\")\nprint(f\"  5-day ahead forecast: {vol_forecast[4]:.3f}%\")\nprint(f\"  20-day ahead forecast: {vol_forecast[-1]:.3f}%\")\nprint(f\"  Unconditional volatility: {uncond_vol:.3f}%\")\nprint(f\"\\n  → Forecast converges to unconditional level as horizon increases\")"
  },
  {
   "cell_type": "markdown",
   "id": "cell-33",
   "metadata": {},
   "source": [
    "## 11. Value at Risk (VaR)\n",
    "\n",
    "**VaR at level $\\alpha$**: Maximum loss that will not be exceeded with probability $1-\\alpha$\n",
    "\n",
    "$$\\text{VaR}_\\alpha = -\\mu_{t+1} + z_\\alpha \\cdot \\sigma_{t+1}$$\n",
    "\n",
    "For normal: $z_{0.05} = 1.645$, $z_{0.01} = 2.326$\n",
    "\n",
    "For Student-t: Use t-quantiles (fatter tails = higher VaR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-34",
   "metadata": {},
   "outputs": [],
   "source": "# Calculate VaR for S&P 500 portfolio\nprint(\"Value at Risk Calculation - S&P 500\")\nprint(\"=\"*50)\n\nportfolio_value = 1_000_000  # EUR\nsigma_1 = vol_forecast[0] / 100  # 1-step ahead volatility (as decimal)\n\n# Normal distribution quantiles\nz_95 = stats.norm.ppf(0.95)\nz_99 = stats.norm.ppf(0.99)\n\nVaR_95_normal = z_95 * sigma_1 * portfolio_value\nVaR_99_normal = z_99 * sigma_1 * portfolio_value\n\nprint(f\"\\nPortfolio value: €{portfolio_value:,.0f}\")\nprint(f\"1-day volatility forecast: {sigma_1*100:.3f}%\")\n\nprint(f\"\\nNormal Distribution VaR:\")\nprint(f\"  VaR 95% (1 day): €{VaR_95_normal:,.0f}\")\nprint(f\"  VaR 99% (1 day): €{VaR_99_normal:,.0f}\")\n\n# Student-t distribution (estimated df from model)\nnu = res_gjr.params.get('nu', 8)  # Default to 8 if not found\n# Adjust quantile for unit variance\nt_95 = stats.t.ppf(0.95, df=nu) * np.sqrt((nu-2)/nu)\nt_99 = stats.t.ppf(0.99, df=nu) * np.sqrt((nu-2)/nu)\n\nVaR_95_t = t_95 * sigma_1 * portfolio_value\nVaR_99_t = t_99 * sigma_1 * portfolio_value\n\nprint(f\"\\nStudent-t Distribution (ν = {nu:.1f}):\")\nprint(f\"  VaR 95% (1 day): €{VaR_95_t:,.0f}\")\nprint(f\"  VaR 99% (1 day): €{VaR_99_t:,.0f}\")\n\n# 10-day VaR (scaling rule)\nprint(f\"\\n10-day VaR (√10 scaling rule):\")\nprint(f\"  Normal 99%: €{VaR_99_normal * np.sqrt(10):,.0f}\")\nprint(f\"  Student-t 99%: €{VaR_99_t * np.sqrt(10):,.0f}\")\n\n# Comparison\nprint(f\"\\n→ Student-t VaR is {(VaR_99_t/VaR_99_normal - 1)*100:.1f}% higher than Normal VaR\")\nprint(\"→ This reflects the fat tails in real financial returns!\")"
  },
  {
   "cell_type": "markdown",
   "id": "cell-35",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "1. **ARIMA assumes constant variance** - not realistic for financial data\n",
    "\n",
    "2. **GARCH(1,1)** is the workhorse model:\n",
    "   - $\\sigma_t^2 = \\omega + \\alpha \\varepsilon_{t-1}^2 + \\beta \\sigma_{t-1}^2$\n",
    "   - α = news reaction, β = persistence\n",
    "   - Stationarity: α + β < 1\n",
    "\n",
    "3. **Leverage effect** requires asymmetric models:\n",
    "   - EGARCH: log-specification, no positivity constraints\n",
    "   - GJR-GARCH: indicator function for negative shocks\n",
    "\n",
    "4. **Student-t distribution** captures fat tails better than normal\n",
    "\n",
    "5. **Applications:**\n",
    "   - VaR and risk management\n",
    "   - Option pricing\n",
    "   - Portfolio optimization\n",
    "\n",
    "### Practical Workflow\n",
    "1. Test for ARCH effects (ARCH-LM test)\n",
    "2. Estimate GARCH(1,1) with Student-t\n",
    "3. Check for asymmetry (GJR/EGARCH)\n",
    "4. Diagnostic checks on standardized residuals\n",
    "5. Forecast volatility, calculate VaR"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}