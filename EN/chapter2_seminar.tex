% Chapter 2: Seminar - ARMA Models
% Quizzes, Practice Problems, and Discussion
% Target: Master Students in Statistics and Data Science

\documentclass[9pt, aspectratio=169, t]{beamer}

% Ensure content fits on slides
\setbeamersize{text margin left=8mm, text margin right=8mm}

%=============================================================================
% THEME AND STYLE CONFIGURATION
%=============================================================================
\usetheme{Madrid}
\usecolortheme{seahorse}

% IDA-Inspired Color Palette
\definecolor{MainBlue}{RGB}{26, 58, 110}
\definecolor{AccentBlue}{RGB}{42, 82, 140}
\definecolor{IDAred}{RGB}{220, 53, 69}
\definecolor{DarkGray}{RGB}{51, 51, 51}
\definecolor{MediumGray}{RGB}{128, 128, 128}
\definecolor{LightGray}{RGB}{248, 248, 248}
\definecolor{VeryLightGray}{RGB}{235, 235, 235}
\definecolor{Crimson}{RGB}{220, 53, 69}
\definecolor{Forest}{RGB}{46, 125, 50}
\definecolor{Amber}{RGB}{181, 133, 63}
\definecolor{Orange}{RGB}{230, 126, 34}

\setbeamercolor{palette primary}{bg=MainBlue, fg=white}
\setbeamercolor{palette secondary}{bg=MainBlue!85, fg=white}
\setbeamercolor{palette tertiary}{bg=MainBlue!70, fg=white}
\setbeamercolor{structure}{fg=MainBlue}
\setbeamercolor{title}{fg=MainBlue}
\setbeamercolor{frametitle}{fg=MainBlue, bg=white}
\setbeamercolor{block title}{bg=MainBlue, fg=white}
\setbeamercolor{block body}{bg=VeryLightGray, fg=DarkGray}
\setbeamercolor{block title alerted}{bg=Crimson, fg=white}
\setbeamercolor{block body alerted}{bg=Crimson!8, fg=DarkGray}
\setbeamercolor{block title example}{bg=Forest, fg=white}
\setbeamercolor{block body example}{bg=Forest!8, fg=DarkGray}
\setbeamercolor{item}{fg=MainBlue}

\setbeamertemplate{navigation symbols}{}

\setbeamertemplate{footline}{
    \leavevmode%
    \hbox{%
        \begin{beamercolorbox}[wd=.333333\paperwidth,ht=2.5ex,dp=1ex,center]{author in head/foot}%
            \usebeamerfont{author in head/foot}\insertshortauthor
        \end{beamercolorbox}%
        \begin{beamercolorbox}[wd=.333333\paperwidth,ht=2.5ex,dp=1ex,center]{title in head/foot}%
            \usebeamerfont{title in head/foot}\insertshorttitle
        \end{beamercolorbox}%
        \begin{beamercolorbox}[wd=.333333\paperwidth,ht=2.5ex,dp=1ex,right]{date in head/foot}%
            \usebeamerfont{date in head/foot}\insertshortdate{}\hspace*{2em}
            \insertframenumber{} / \inserttotalframenumber\hspace*{2ex}
        \end{beamercolorbox}}%
    \vskip0pt%
}

%=============================================================================
% PACKAGES
%=============================================================================
\usepackage{amsmath, amssymb, amsthm}
\usepackage{mathtools}
\usepackage{bm}
\usepackage{tikz}
\usetikzlibrary{arrows.meta, positioning, shapes, calc}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{array}
\usepackage{graphicx}
\usepackage{hyperref}
\hypersetup{colorlinks=false, pdfborder={0 0 0}}
\graphicspath{{logos/}{charts/}}

%=============================================================================
% CUSTOM COMMANDS
%=============================================================================
\newcommand{\E}{\mathbb{E}}
\newcommand{\Var}{\text{Var}}
\newcommand{\Cov}{\text{Cov}}
\newcommand{\Corr}{\text{Corr}}
\newcommand{\R}{\mathbb{R}}

%=============================================================================
% TITLE INFORMATION
%=============================================================================
\title[Chapter 2: Seminar]{Chapter 2: Seminar --- ARMA Models}
\subtitle{Bachelor program Faculty of Cybernetics, Statistics and Economic Informatics, Bucharest University of Economic Studies, Romania}
\author[Prof. dr. Daniel Traian Pele]{Prof. dr. Daniel Traian Pele\\[0.2cm]\footnotesize\texttt{danpele@ase.ro}}
\institute{Bucharest University of Economic Studies}
\date{Academic Year 2025--2026}

\begin{document}

%=============================================================================
% TITLE SLIDE
%=============================================================================
\begin{frame}[plain]
    \begin{tikzpicture}[remember picture, overlay]
        \fill[IDAred] (current page.north west) rectangle ([yshift=-0.15cm]current page.north east);
        \node[anchor=north west] at ([xshift=0.5cm, yshift=-0.3cm]current page.north west) {
            \href{https://www.ase.ro}{\includegraphics[height=1.1cm]{ase_logo.png}}
        };
        \node[anchor=north] at ([yshift=-0.3cm]current page.north) {
            \href{https://ai4efin.ase.ro}{\includegraphics[height=1.1cm]{ai4efin_logo.png}}
        };
        \node[anchor=north east] at ([xshift=-0.5cm, yshift=-0.3cm]current page.north east) {
            \href{https://www.digital-finance-msca.com}{\includegraphics[height=1.1cm]{msca_logo.png}}
        };
    \end{tikzpicture}
    \vfill
    \begin{center}
        {\Large\textcolor{MediumGray}{Time Series Analysis and Forecasting}}\\[0.3cm]
        {\Huge\textbf{\textcolor{MainBlue}{Chapter 2: ARMA Models}}}\\[0.5cm]
        {\Large\textcolor{IDAred}{Seminar}}
    \end{center}
    \vfill

    \begin{tikzpicture}[remember picture, overlay]
        \fill[IDAred] (current page.south west) rectangle ([yshift=0.15cm]current page.south east);
        \node[anchor=south west] at ([xshift=0.5cm, yshift=0.8cm]current page.south west) {
            \href{https://theida.net}{\includegraphics[height=0.9cm]{ida_logo.png}}
        };
        \node[anchor=south] at ([xshift=-3cm, yshift=0.8cm]current page.south) {
            \href{https://blockchain-research-center.com}{\includegraphics[height=0.9cm]{brc_logo.png}}
        };
        \node[anchor=south] at ([yshift=0.8cm]current page.south) {
            \href{https://quantinar.com}{\includegraphics[height=0.9cm]{qr_logo.png}}
        };
        \node[anchor=south] at ([xshift=3cm, yshift=0.8cm]current page.south) {
            \href{https://quantlet.com}{\includegraphics[height=0.9cm]{ql_logo.png}}
        };
        \node[anchor=south east] at ([xshift=-0.5cm, yshift=0.8cm]current page.south east) {
            \href{https://ipe.ro/new}{\includegraphics[height=0.9cm]{acad_logo.png}}
        };
    \end{tikzpicture}
\end{frame}

%=============================================================================
% OUTLINE
%=============================================================================
\begin{frame}{Seminar Outline}
    \tableofcontents
\end{frame}

%=============================================================================
% SECTION 1: MULTIPLE CHOICE QUIZ
%=============================================================================
\section{Multiple Choice Quiz}

\begin{frame}{Quiz 1: Lag Operator}
    \begin{alertblock}{Question}
        What is the result of applying $(1-L)^2$ to $X_t$?
    \end{alertblock}

    \vspace{0.5cm}
    \begin{enumerate}[A.]
        \item $X_t - X_{t-1}$
        \item $X_t - 2X_{t-1} + X_{t-2}$
        \item $X_t + X_{t-1} + X_{t-2}$
        \item $X_t - X_{t-2}$
    \end{enumerate}

    \vspace{0.5cm}
    \begin{center}
        \textit{Answer on next slide...}
    \end{center}
\end{frame}

\begin{frame}{Quiz 1: Solution}
    \begin{exampleblock}{Answer: B -- $X_t - 2X_{t-1} + X_{t-2}$}
        \textbf{Explanation:}
        \begin{align*}
            (1-L)^2 X_t &= (1 - 2L + L^2)X_t \\
            &= X_t - 2LX_t + L^2X_t \\
            &= X_t - 2X_{t-1} + X_{t-2}
        \end{align*}

        This is the \textbf{second difference} of $X_t$.

        \textbf{Note:} $(1-L)$ is the first difference operator, $(1-L)^2$ is the second difference.
    \end{exampleblock}
\end{frame}

\begin{frame}{Quiz 2: AR(1) Stationarity}
    \begin{alertblock}{Question}
        For which value of $\phi$ is the AR(1) process $X_t = 0.5 + \phi X_{t-1} + \varepsilon_t$ stationary?
    \end{alertblock}

    \vspace{0.5cm}
    \begin{enumerate}[A.]
        \item $\phi = 1.2$
        \item $\phi = 1.0$
        \item $\phi = -0.8$
        \item $\phi = -1.5$
    \end{enumerate}

    \vspace{0.5cm}
    \begin{center}
        \textit{Answer on next slide...}
    \end{center}
\end{frame}

\begin{frame}{Quiz 2: Solution}
    \begin{exampleblock}{Answer: C -- $\phi = -0.8$ (Stationary)}
        \vspace{-0.2cm}
        \begin{center}
            \includegraphics[width=0.9\textwidth, height=0.5\textheight, keepaspectratio]{charts/sem2_stationarity_region.pdf}
        \end{center}
        \vspace{-0.2cm}
        {\footnotesize
        \textbf{AR(1) stationarity}: $|\phi| < 1$ (root outside unit circle). Only C satisfies: $|-0.8| = 0.8 < 1$
        }
    \end{exampleblock}
\end{frame}

\begin{frame}{Visual: AR(1) Process Behavior}
    \begin{center}
        \includegraphics[width=0.95\textwidth]{charts/ch2_def_ar1.pdf}
    \end{center}
    \vspace{-0.2cm}
    \small Positive $\phi$: persistent, smooth patterns. Negative $\phi$: oscillating behavior around mean.
\end{frame}

\begin{frame}{Quiz 3: ACF Pattern}
    \begin{alertblock}{Question}
        You observe the following ACF pattern: significant spike at lag 1, then all other lags are within confidence bands. The PACF shows gradual decay. What model is suggested?
    \end{alertblock}

    \vspace{0.5cm}
    \begin{enumerate}[A.]
        \item AR(1)
        \item MA(1)
        \item ARMA(1,1)
        \item White noise
    \end{enumerate}

    \vspace{0.5cm}
    \begin{center}
        \textit{Answer on next slide...}
    \end{center}
\end{frame}

\begin{frame}{Quiz 3: Solution}
    \begin{exampleblock}{Answer: B -- MA(1)}
        \vspace{-0.2cm}
        \begin{center}
            \includegraphics[width=0.95\textwidth, height=0.5\textheight, keepaspectratio]{charts/sem2_acf_pacf_patterns.pdf}
        \end{center}
        \vspace{-0.2cm}
        {\footnotesize
        \textbf{Pattern}: ACF cuts off after lag 1 $\Rightarrow$ MA(1); PACF decays $\Rightarrow$ confirms MA structure (not AR)
        }
    \end{exampleblock}
\end{frame}

\begin{frame}{Visual: MA(1) Process and ACF}
    \begin{center}
        \includegraphics[width=0.95\textwidth]{charts/ch2_def_ma1.pdf}
    \end{center}
    \vspace{-0.2cm}
    \small MA(1) process (left). Key signature: ACF cuts off sharply after lag 1 (right).
\end{frame}

\begin{frame}{Quiz 4: MA Invertibility}
    \begin{alertblock}{Question}
        For the MA(1) process $X_t = \varepsilon_t + 1.5\varepsilon_{t-1}$, is the process invertible?
    \end{alertblock}

    \vspace{0.5cm}
    \begin{enumerate}[A.]
        \item Yes, because MA processes are always invertible
        \item Yes, because $1.5 > 0$
        \item No, because $|\theta| = 1.5 > 1$
        \item No, because MA processes are never invertible
    \end{enumerate}

    \vspace{0.5cm}
    \begin{center}
        \textit{Answer on next slide...}
    \end{center}
\end{frame}

\begin{frame}{Quiz 4: Solution}
    \begin{exampleblock}{Answer: C -- Not invertible ($|\theta| = 1.5 > 1$)}
        \vspace{-0.2cm}
        \begin{center}
            \includegraphics[width=0.9\textwidth, height=0.5\textheight, keepaspectratio]{charts/sem2_invertibility.pdf}
        \end{center}
        \vspace{-0.2cm}
        {\footnotesize
        \textbf{MA invertibility}: Root $z = -1/\theta$ must be outside unit circle $\Leftrightarrow |\theta| < 1$. Here $z = -0.67$ is inside!
        }
    \end{exampleblock}
\end{frame}

\begin{frame}{Visual: Invertibility Concept}
    \begin{center}
        \includegraphics[width=0.95\textwidth]{charts/ch2_def_invertibility.pdf}
    \end{center}
    \vspace{-0.2cm}
    \small Left: invertibility requires roots outside unit circle. Right: AR($\infty$) weights decay only when $|\theta| < 1$.
\end{frame}

\begin{frame}{Quiz 5: ARMA Representation}
    \begin{alertblock}{Question}
        The compact form $\phi(L)X_t = \theta(L)\varepsilon_t$ represents which model?
    \end{alertblock}

    \vspace{0.5cm}
    \begin{enumerate}[A.]
        \item Pure AR model
        \item Pure MA model
        \item ARMA model
        \item None of the above
    \end{enumerate}

    \vspace{0.5cm}
    \begin{center}
        \textit{Answer on next slide...}
    \end{center}
\end{frame}

\begin{frame}{Quiz 5: Solution}
    \begin{exampleblock}{Answer: C -- ARMA model}
        \begin{itemize}
            \item $\phi(L) = 1 - \phi_1 L - \cdots - \phi_p L^p$ is the AR polynomial
            \item $\theta(L) = 1 + \theta_1 L + \cdots + \theta_q L^q$ is the MA polynomial
        \end{itemize}

        The equation $\phi(L)X_t = \theta(L)\varepsilon_t$ expands to:
        $$X_t - \phi_1 X_{t-1} - \cdots - \phi_p X_{t-p} = \varepsilon_t + \theta_1\varepsilon_{t-1} + \cdots + \theta_q\varepsilon_{t-q}$$

        This is the general \textbf{ARMA(p,q)} model.

        \textbf{Special cases:} $\theta(L) = 1$ (no MA): Pure AR; $\phi(L) = 1$ (no AR): Pure MA
    \end{exampleblock}
\end{frame}

\begin{frame}{Visual: ARMA Process}
    \begin{center}
        \includegraphics[width=0.95\textwidth]{charts/ch2_def_arma.pdf}
    \end{center}
    \vspace{-0.2cm}
    \small ARMA(1,1) combines AR and MA components. ACF shows decay after initial lag.
\end{frame}

\begin{frame}{Quiz 6: Information Criteria}
    \begin{alertblock}{Question}
        When comparing ARMA(1,1) and ARMA(2,1) using BIC, which statement is correct?
    \end{alertblock}

    \vspace{0.5cm}
    \begin{enumerate}[A.]
        \item Lower BIC always means better forecasts
        \item BIC penalizes complexity less than AIC
        \item The model with lower BIC is preferred
        \item BIC can only compare models with same number of parameters
    \end{enumerate}

    \vspace{0.5cm}
    \begin{center}
        \textit{Answer on next slide...}
    \end{center}
\end{frame}

\begin{frame}{Quiz 6: Solution}
    \begin{exampleblock}{Answer: C -- Lower BIC is preferred}
        \vspace{-0.2cm}
        \begin{center}
            \includegraphics[width=0.9\textwidth, height=0.5\textheight, keepaspectratio]{charts/sem2_information_criteria.pdf}
        \end{center}
        \vspace{-0.2cm}
        {\footnotesize
        \textbf{AIC}: $-2\ln(\hat{L}) + 2k$ \quad \textbf{BIC}: $-2\ln(\hat{L}) + k\ln(n)$ \quad BIC penalizes complexity more $\Rightarrow$ more parsimonious models
        }
    \end{exampleblock}
\end{frame}

\begin{frame}{Quiz 7: Ljung-Box Test}
    \begin{alertblock}{Question}
        After fitting an ARMA(2,1) model, you run the Ljung-Box test on residuals and get p-value = 0.02. What do you conclude?
    \end{alertblock}

    \vspace{0.5cm}
    \begin{enumerate}[A.]
        \item The model is adequate
        \item Residuals are white noise
        \item There is significant autocorrelation in residuals
        \item The model has too many parameters
    \end{enumerate}

    \vspace{0.5cm}
    \begin{center}
        \textit{Answer on next slide...}
    \end{center}
\end{frame}

\begin{frame}{Quiz 7: Solution}
    \begin{exampleblock}{Answer: C -- There is significant autocorrelation in residuals}
        The Ljung-Box test has:
        \begin{itemize}
            \item $H_0$: Residuals are white noise (no autocorrelation)
            \item $H_1$: Residuals have significant autocorrelation
        \end{itemize}

        With p-value = 0.02 $<$ 0.05:
        \begin{itemize}
            \item We \textbf{reject} $H_0$
            \item Conclusion: residuals are \textbf{not} white noise
            \item The model is \textbf{inadequate} --- significant structure remains
        \end{itemize}

        \textbf{Next step:} Try a different model (e.g., increase $p$ or $q$)
    \end{exampleblock}
\end{frame}

\begin{frame}{Quiz 8: Forecasting}
    \begin{alertblock}{Question}
        For an AR(1) model with $\phi = 0.6$ and mean $\mu = 10$, what happens to forecasts as horizon $h \to \infty$?
    \end{alertblock}

    \vspace{0.5cm}
    \begin{enumerate}[A.]
        \item Forecasts grow without bound
        \item Forecasts converge to 0
        \item Forecasts converge to $\mu = 10$
        \item Forecasts oscillate forever
    \end{enumerate}

    \vspace{0.5cm}
    \begin{center}
        \textit{Answer on next slide...}
    \end{center}
\end{frame}

\begin{frame}{Quiz 8: Solution}
    \begin{exampleblock}{Answer: C -- Forecasts converge to $\mu = 10$}
        For AR(1), the $h$-step ahead forecast is:
        $$\hat{X}_{n+h|n} = \mu + \phi^h(X_n - \mu)$$

        Since $|\phi| = 0.6 < 1$:
        $$\lim_{h \to \infty} \phi^h = 0$$

        Therefore:
        $$\lim_{h \to \infty} \hat{X}_{n+h|n} = \mu + 0 \cdot (X_n - \mu) = \mu = 10$$

        \textbf{Key insight:} Long-run forecasts from stationary ARMA models always converge to the unconditional mean.
    \end{exampleblock}
\end{frame}

\begin{frame}{Quiz 9: AR(2) Roots}
    \begin{alertblock}{Question}
        An AR(2) process has characteristic roots $z_1 = 0.8$ and $z_2 = -0.5$. Is it stationary?
    \end{alertblock}

    \vspace{0.5cm}
    \begin{enumerate}[A.]
        \item Yes, because both roots are inside the unit circle
        \item No, because one root is negative
        \item No, because roots must be outside the unit circle
        \item Cannot determine without more information
    \end{enumerate}

    \vspace{0.5cm}
    \begin{center}
        \textit{Answer on next slide...}
    \end{center}
\end{frame}

\begin{frame}{Quiz 9: Solution}
    \begin{exampleblock}{Answer: C -- Roots must be outside the unit circle}
        For AR stationarity, roots of $\phi(z) = 0$ must lie \textbf{outside} the unit circle, i.e., $|z| > 1$.

        Here: $|z_1| = 0.8 < 1$ and $|z_2| = 0.5 < 1$ -- both \textbf{inside} unit circle.

        $\rightarrow$ \textcolor{Crimson}{\textbf{Non-stationary}} (actually explosive)

        \textbf{Note:} Equivalent condition: coefficients $\phi_1, \phi_2$ must satisfy stationarity triangle.
    \end{exampleblock}
\end{frame}

\begin{frame}{Quiz 10: MA(q) Properties}
    \begin{alertblock}{Question}
        For an MA(2) process, the ACF:
    \end{alertblock}

    \vspace{0.5cm}
    \begin{enumerate}[A.]
        \item Decays exponentially
        \item Cuts off after lag 2
        \item Cuts off after lag 1
        \item Never cuts off
    \end{enumerate}

    \vspace{0.5cm}
    \begin{center}
        \textit{Answer on next slide...}
    \end{center}
\end{frame}

\begin{frame}{Quiz 10: Solution}
    \begin{exampleblock}{Answer: B -- Cuts off after lag 2}
        For MA($q$), the ACF is exactly zero for lags $> q$.

        \begin{itemize}
            \item MA(1): ACF cuts off after lag 1
            \item MA(2): ACF cuts off after lag 2
            \item MA($q$): ACF cuts off after lag $q$
        \end{itemize}

        This is the key identification feature: ACF cutoff $\Rightarrow$ MA order.

        Meanwhile, PACF of MA processes decays (doesn't cut off).
    \end{exampleblock}
\end{frame}

\begin{frame}{Quiz 11: ARMA Parsimony}
    \begin{alertblock}{Question}
        Why might ARMA(1,1) be preferred over AR(5) even if both fit equally well?
    \end{alertblock}

    \vspace{0.5cm}
    \begin{enumerate}[A.]
        \item ARMA models are always better
        \item Fewer parameters reduce overfitting risk
        \item AR models cannot capture trends
        \item MA components are more stable
    \end{enumerate}

    \vspace{0.5cm}
    \begin{center}
        \textit{Answer on next slide...}
    \end{center}
\end{frame}

\begin{frame}{Quiz 11: Solution}
    \begin{exampleblock}{Answer: B -- Fewer parameters reduce overfitting risk}
        \textbf{Parsimony principle}: prefer simpler models.

        \begin{itemize}
            \item ARMA(1,1): 2 parameters ($\phi_1$, $\theta_1$)
            \item AR(5): 5 parameters ($\phi_1, \ldots, \phi_5$)
        \end{itemize}

        Fewer parameters means:
        \begin{itemize}
            \item Less risk of overfitting
            \item Better out-of-sample forecasts
            \item More interpretable model
        \end{itemize}

        BIC penalizes complexity more than AIC, often selecting more parsimonious models.
    \end{exampleblock}
\end{frame}

\begin{frame}{Quiz 12: Residual Diagnostics}
    \begin{alertblock}{Question}
        After fitting an ARMA model, the residual ACF shows a significant spike at lag 5. This suggests:
    \end{alertblock}

    \vspace{0.5cm}
    \begin{enumerate}[A.]
        \item The model is adequate
        \item The model may need higher order terms
        \item Residuals are white noise
        \item The data is non-stationary
    \end{enumerate}

    \vspace{0.5cm}
    \begin{center}
        \textit{Answer on next slide...}
    \end{center}
\end{frame}

\begin{frame}{Quiz 12: Solution}
    \begin{exampleblock}{Answer: B -- The model may need higher order terms}
        Good residuals should be white noise with no significant ACF.

        A significant spike at lag 5 indicates remaining autocorrelation structure not captured by the model.

        \textbf{Actions:}
        \begin{itemize}
            \item Consider adding AR or MA terms
            \item Check if AR(5) or MA(5) component helps
            \item Re-run Ljung-Box test after modification
        \end{itemize}
    \end{exampleblock}
\end{frame}

\begin{frame}{Quiz 13: Wold Decomposition}
    \begin{alertblock}{Question}
        The Wold decomposition theorem states that any stationary process can be written as:
    \end{alertblock}

    \vspace{0.5cm}
    \begin{enumerate}[A.]
        \item A finite AR process
        \item A finite MA process
        \item An infinite MA process plus a deterministic component
        \item An ARIMA process
    \end{enumerate}

    \vspace{0.5cm}
    \begin{center}
        \textit{Answer on next slide...}
    \end{center}
\end{frame}

\begin{frame}{Quiz 13: Solution}
    \begin{exampleblock}{Answer: C -- An infinite MA process plus a deterministic component}
        Wold's theorem: Any stationary process can be written as:
        $$X_t = \sum_{j=0}^{\infty} \psi_j \varepsilon_{t-j} + \eta_t$$
        where $\eta_t$ is deterministic and $\sum \psi_j^2 < \infty$.

        \textbf{Implication:} MA($\infty$) is the most general representation. ARMA models are parsimonious approximations to this infinite MA.
    \end{exampleblock}
\end{frame}

\begin{frame}{Quiz 14: Unit Root vs Trend Stationary}
    \begin{alertblock}{Question}
        How do you make a unit root process stationary?
    \end{alertblock}

    \vspace{0.5cm}
    \begin{enumerate}[A.]
        \item Subtract a linear trend
        \item Take first differences
        \item Apply moving average
        \item Use seasonal adjustment
    \end{enumerate}

    \vspace{0.5cm}
    \begin{center}
        \textit{Answer on next slide...}
    \end{center}
\end{frame}

\begin{frame}{Quiz 14: Solution}
    \begin{exampleblock}{Answer: B -- Take first differences}
        \begin{itemize}
            \item \textbf{Unit root} (stochastic trend): Use \textbf{differencing}
            \item \textbf{Trend stationary} (deterministic trend): Use \textbf{detrending} (regression)
        \end{itemize}

        For random walk $X_t = X_{t-1} + \varepsilon_t$:
        $$\Delta X_t = X_t - X_{t-1} = \varepsilon_t$$
        which is stationary white noise.
    \end{exampleblock}
\end{frame}

%=============================================================================
% SECTION 2: TRUE/FALSE
%=============================================================================
\section{True/False Questions}

\begin{frame}{True/False Questions}
    \begin{alertblock}{Question}
        Determine if each statement is True or False:
    \end{alertblock}

    \vspace{0.3cm}
    \begin{enumerate}
        \item An AR(2) process can exhibit pseudo-cyclical behavior.
        \item MA processes require a stationarity condition.
        \item The PACF of an AR(p) process cuts off after lag $p$.
        \item If AIC selects ARMA(2,1) and BIC selects ARMA(1,1), they cannot both be correct.
        \item Forecast confidence intervals narrow as the forecast horizon increases.
        \item The Yule-Walker equations can be used to estimate MA parameters.
    \end{enumerate}

    \vspace{0.3cm}
    \begin{center}
        \textit{Answer on next slide...}
    \end{center}
\end{frame}

\begin{frame}{True/False: Solutions}
    \begin{exampleblock}{Answers}
    \begin{enumerate}
        \item \textcolor{Forest}{\textbf{TRUE}}: AR(2) with complex roots shows damped oscillations
        \item \textcolor{Crimson}{\textbf{FALSE}}: MA processes are always stationary; they need \textit{invertibility} condition
        \item \textcolor{Forest}{\textbf{TRUE}}: This is the key identification feature of AR(p)
        \item \textcolor{Crimson}{\textbf{FALSE}}: Both can be ``correct'' --- they optimize different criteria (AIC favors fit, BIC favors parsimony)
        \item \textcolor{Crimson}{\textbf{FALSE}}: Confidence intervals \textit{widen} as horizon increases (more uncertainty)
        \item \textcolor{Crimson}{\textbf{FALSE}}: Yule-Walker is for AR models only; MA uses MLE
    \end{enumerate}
    \end{exampleblock}
\end{frame}

%=============================================================================
% SECTION 3: CALCULATION EXERCISES
%=============================================================================
\section{Calculation Exercises}

\begin{frame}{Exercise 1: AR(1) Properties}
    \textbf{Problem:} Consider the AR(1) process:
    $$X_t = 2 + 0.7 X_{t-1} + \varepsilon_t, \quad \varepsilon_t \sim WN(0, 9)$$

    Calculate:
    \begin{enumerate}
        \item The mean $\mu$
        \item The variance $\gamma(0)$
        \item The autocovariance $\gamma(1)$ and $\gamma(2)$
        \item The autocorrelation $\rho(1)$ and $\rho(2)$
    \end{enumerate}
\end{frame}

\begin{frame}{Exercise 1: Solution}
    Given: $c = 2$, $\phi = 0.7$, $\sigma^2 = 9$

    \vspace{0.3cm}
    \textbf{1. Mean:}
    $$\mu = \frac{c}{1-\phi} = \frac{2}{1-0.7} = \frac{2}{0.3} = 6.67$$

    \textbf{2. Variance:}
    $$\gamma(0) = \frac{\sigma^2}{1-\phi^2} = \frac{9}{1-0.49} = \frac{9}{0.51} = 17.65$$

    \textbf{3. Autocovariance:}
    $$\gamma(1) = \phi \cdot \gamma(0) = 0.7 \times 17.65 = 12.35$$
    $$\gamma(2) = \phi^2 \cdot \gamma(0) = 0.49 \times 17.65 = 8.65$$

    \textbf{4. Autocorrelation:}
    $$\rho(1) = \phi = 0.7, \quad \rho(2) = \phi^2 = 0.49$$
\end{frame}

\begin{frame}{Exercise 2: MA(1) Properties}
    \textbf{Problem:} Consider the MA(1) process:
    $$X_t = 5 + \varepsilon_t - 0.4\varepsilon_{t-1}, \quad \varepsilon_t \sim WN(0, 4)$$

    Calculate:
    \begin{enumerate}
        \item The mean $\mu$
        \item The variance $\gamma(0)$
        \item The autocovariance $\gamma(1)$
        \item The autocorrelation $\rho(1)$
        \item Is this process invertible?
    \end{enumerate}
\end{frame}

\begin{frame}{Exercise 2: Solution}
    Given: $\mu = 5$, $\theta = -0.4$, $\sigma^2 = 4$

    \vspace{0.3cm}
    \textbf{1. Mean:}
    $$\E[X_t] = \mu = 5$$

    \textbf{2. Variance:}
    $$\gamma(0) = \sigma^2(1 + \theta^2) = 4(1 + 0.16) = 4 \times 1.16 = 4.64$$

    \textbf{3. Autocovariance at lag 1:}
    $$\gamma(1) = \theta\sigma^2 = -0.4 \times 4 = -1.6$$

    \textbf{4. Autocorrelation:}
    $$\rho(1) = \frac{\gamma(1)}{\gamma(0)} = \frac{-1.6}{4.64} = -0.345$$

    \textbf{5. Invertibility:} $|\theta| = 0.4 < 1$ $\rightarrow$ \textcolor{Forest}{\textbf{Yes, invertible}}
\end{frame}

\begin{frame}{Exercise 3: Characteristic Roots}
    \textbf{Problem:} Consider the AR(2) process:
    $$X_t = 0.5X_{t-1} + 0.3X_{t-2} + \varepsilon_t$$

    \begin{enumerate}
        \item Write the characteristic equation
        \item Find the characteristic roots
        \item Is this process stationary?
    \end{enumerate}
\end{frame}

\begin{frame}{Exercise 3: Solution}
    \textbf{1. Characteristic equation:}
    $$\phi(z) = 1 - \phi_1 z - \phi_2 z^2 = 1 - 0.5z - 0.3z^2 = 0$$

    Or: $0.3z^2 + 0.5z - 1 = 0$

    \vspace{0.3cm}
    \textbf{2. Roots (using quadratic formula):}
    $$z = \frac{-0.5 \pm \sqrt{0.25 + 1.2}}{0.6} = \frac{-0.5 \pm 1.204}{0.6}$$

    $$z_1 = \frac{0.704}{0.6} = 1.17, \quad z_2 = \frac{-1.704}{0.6} = -2.84$$

    \vspace{0.3cm}
    \textbf{3. Stationarity check:}

    Both roots have $|z| > 1$: $|z_1| = 1.17 > 1$ and $|z_2| = 2.84 > 1$

    $\rightarrow$ \textcolor{Forest}{\textbf{Stationary}} (roots outside unit circle)
\end{frame}

\begin{frame}{Exercise 4: Forecasting}
    \textbf{Problem:} You have fit an AR(1) model:
    $$X_t = 3 + 0.8X_{t-1} + \varepsilon_t, \quad \sigma^2 = 4$$

    Given $X_{100} = 20$, calculate:
    \begin{enumerate}
        \item The 1-step ahead forecast $\hat{X}_{101|100}$
        \item The 2-step ahead forecast $\hat{X}_{102|100}$
        \item The long-run forecast $\hat{X}_{100+h|100}$ as $h \to \infty$
        \item The 95\% confidence interval for $\hat{X}_{101|100}$
    \end{enumerate}
\end{frame}

\begin{frame}{Exercise 4: Solution}
    Given: $c = 3$, $\phi = 0.8$, $\sigma^2 = 4$, $X_{100} = 20$

    \textbf{Mean:} $\mu = \frac{3}{1-0.8} = 15$

    \vspace{0.3cm}
    \textbf{1. One-step forecast:}
    $$\hat{X}_{101|100} = c + \phi X_{100} = 3 + 0.8 \times 20 = 19$$

    \textbf{2. Two-step forecast:}
    $$\hat{X}_{102|100} = c + \phi \hat{X}_{101|100} = 3 + 0.8 \times 19 = 18.2$$

    \textbf{3. Long-run forecast:}
    $$\lim_{h \to \infty} \hat{X}_{100+h|100} = \mu = 15$$

    \textbf{4. 95\% CI for 1-step:}
    $$\text{MSFE}(1) = \sigma^2 = 4, \quad \sqrt{\text{MSFE}(1)} = 2$$
    $$CI: 19 \pm 1.96 \times 2 = [15.08, 22.92]$$
\end{frame}

%=============================================================================
% SECTION 4: PYTHON EXERCISES
%=============================================================================
\section{Python Exercises}

\begin{frame}{Python Exercise 1: Simulate and Fit AR(1)}
    \textbf{Task:}
    \begin{enumerate}
        \item Simulate 500 observations from AR(1) with $\phi = 0.7$
        \item Plot the series and ACF/PACF
        \item Fit an AR(1) model and check if $\hat{\phi} \approx 0.7$
        \item Examine residual diagnostics
    \end{enumerate}

    \vspace{0.3cm}
    \textbf{Hint code:}

    \texttt{np.random.seed(42)}

    \texttt{n = 500}

    \texttt{phi = 0.7}

    \texttt{x = np.zeros(n)}

    \texttt{for t in range(1, n):}

    \texttt{\quad x[t] = phi * x[t-1] + np.random.randn()}
\end{frame}

\begin{frame}{Python Exercise 2: Model Selection}
    \textbf{Task:}
    \begin{enumerate}
        \item Load a real time series (e.g., stock returns)
        \item Check stationarity using ADF test
        \item Compare AIC/BIC for ARMA(1,0), ARMA(0,1), ARMA(1,1), ARMA(2,1)
        \item Select the best model
        \item Generate forecasts with confidence intervals
    \end{enumerate}

    \vspace{0.3cm}
    \textbf{Key functions:}
    \begin{itemize}
        \item \texttt{adfuller()} for stationarity test
        \item \texttt{ARIMA(data, order=(p,0,q)).fit()} for fitting
        \item \texttt{results.aic}, \texttt{results.bic} for criteria
        \item \texttt{results.get\_forecast(h)} for predictions
    \end{itemize}
\end{frame}

\begin{frame}{Python Exercise 3: Diagnostic Checking}
    \textbf{Task:} After fitting a model, perform complete diagnostics:
    \begin{enumerate}
        \item Plot residuals over time
        \item Plot ACF of residuals
        \item Create Q-Q plot
        \item Run Ljung-Box test
        \item Check if AR/MA roots are outside unit circle
    \end{enumerate}

    \vspace{0.3cm}
    \textbf{Key functions:}
    \begin{itemize}
        \item \texttt{results.resid} for residuals
        \item \texttt{plot\_acf(resid)} for ACF plot
        \item \texttt{stats.probplot(resid)} for Q-Q plot
        \item \texttt{acorr\_ljungbox(resid)} for portmanteau test
        \item \texttt{results.arroots}, \texttt{results.maroots} for roots
    \end{itemize}
\end{frame}

%=============================================================================
% SECTION 5: REAL DATA ANALYSIS
%=============================================================================
\section{Real Data Analysis}

\begin{frame}{Case Study: Industrial Production Index}
    \vspace{-0.3cm}
    \begin{center}
        \includegraphics[width=0.85\textwidth, height=0.55\textheight, keepaspectratio]{charts/arma_examples.pdf}
    \end{center}
    \vspace{-0.2cm}
    {\footnotesize
    \begin{itemize}
        \item US Industrial Production: monthly data, already stationary (growth rates)
        \item Shows typical ARMA patterns: mean-reverting with short-term dependence
        \item Volatility clustering visible -- ARMA captures the conditional mean
        \item Suitable for ARMA modeling without differencing
    \end{itemize}
    }
\end{frame}

\begin{frame}{ACF/PACF Pattern Recognition}
    \vspace{-0.3cm}
    \begin{center}
        \includegraphics[width=0.85\textwidth, height=0.55\textheight, keepaspectratio]{charts/acf_pacf_examples.pdf}
    \end{center}
    \vspace{-0.2cm}
    {\footnotesize
    \begin{itemize}
        \item ACF shows gradual decay -- suggests AR component
        \item PACF cuts off after lag 2 -- suggests AR(2) might be appropriate
        \item Some significant lags in ACF beyond lag 2 -- MA terms may help
        \item Pattern consistent with ARMA(2,1) or similar low-order models
    \end{itemize}
    }
\end{frame}

\begin{frame}{ARMA Estimation Results}
    {\small
    \begin{block}{Model: ARMA(2,1) for Industrial Production Growth}
        \begin{center}
        \begin{tabular}{lcccc}
            \toprule
            \textbf{Parameter} & \textbf{Estimate} & \textbf{Std. Error} & \textbf{z-stat} & \textbf{p-value} \\
            \midrule
            $c$ (const) & $0.156$ & $0.048$ & $3.25$ & $0.001$ \\
            $\phi_1$ (AR.L1) & $0.423$ & $0.089$ & $4.75$ & $<0.001$ \\
            $\phi_2$ (AR.L2) & $0.187$ & $0.072$ & $2.60$ & $0.009$ \\
            $\theta_1$ (MA.L1) & $-0.156$ & $0.091$ & $-1.71$ & $0.087$ \\
            \bottomrule
        \end{tabular}
        \end{center}
    \end{block}

    \vspace{0.2cm}

    \begin{exampleblock}{Model Selection}
        AIC: $-412.5$, BIC: $-398.2$. Model passes stationarity and invertibility checks.
    \end{exampleblock}
    }
\end{frame}

\begin{frame}{Forecast Performance}
    \vspace{-0.3cm}
    \begin{center}
        \includegraphics[width=0.85\textwidth, height=0.55\textheight, keepaspectratio]{charts/arma_forecast.pdf}
    \end{center}
    \vspace{-0.2cm}
    {\footnotesize
    \begin{itemize}
        \item ARMA forecasts mean-revert toward unconditional mean
        \item Short-term forecasts capture recent dynamics
        \item Confidence intervals expand with forecast horizon
        \item Comparison with naive forecast shows ARMA improvement
    \end{itemize}
    }
\end{frame}

\begin{frame}{Model Diagnostics}
    \vspace{-0.3cm}
    \begin{center}
        \includegraphics[width=0.85\textwidth, height=0.55\textheight, keepaspectratio]{charts/arma_residual_diagnostics.pdf}
    \end{center}
    \vspace{-0.2cm}
    {\footnotesize
    \begin{itemize}
        \item Residuals appear random with no systematic patterns
        \item ACF of residuals within confidence bounds
        \item Q-Q plot shows approximate normality
        \item Ljung-Box test: $p > 0.05$ -- no significant autocorrelation in residuals
    \end{itemize}
    }
\end{frame}

%=============================================================================
% SECTION 6: DISCUSSION
%=============================================================================
\section{Discussion Questions}

\begin{frame}{Discussion 1: Model Selection}
    \textbf{Scenario:} You're modeling monthly inflation rates. After checking stationarity (passed), you find:
    \begin{itemize}
        \item ACF: significant at lags 1, 2, 3, then decays
        \item PACF: significant at lags 1, 2, then cuts off
        \item AIC selects ARMA(2,3)
        \item BIC selects ARMA(2,0) = AR(2)
    \end{itemize}

    \vspace{0.3cm}
    \textbf{Questions:}
    \begin{enumerate}
        \item What does the ACF/PACF pattern suggest?
        \item Why do AIC and BIC disagree?
        \item Which model would you choose and why?
        \item What additional checks would you perform?
    \end{enumerate}
\end{frame}

\begin{frame}{Discussion 2: Forecast Evaluation}
    \textbf{Scenario:} You fit an ARMA(1,1) model to daily stock returns. The in-sample fit looks good (Ljung-Box p-value = 0.45), but out-of-sample RMSE is worse than a simple random walk forecast.

    \vspace{0.3cm}
    \textbf{Questions:}
    \begin{enumerate}
        \item Is this surprising? Why or why not?
        \item What does this tell us about stock return predictability?
        \item Should you conclude the ARMA model is useless?
        \item What alternatives might you consider?
    \end{enumerate}

    \vspace{0.3cm}
    \textbf{Hint:} Think about the Efficient Market Hypothesis and what ARMA captures vs. what it doesn't (e.g., volatility clustering).
\end{frame}

\begin{frame}{Discussion 3: Real-World Application}
    \textbf{Scenario:} A central bank economist asks you to forecast quarterly GDP growth for policy planning.

    \vspace{0.3cm}
    \textbf{Questions:}
    \begin{enumerate}
        \item What preliminary analysis would you do before fitting ARMA?
        \item GDP is often non-stationary --- how would you handle this?
        \item Would you use AIC or BIC for model selection? Why?
        \item How would you communicate forecast uncertainty to policymakers?
        \item What limitations of ARMA models should you mention?
    \end{enumerate}
\end{frame}

%=============================================================================
% SUMMARY
%=============================================================================
\section{Summary}

\begin{frame}{Key Takeaways from Today's Seminar}
    \begin{enumerate}
        \item \textbf{AR models:} Current value depends on past values
        \begin{itemize}
            \item Stationarity: $|\phi| < 1$ for AR(1)
            \item PACF cuts off at lag $p$
        \end{itemize}

        \item \textbf{MA models:} Current value depends on past shocks
        \begin{itemize}
            \item Always stationary; invertibility: $|\theta| < 1$ for MA(1)
            \item ACF cuts off at lag $q$
        \end{itemize}

        \item \textbf{Model selection:} Use ACF/PACF patterns + information criteria

        \item \textbf{Diagnostics:} Residuals must be white noise (Ljung-Box test)

        \item \textbf{Forecasting:} Point forecasts converge to mean; uncertainty grows
    \end{enumerate}

    \vspace{0.3cm}
    \textbf{Next Seminar:} ARIMA and Seasonal Models
\end{frame}

\end{document}
