% Chapter 2: Seminar - ARMA Models
% Quizzes, Practice Problems, and Discussion
% Target: Master Students in Statistics and Data Science

\documentclass[9pt, aspectratio=169, t]{beamer}

% Ensure content fits on slides
\setbeamersize{text margin left=8mm, text margin right=8mm}

%=============================================================================
% THEME AND STYLE CONFIGURATION
%=============================================================================
\usetheme{Madrid}
\usecolortheme{seahorse}

% IDA-Inspired Color Palette
\definecolor{MainBlue}{RGB}{26, 58, 110}
\definecolor{AccentBlue}{RGB}{42, 82, 140}
\definecolor{IDAred}{RGB}{220, 53, 69}
\definecolor{DarkGray}{RGB}{51, 51, 51}
\definecolor{MediumGray}{RGB}{128, 128, 128}
\definecolor{LightGray}{RGB}{248, 248, 248}
\definecolor{VeryLightGray}{RGB}{235, 235, 235}
\definecolor{Crimson}{RGB}{220, 53, 69}
\definecolor{Forest}{RGB}{46, 125, 50}
\definecolor{Amber}{RGB}{181, 133, 63}
\definecolor{Orange}{RGB}{230, 126, 34}

\setbeamercolor{palette primary}{bg=MainBlue, fg=white}
\setbeamercolor{palette secondary}{bg=MainBlue!85, fg=white}
\setbeamercolor{palette tertiary}{bg=MainBlue!70, fg=white}
\setbeamercolor{structure}{fg=MainBlue}
\setbeamercolor{title}{fg=MainBlue}
\setbeamercolor{frametitle}{fg=MainBlue, bg=white}
\setbeamercolor{block title}{bg=MainBlue, fg=white}
\setbeamercolor{block body}{bg=VeryLightGray, fg=DarkGray}
\setbeamercolor{block title alerted}{bg=Crimson, fg=white}
\setbeamercolor{block body alerted}{bg=Crimson!8, fg=DarkGray}
\setbeamercolor{block title example}{bg=Forest, fg=white}
\setbeamercolor{block body example}{bg=Forest!8, fg=DarkGray}
\setbeamercolor{item}{fg=MainBlue}

\setbeamertemplate{navigation symbols}{}

\setbeamertemplate{footline}{
    \leavevmode%
    \hbox{%
        \begin{beamercolorbox}[wd=.333333\paperwidth,ht=2.5ex,dp=1ex,center]{author in head/foot}%
            \usebeamerfont{author in head/foot}\insertshortauthor
        \end{beamercolorbox}%
        \begin{beamercolorbox}[wd=.333333\paperwidth,ht=2.5ex,dp=1ex,center]{title in head/foot}%
            \usebeamerfont{title in head/foot}\insertshorttitle
        \end{beamercolorbox}%
        \begin{beamercolorbox}[wd=.333333\paperwidth,ht=2.5ex,dp=1ex,right]{date in head/foot}%
            \usebeamerfont{date in head/foot}\insertshortdate{}\hspace*{2em}
            \insertframenumber{} / \inserttotalframenumber\hspace*{2ex}
        \end{beamercolorbox}}%
    \vskip0pt%
}

%=============================================================================
% PACKAGES
%=============================================================================
\usepackage{amsmath, amssymb, amsthm}
\usepackage{mathtools}
\usepackage{bm}
\usepackage{tikz}
\usetikzlibrary{arrows.meta, positioning, shapes, calc}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{array}
\usepackage{graphicx}
\graphicspath{{logos/}{charts/}}

%=============================================================================
% CUSTOM COMMANDS
%=============================================================================
\newcommand{\E}{\mathbb{E}}
\newcommand{\Var}{\text{Var}}
\newcommand{\Cov}{\text{Cov}}
\newcommand{\Corr}{\text{Corr}}
\newcommand{\R}{\mathbb{R}}

%=============================================================================
% TITLE INFORMATION
%=============================================================================
\title[Chapter 2: Seminar]{Chapter 2: Seminar --- ARMA Models}
\subtitle{Bachelor program Faculty of Cybernetics, Statistics and Economic Informatics, Bucharest University of Economic Studies, Romania}
\author[Time Series Analysis]{Time Series Analysis and Forecasting}
\institute{Bucharest University of Economic Studies}
\date{Academic Year 2025-2026}

\begin{document}

%=============================================================================
% TITLE SLIDE
%=============================================================================
\begin{frame}
    \titlepage
\end{frame}

%=============================================================================
% OUTLINE
%=============================================================================
\begin{frame}{Seminar Outline}
    \tableofcontents
\end{frame}

%=============================================================================
% SECTION 1: MULTIPLE CHOICE QUIZ
%=============================================================================
\section{Multiple Choice Quiz}

\begin{frame}{Quiz 1: Lag Operator}
    \textbf{Question:} What is the result of applying $(1-L)^2$ to $X_t$?

    \vspace{0.5cm}
    \begin{enumerate}[A.]
        \item $X_t - X_{t-1}$
        \item $X_t - 2X_{t-1} + X_{t-2}$
        \item $X_t + X_{t-1} + X_{t-2}$
        \item $X_t - X_{t-2}$
    \end{enumerate}
\end{frame}

\begin{frame}{Quiz 1: Solution}
    \textbf{Answer: B}

    \vspace{0.3cm}
    \textbf{Explanation:}
    \begin{align*}
        (1-L)^2 X_t &= (1 - 2L + L^2)X_t \\
        &= X_t - 2LX_t + L^2X_t \\
        &= X_t - 2X_{t-1} + X_{t-2}
    \end{align*}

    This is the \textbf{second difference} of $X_t$.

    \vspace{0.3cm}
    \textbf{Note:} $(1-L)$ is the first difference operator, $(1-L)^2$ is the second difference.
\end{frame}

\begin{frame}{Quiz 2: AR(1) Stationarity}
    \textbf{Question:} For which value of $\phi$ is the AR(1) process $X_t = 0.5 + \phi X_{t-1} + \varepsilon_t$ stationary?

    \vspace{0.5cm}
    \begin{enumerate}[A.]
        \item $\phi = 1.2$
        \item $\phi = 1.0$
        \item $\phi = -0.8$
        \item $\phi = -1.5$
    \end{enumerate}
\end{frame}

\begin{frame}{Quiz 2: Solution}
    \textbf{Answer: C}

    \vspace{0.3cm}
    \textbf{Explanation:} AR(1) is stationary if and only if $|\phi| < 1$.

    \vspace{0.3cm}
    Checking each option:
    \begin{itemize}
        \item A. $|\phi| = 1.2 > 1$ $\rightarrow$ Non-stationary (explosive)
        \item B. $|\phi| = 1.0$ $\rightarrow$ Non-stationary (unit root / random walk)
        \item C. $|\phi| = 0.8 < 1$ $\rightarrow$ \textcolor{Forest}{\textbf{Stationary}}
        \item D. $|\phi| = 1.5 > 1$ $\rightarrow$ Non-stationary (explosive)
    \end{itemize}

    \vspace{0.3cm}
    The stationarity condition requires the root of $1 - \phi z = 0$ to lie outside the unit circle, i.e., $|1/\phi| > 1$, which means $|\phi| < 1$.
\end{frame}

\begin{frame}{Quiz 3: ACF Pattern}
    \textbf{Question:} You observe the following ACF pattern: significant spike at lag 1, then all other lags are within confidence bands. The PACF shows gradual decay. What model is suggested?

    \vspace{0.5cm}
    \begin{enumerate}[A.]
        \item AR(1)
        \item MA(1)
        \item ARMA(1,1)
        \item White noise
    \end{enumerate}
\end{frame}

\begin{frame}{Quiz 3: Solution}
    \textbf{Answer: B}

    \vspace{0.3cm}
    \textbf{Explanation:}
    \begin{center}
    \begin{tabular}{lcc}
        \toprule
        \textbf{Model} & \textbf{ACF} & \textbf{PACF} \\
        \midrule
        AR(p) & Decays & Cuts off at lag $p$ \\
        MA(q) & Cuts off at lag $q$ & Decays \\
        ARMA & Decays & Decays \\
        \bottomrule
    \end{tabular}
    \end{center}

    \vspace{0.3cm}
    The pattern described:
    \begin{itemize}
        \item ACF cuts off after lag 1 $\rightarrow$ suggests MA
        \item PACF decays $\rightarrow$ confirms MA (not AR)
    \end{itemize}

    Therefore, this is an \textbf{MA(1)} process.
\end{frame}

\begin{frame}{Quiz 4: MA Invertibility}
    \textbf{Question:} For the MA(1) process $X_t = \varepsilon_t + 1.5\varepsilon_{t-1}$, is the process invertible?

    \vspace{0.5cm}
    \begin{enumerate}[A.]
        \item Yes, because MA processes are always invertible
        \item Yes, because $1.5 > 0$
        \item No, because $|\theta| = 1.5 > 1$
        \item No, because MA processes are never invertible
    \end{enumerate}
\end{frame}

\begin{frame}{Quiz 4: Solution}
    \textbf{Answer: C}

    \vspace{0.3cm}
    \textbf{Explanation:} An MA(1) process $X_t = \varepsilon_t + \theta\varepsilon_{t-1}$ is invertible if $|\theta| < 1$.

    \vspace{0.3cm}
    Here, $\theta = 1.5$, so $|\theta| = 1.5 > 1$ $\rightarrow$ \textbf{Not invertible}

    \vspace{0.3cm}
    \textbf{Key points:}
    \begin{itemize}
        \item MA processes are \textit{always stationary} (for finite coefficients)
        \item But they are \textit{not always invertible}
        \item Invertibility requires roots of $\theta(z) = 1 + \theta z = 0$ outside unit circle
        \item Root: $z = -1/\theta = -1/1.5 = -0.67$ is \textit{inside} unit circle
    \end{itemize}
\end{frame}

\begin{frame}{Quiz 5: ARMA Representation}
    \textbf{Question:} The compact form $\phi(L)X_t = \theta(L)\varepsilon_t$ represents which model?

    \vspace{0.5cm}
    \begin{enumerate}[A.]
        \item Pure AR model
        \item Pure MA model
        \item ARMA model
        \item None of the above
    \end{enumerate}
\end{frame}

\begin{frame}{Quiz 5: Solution}
    \textbf{Answer: C}

    \vspace{0.3cm}
    \textbf{Explanation:}
    \begin{itemize}
        \item $\phi(L) = 1 - \phi_1 L - \cdots - \phi_p L^p$ is the AR polynomial
        \item $\theta(L) = 1 + \theta_1 L + \cdots + \theta_q L^q$ is the MA polynomial
    \end{itemize}

    \vspace{0.3cm}
    The equation $\phi(L)X_t = \theta(L)\varepsilon_t$ expands to:
    $$X_t - \phi_1 X_{t-1} - \cdots - \phi_p X_{t-p} = \varepsilon_t + \theta_1\varepsilon_{t-1} + \cdots + \theta_q\varepsilon_{t-q}$$

    This is the general \textbf{ARMA(p,q)} model.

    \vspace{0.3cm}
    \textbf{Special cases:}
    \begin{itemize}
        \item $\theta(L) = 1$ (no MA): Pure AR
        \item $\phi(L) = 1$ (no AR): Pure MA
    \end{itemize}
\end{frame}

\begin{frame}{Quiz 6: Information Criteria}
    \textbf{Question:} When comparing ARMA(1,1) and ARMA(2,1) using BIC, which statement is correct?

    \vspace{0.5cm}
    \begin{enumerate}[A.]
        \item Lower BIC always means better forecasts
        \item BIC penalizes complexity less than AIC
        \item The model with lower BIC is preferred
        \item BIC can only compare models with same number of parameters
    \end{enumerate}
\end{frame}

\begin{frame}{Quiz 6: Solution}
    \textbf{Answer: C}

    \vspace{0.3cm}
    \textbf{Explanation:}
    \begin{itemize}
        \item A is \textbf{false}: Lower BIC indicates better in-sample fit relative to complexity, but doesn't guarantee best forecasts
        \item B is \textbf{false}: BIC penalizes complexity \textit{more} than AIC (penalty is $k\ln(n)$ vs $2k$)
        \item C is \textbf{true}: Lower BIC = better balance of fit and parsimony
        \item D is \textbf{false}: BIC is specifically designed to compare models with different numbers of parameters
    \end{itemize}

    \vspace{0.3cm}
    \textbf{Formulas:}
    $$\text{AIC} = -2\ln(\hat{L}) + 2k$$
    $$\text{BIC} = -2\ln(\hat{L}) + k\ln(n)$$
\end{frame}

\begin{frame}{Quiz 7: Ljung-Box Test}
    \textbf{Question:} After fitting an ARMA(2,1) model, you run the Ljung-Box test on residuals and get p-value = 0.02. What do you conclude?

    \vspace{0.5cm}
    \begin{enumerate}[A.]
        \item The model is adequate
        \item Residuals are white noise
        \item There is significant autocorrelation in residuals
        \item The model has too many parameters
    \end{enumerate}
\end{frame}

\begin{frame}{Quiz 7: Solution}
    \textbf{Answer: C}

    \vspace{0.3cm}
    \textbf{Explanation:} The Ljung-Box test has:
    \begin{itemize}
        \item $H_0$: Residuals are white noise (no autocorrelation)
        \item $H_1$: Residuals have significant autocorrelation
    \end{itemize}

    \vspace{0.3cm}
    With p-value = 0.02 $<$ 0.05:
    \begin{itemize}
        \item We \textbf{reject} $H_0$
        \item Conclusion: residuals are \textbf{not} white noise
        \item The model is \textbf{inadequate} --- significant structure remains
    \end{itemize}

    \vspace{0.3cm}
    \textbf{Next step:} Try a different model (e.g., increase $p$ or $q$)
\end{frame}

\begin{frame}{Quiz 8: Forecasting}
    \textbf{Question:} For an AR(1) model with $\phi = 0.6$ and mean $\mu = 10$, what happens to forecasts as horizon $h \to \infty$?

    \vspace{0.5cm}
    \begin{enumerate}[A.]
        \item Forecasts grow without bound
        \item Forecasts converge to 0
        \item Forecasts converge to $\mu = 10$
        \item Forecasts oscillate forever
    \end{enumerate}
\end{frame}

\begin{frame}{Quiz 8: Solution}
    \textbf{Answer: C}

    \vspace{0.3cm}
    \textbf{Explanation:} For AR(1), the $h$-step ahead forecast is:
    $$\hat{X}_{n+h|n} = \mu + \phi^h(X_n - \mu)$$

    \vspace{0.3cm}
    Since $|\phi| = 0.6 < 1$:
    $$\lim_{h \to \infty} \phi^h = 0$$

    Therefore:
    $$\lim_{h \to \infty} \hat{X}_{n+h|n} = \mu + 0 \cdot (X_n - \mu) = \mu = 10$$

    \vspace{0.3cm}
    \textbf{Key insight:} Long-run forecasts from stationary ARMA models always converge to the unconditional mean.
\end{frame}

%=============================================================================
% SECTION 2: TRUE/FALSE
%=============================================================================
\section{True/False Questions}

\begin{frame}{True/False Questions}
    Determine if each statement is True or False:

    \vspace{0.3cm}
    \begin{enumerate}
        \item An AR(2) process can exhibit pseudo-cyclical behavior.
        \item MA processes require a stationarity condition.
        \item The PACF of an AR(p) process cuts off after lag $p$.
        \item If AIC selects ARMA(2,1) and BIC selects ARMA(1,1), they cannot both be correct.
        \item Forecast confidence intervals narrow as the forecast horizon increases.
        \item The Yule-Walker equations can be used to estimate MA parameters.
    \end{enumerate}
\end{frame}

\begin{frame}{True/False: Solutions}
    \begin{enumerate}
        \item \textcolor{Forest}{\textbf{TRUE}}: AR(2) with complex roots shows damped oscillations
        \item \textcolor{Crimson}{\textbf{FALSE}}: MA processes are always stationary; they need \textit{invertibility} condition
        \item \textcolor{Forest}{\textbf{TRUE}}: This is the key identification feature of AR(p)
        \item \textcolor{Crimson}{\textbf{FALSE}}: Both can be ``correct'' --- they optimize different criteria (AIC favors fit, BIC favors parsimony)
        \item \textcolor{Crimson}{\textbf{FALSE}}: Confidence intervals \textit{widen} as horizon increases (more uncertainty)
        \item \textcolor{Crimson}{\textbf{FALSE}}: Yule-Walker is for AR models only; MA uses MLE
    \end{enumerate}
\end{frame}

%=============================================================================
% SECTION 3: CALCULATION EXERCISES
%=============================================================================
\section{Calculation Exercises}

\begin{frame}{Exercise 1: AR(1) Properties}
    \textbf{Problem:} Consider the AR(1) process:
    $$X_t = 2 + 0.7 X_{t-1} + \varepsilon_t, \quad \varepsilon_t \sim WN(0, 9)$$

    Calculate:
    \begin{enumerate}
        \item The mean $\mu$
        \item The variance $\gamma(0)$
        \item The autocovariance $\gamma(1)$ and $\gamma(2)$
        \item The autocorrelation $\rho(1)$ and $\rho(2)$
    \end{enumerate}
\end{frame}

\begin{frame}{Exercise 1: Solution}
    Given: $c = 2$, $\phi = 0.7$, $\sigma^2 = 9$

    \vspace{0.3cm}
    \textbf{1. Mean:}
    $$\mu = \frac{c}{1-\phi} = \frac{2}{1-0.7} = \frac{2}{0.3} = 6.67$$

    \textbf{2. Variance:}
    $$\gamma(0) = \frac{\sigma^2}{1-\phi^2} = \frac{9}{1-0.49} = \frac{9}{0.51} = 17.65$$

    \textbf{3. Autocovariance:}
    $$\gamma(1) = \phi \cdot \gamma(0) = 0.7 \times 17.65 = 12.35$$
    $$\gamma(2) = \phi^2 \cdot \gamma(0) = 0.49 \times 17.65 = 8.65$$

    \textbf{4. Autocorrelation:}
    $$\rho(1) = \phi = 0.7, \quad \rho(2) = \phi^2 = 0.49$$
\end{frame}

\begin{frame}{Exercise 2: MA(1) Properties}
    \textbf{Problem:} Consider the MA(1) process:
    $$X_t = 5 + \varepsilon_t - 0.4\varepsilon_{t-1}, \quad \varepsilon_t \sim WN(0, 4)$$

    Calculate:
    \begin{enumerate}
        \item The mean $\mu$
        \item The variance $\gamma(0)$
        \item The autocovariance $\gamma(1)$
        \item The autocorrelation $\rho(1)$
        \item Is this process invertible?
    \end{enumerate}
\end{frame}

\begin{frame}{Exercise 2: Solution}
    Given: $\mu = 5$, $\theta = -0.4$, $\sigma^2 = 4$

    \vspace{0.3cm}
    \textbf{1. Mean:}
    $$\E[X_t] = \mu = 5$$

    \textbf{2. Variance:}
    $$\gamma(0) = \sigma^2(1 + \theta^2) = 4(1 + 0.16) = 4 \times 1.16 = 4.64$$

    \textbf{3. Autocovariance at lag 1:}
    $$\gamma(1) = \theta\sigma^2 = -0.4 \times 4 = -1.6$$

    \textbf{4. Autocorrelation:}
    $$\rho(1) = \frac{\gamma(1)}{\gamma(0)} = \frac{-1.6}{4.64} = -0.345$$

    \textbf{5. Invertibility:} $|\theta| = 0.4 < 1$ $\rightarrow$ \textcolor{Forest}{\textbf{Yes, invertible}}
\end{frame}

\begin{frame}{Exercise 3: Characteristic Roots}
    \textbf{Problem:} Consider the AR(2) process:
    $$X_t = 0.5X_{t-1} + 0.3X_{t-2} + \varepsilon_t$$

    \begin{enumerate}
        \item Write the characteristic equation
        \item Find the characteristic roots
        \item Is this process stationary?
    \end{enumerate}
\end{frame}

\begin{frame}{Exercise 3: Solution}
    \textbf{1. Characteristic equation:}
    $$\phi(z) = 1 - \phi_1 z - \phi_2 z^2 = 1 - 0.5z - 0.3z^2 = 0$$

    Or: $0.3z^2 + 0.5z - 1 = 0$

    \vspace{0.3cm}
    \textbf{2. Roots (using quadratic formula):}
    $$z = \frac{-0.5 \pm \sqrt{0.25 + 1.2}}{0.6} = \frac{-0.5 \pm 1.204}{0.6}$$

    $$z_1 = \frac{0.704}{0.6} = 1.17, \quad z_2 = \frac{-1.704}{0.6} = -2.84$$

    \vspace{0.3cm}
    \textbf{3. Stationarity check:}

    Both roots have $|z| > 1$: $|z_1| = 1.17 > 1$ and $|z_2| = 2.84 > 1$

    $\rightarrow$ \textcolor{Forest}{\textbf{Stationary}} (roots outside unit circle)
\end{frame}

\begin{frame}{Exercise 4: Forecasting}
    \textbf{Problem:} You have fit an AR(1) model:
    $$X_t = 3 + 0.8X_{t-1} + \varepsilon_t, \quad \sigma^2 = 4$$

    Given $X_{100} = 20$, calculate:
    \begin{enumerate}
        \item The 1-step ahead forecast $\hat{X}_{101|100}$
        \item The 2-step ahead forecast $\hat{X}_{102|100}$
        \item The long-run forecast $\hat{X}_{100+h|100}$ as $h \to \infty$
        \item The 95\% confidence interval for $\hat{X}_{101|100}$
    \end{enumerate}
\end{frame}

\begin{frame}{Exercise 4: Solution}
    Given: $c = 3$, $\phi = 0.8$, $\sigma^2 = 4$, $X_{100} = 20$

    \textbf{Mean:} $\mu = \frac{3}{1-0.8} = 15$

    \vspace{0.3cm}
    \textbf{1. One-step forecast:}
    $$\hat{X}_{101|100} = c + \phi X_{100} = 3 + 0.8 \times 20 = 19$$

    \textbf{2. Two-step forecast:}
    $$\hat{X}_{102|100} = c + \phi \hat{X}_{101|100} = 3 + 0.8 \times 19 = 18.2$$

    \textbf{3. Long-run forecast:}
    $$\lim_{h \to \infty} \hat{X}_{100+h|100} = \mu = 15$$

    \textbf{4. 95\% CI for 1-step:}
    $$\text{MSFE}(1) = \sigma^2 = 4, \quad \sqrt{\text{MSFE}(1)} = 2$$
    $$CI: 19 \pm 1.96 \times 2 = [15.08, 22.92]$$
\end{frame}

%=============================================================================
% SECTION 4: PYTHON EXERCISES
%=============================================================================
\section{Python Exercises}

\begin{frame}{Python Exercise 1: Simulate and Fit AR(1)}
    \textbf{Task:}
    \begin{enumerate}
        \item Simulate 500 observations from AR(1) with $\phi = 0.7$
        \item Plot the series and ACF/PACF
        \item Fit an AR(1) model and check if $\hat{\phi} \approx 0.7$
        \item Examine residual diagnostics
    \end{enumerate}

    \vspace{0.3cm}
    \textbf{Hint code:}

    \texttt{np.random.seed(42)}

    \texttt{n = 500}

    \texttt{phi = 0.7}

    \texttt{x = np.zeros(n)}

    \texttt{for t in range(1, n):}

    \texttt{\quad x[t] = phi * x[t-1] + np.random.randn()}
\end{frame}

\begin{frame}{Python Exercise 2: Model Selection}
    \textbf{Task:}
    \begin{enumerate}
        \item Load a real time series (e.g., stock returns)
        \item Check stationarity using ADF test
        \item Compare AIC/BIC for ARMA(1,0), ARMA(0,1), ARMA(1,1), ARMA(2,1)
        \item Select the best model
        \item Generate forecasts with confidence intervals
    \end{enumerate}

    \vspace{0.3cm}
    \textbf{Key functions:}
    \begin{itemize}
        \item \texttt{adfuller()} for stationarity test
        \item \texttt{ARIMA(data, order=(p,0,q)).fit()} for fitting
        \item \texttt{results.aic}, \texttt{results.bic} for criteria
        \item \texttt{results.get\_forecast(h)} for predictions
    \end{itemize}
\end{frame}

\begin{frame}{Python Exercise 3: Diagnostic Checking}
    \textbf{Task:} After fitting a model, perform complete diagnostics:
    \begin{enumerate}
        \item Plot residuals over time
        \item Plot ACF of residuals
        \item Create Q-Q plot
        \item Run Ljung-Box test
        \item Check if AR/MA roots are outside unit circle
    \end{enumerate}

    \vspace{0.3cm}
    \textbf{Key functions:}
    \begin{itemize}
        \item \texttt{results.resid} for residuals
        \item \texttt{plot\_acf(resid)} for ACF plot
        \item \texttt{stats.probplot(resid)} for Q-Q plot
        \item \texttt{acorr\_ljungbox(resid)} for portmanteau test
        \item \texttt{results.arroots}, \texttt{results.maroots} for roots
    \end{itemize}
\end{frame}

%=============================================================================
% SECTION 5: DISCUSSION
%=============================================================================
\section{Discussion Questions}

\begin{frame}{Discussion 1: Model Selection}
    \textbf{Scenario:} You're modeling monthly inflation rates. After checking stationarity (passed), you find:
    \begin{itemize}
        \item ACF: significant at lags 1, 2, 3, then decays
        \item PACF: significant at lags 1, 2, then cuts off
        \item AIC selects ARMA(2,3)
        \item BIC selects ARMA(2,0) = AR(2)
    \end{itemize}

    \vspace{0.3cm}
    \textbf{Questions:}
    \begin{enumerate}
        \item What does the ACF/PACF pattern suggest?
        \item Why do AIC and BIC disagree?
        \item Which model would you choose and why?
        \item What additional checks would you perform?
    \end{enumerate}
\end{frame}

\begin{frame}{Discussion 2: Forecast Evaluation}
    \textbf{Scenario:} You fit an ARMA(1,1) model to daily stock returns. The in-sample fit looks good (Ljung-Box p-value = 0.45), but out-of-sample RMSE is worse than a simple random walk forecast.

    \vspace{0.3cm}
    \textbf{Questions:}
    \begin{enumerate}
        \item Is this surprising? Why or why not?
        \item What does this tell us about stock return predictability?
        \item Should you conclude the ARMA model is useless?
        \item What alternatives might you consider?
    \end{enumerate}

    \vspace{0.3cm}
    \textbf{Hint:} Think about the Efficient Market Hypothesis and what ARMA captures vs. what it doesn't (e.g., volatility clustering).
\end{frame}

\begin{frame}{Discussion 3: Real-World Application}
    \textbf{Scenario:} A central bank economist asks you to forecast quarterly GDP growth for policy planning.

    \vspace{0.3cm}
    \textbf{Questions:}
    \begin{enumerate}
        \item What preliminary analysis would you do before fitting ARMA?
        \item GDP is often non-stationary --- how would you handle this?
        \item Would you use AIC or BIC for model selection? Why?
        \item How would you communicate forecast uncertainty to policymakers?
        \item What limitations of ARMA models should you mention?
    \end{enumerate}
\end{frame}

%=============================================================================
% SUMMARY
%=============================================================================
\section{Summary}

\begin{frame}{Key Takeaways from Today's Seminar}
    \begin{enumerate}
        \item \textbf{AR models:} Current value depends on past values
        \begin{itemize}
            \item Stationarity: $|\phi| < 1$ for AR(1)
            \item PACF cuts off at lag $p$
        \end{itemize}

        \item \textbf{MA models:} Current value depends on past shocks
        \begin{itemize}
            \item Always stationary; invertibility: $|\theta| < 1$ for MA(1)
            \item ACF cuts off at lag $q$
        \end{itemize}

        \item \textbf{Model selection:} Use ACF/PACF patterns + information criteria

        \item \textbf{Diagnostics:} Residuals must be white noise (Ljung-Box test)

        \item \textbf{Forecasting:} Point forecasts converge to mean; uncertainty grows
    \end{enumerate}

    \vspace{0.3cm}
    \textbf{Next Seminar:} ARIMA and Seasonal Models
\end{frame}

\end{document}
