{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/danpele/Time-Series-Analysis/blob/main/chapter8_seminar_notebook.ipynb)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 8 Seminar: Modern Extensions - Practice\n",
    "\n",
    "**Course:** Time Series Analysis and Forecasting  \n",
    "**Program:** Bachelor program, Faculty of Cybernetics, Statistics and Economic Informatics, Bucharest University of Economic Studies, Romania  \n",
    "**Academic Year:** 2025-2026\n",
    "\n",
    "---\n",
    "\n",
    "## Seminar Objectives\n",
    "\n",
    "In this practical seminar, you will:\n",
    "1. Estimate the Hurst exponent and identify long memory\n",
    "2. Apply feature engineering for time series ML\n",
    "3. Build and evaluate Random Forest models\n",
    "4. Implement simple LSTM networks\n",
    "5. Compare classical vs ML approaches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages (for Colab)\n",
    "import sys\n",
    "if 'google.colab' in sys.modules:\n",
    "    !pip install nolds arch yfinance -q\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "try:\n",
    "    import nolds\n",
    "    HAS_NOLDS = True\n",
    "except ImportError:\n",
    "    HAS_NOLDS = False\n",
    "    print(\"nolds not installed - using manual Hurst calculation\")\n",
    "\n",
    "try:\n",
    "    import yfinance as yf\n",
    "    HAS_YF = True\n",
    "except ImportError:\n",
    "    HAS_YF = False\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (12, 5)\n",
    "plt.rcParams['font.size'] = 11\n",
    "plt.rcParams['axes.facecolor'] = 'none'\n",
    "plt.rcParams['figure.facecolor'] = 'none'\n",
    "plt.rcParams['axes.grid'] = False\n",
    "plt.rcParams['axes.spines.top'] = False\n",
    "plt.rcParams['axes.spines.right'] = False\n",
    "plt.rcParams['legend.frameon'] = False\n",
    "\n",
    "COLORS = {'blue': '#1A3A6E', 'red': '#DC3545', 'green': '#2E7D32', 'orange': '#E67E22', 'gray': '#666666'}\n",
    "\n",
    "print(\"Setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1: Hurst Exponent Estimation\n",
    "\n",
    "**Task:** Estimate the Hurst exponent for different types of series and interpret the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hurst_rs(series, min_window=10):\n",
    "    \"\"\"Calculate Hurst exponent using R/S analysis.\"\"\"\n",
    "    n = len(series)\n",
    "    max_k = int(np.floor(n / min_window))\n",
    "    \n",
    "    rs_values = []\n",
    "    n_values = []\n",
    "    \n",
    "    for k in range(2, max_k + 1):\n",
    "        size = n // k\n",
    "        rs_k = []\n",
    "        \n",
    "        for i in range(k):\n",
    "            subseries = series[i * size:(i + 1) * size]\n",
    "            mean_sub = np.mean(subseries)\n",
    "            cumdev = np.cumsum(subseries - mean_sub)\n",
    "            R = np.max(cumdev) - np.min(cumdev)\n",
    "            S = np.std(subseries, ddof=1)\n",
    "            if S > 0:\n",
    "                rs_k.append(R / S)\n",
    "        \n",
    "        if rs_k:\n",
    "            rs_values.append(np.mean(rs_k))\n",
    "            n_values.append(size)\n",
    "    \n",
    "    log_n = np.log(n_values)\n",
    "    log_rs = np.log(rs_values)\n",
    "    \n",
    "    slope, _ = np.polyfit(log_n, log_rs, 1)\n",
    "    return slope\n",
    "\n",
    "# Generate different types of series\n",
    "np.random.seed(42)\n",
    "n = 1000\n",
    "\n",
    "# 1. Random walk (H ≈ 0.5)\n",
    "random_walk = np.cumsum(np.random.randn(n))\n",
    "\n",
    "# 2. Mean-reverting (H < 0.5)\n",
    "mean_rev = np.zeros(n)\n",
    "for i in range(1, n):\n",
    "    mean_rev[i] = -0.5 * mean_rev[i-1] + np.random.randn()\n",
    "\n",
    "# 3. Trending/Persistent (H > 0.5)\n",
    "persistent = np.zeros(n)\n",
    "for i in range(1, n):\n",
    "    persistent[i] = 0.7 * persistent[i-1] + np.sign(persistent[i-1]) * 0.3 + np.random.randn() * 0.5\n",
    "persistent = np.cumsum(persistent)\n",
    "\n",
    "print(\"Hurst Exponent Estimation\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "series_dict = {\n",
    "    'Random Walk': random_walk,\n",
    "    'Mean-Reverting': mean_rev,\n",
    "    'Persistent': persistent\n",
    "}\n",
    "\n",
    "for name, series in series_dict.items():\n",
    "    h = hurst_rs(series)\n",
    "    if HAS_NOLDS:\n",
    "        h_nolds = nolds.hurst_rs(series)\n",
    "        print(f\"{name:20s}: H = {h:.3f} (manual), {h_nolds:.3f} (nolds)\")\n",
    "    else:\n",
    "        print(f\"{name:20s}: H = {h:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the three types of series\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "h_vals = {}\n",
    "for ax, (name, series) in zip(axes, series_dict.items()):\n",
    "    h = hurst_rs(series)\n",
    "    h_vals[name] = h\n",
    "    \n",
    "    if h < 0.5:\n",
    "        color = COLORS['green']\n",
    "        behavior = 'Mean-reverting'\n",
    "    elif h > 0.5:\n",
    "        color = COLORS['red']\n",
    "        behavior = 'Trending'\n",
    "    else:\n",
    "        color = COLORS['gray']\n",
    "        behavior = 'Random Walk'\n",
    "    \n",
    "    ax.plot(series, color=color, linewidth=0.8)\n",
    "    ax.set_title(f\"{name}\\nH = {h:.3f} ({behavior})\", fontweight='bold')\n",
    "    ax.set_xlabel('Time')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nInterpretation Guide:\")\n",
    "print(\"  H < 0.5: Anti-persistent (mean-reverting) - good for mean-reversion strategies\")\n",
    "print(\"  H = 0.5: Random walk (no memory) - unpredictable\")\n",
    "print(\"  H > 0.5: Persistent (trending) - good for momentum strategies\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2: Long Memory in Real Financial Data\n",
    "\n",
    "**Task:** Download Bitcoin data and test for long memory in returns vs volatility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Get Bitcoin data\nif HAS_YF:\n    btc = yf.download('BTC-USD', start='2020-01-01', end='2024-12-31', progress=False)\n    # Handle MultiIndex columns from newer yfinance versions\n    if isinstance(btc.columns, pd.MultiIndex):\n        btc.columns = btc.columns.droplevel(1)\n    # Ensure we get a Series and extract values\n    close_prices = btc['Close'].squeeze()\n    price = close_prices.dropna().values\n    returns = np.diff(np.log(price)) * 100  # Log returns in %\nelse:\n    # Simulate if no yfinance\n    np.random.seed(123)\n    n = 1000\n    returns = np.random.randn(n) * 3  # Simulate returns\n    # Add volatility clustering\n    vol = np.ones(n)\n    for i in range(1, n):\n        vol[i] = 0.1 + 0.85 * vol[i-1] + 0.1 * returns[i-1]**2\n    returns = returns * np.sqrt(vol)\n    price = 10000 * np.exp(np.cumsum(returns / 100))\n\nprint(f\"Data loaded: {len(returns)} observations\")\n\n# Calculate volatility proxies\nabs_returns = np.abs(returns)\nsquared_returns = returns ** 2\n\nprint(\"\\nLong Memory Analysis: Returns vs Volatility\")\nprint(\"=\" * 60)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare Hurst exponents\n",
    "h_returns = hurst_rs(returns)\n",
    "h_abs = hurst_rs(abs_returns)\n",
    "h_sq = hurst_rs(squared_returns)\n",
    "\n",
    "print(f\"{'Series':<25} {'Hurst':>10} {'Memory Type':>20}\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "for name, h in [('Returns', h_returns), ('|Returns|', h_abs), ('Returns²', h_sq)]:\n",
    "    if h < 0.45:\n",
    "        mem_type = 'Anti-persistent'\n",
    "    elif h > 0.55:\n",
    "        mem_type = 'Long memory (persistent)'\n",
    "    else:\n",
    "        mem_type = 'Short memory (~random)'\n",
    "    print(f\"{name:<25} {h:>10.3f} {mem_type:>20}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"KEY FINDING: Volatility has long memory, returns do not!\")\n",
    "print(\"This is a stylized fact used in FIGARCH models.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize ACF decay comparison\n",
    "from statsmodels.tsa.stattools import acf\n",
    "\n",
    "max_lag = 100\n",
    "acf_returns = acf(returns, nlags=max_lag, fft=True)\n",
    "acf_abs = acf(abs_returns, nlags=max_lag, fft=True)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 4))\n",
    "\n",
    "# Returns ACF (fast decay)\n",
    "axes[0].bar(range(max_lag+1), acf_returns, color=COLORS['blue'], alpha=0.7, width=0.8)\n",
    "axes[0].axhline(y=1.96/np.sqrt(len(returns)), color='red', linestyle='--', alpha=0.5)\n",
    "axes[0].axhline(y=-1.96/np.sqrt(len(returns)), color='red', linestyle='--', alpha=0.5)\n",
    "axes[0].set_title(f'ACF of Returns (H = {h_returns:.3f})\\nFast Decay = Short Memory', fontweight='bold')\n",
    "axes[0].set_xlabel('Lag')\n",
    "axes[0].set_xlim(-1, max_lag)\n",
    "\n",
    "# Absolute returns ACF (slow decay)\n",
    "axes[1].bar(range(max_lag+1), acf_abs, color=COLORS['red'], alpha=0.7, width=0.8)\n",
    "axes[1].axhline(y=1.96/np.sqrt(len(abs_returns)), color='red', linestyle='--', alpha=0.5)\n",
    "axes[1].axhline(y=-1.96/np.sqrt(len(abs_returns)), color='red', linestyle='--', alpha=0.5)\n",
    "axes[1].set_title(f'ACF of |Returns| (H = {h_abs:.3f})\\nSlow Decay = Long Memory', fontweight='bold')\n",
    "axes[1].set_xlabel('Lag')\n",
    "axes[1].set_xlim(-1, max_lag)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3: Feature Engineering for Time Series ML\n",
    "\n",
    "**Task:** Create lag features and rolling statistics for ML models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_features(series, lags=7, rolling_windows=[7, 14, 30]):\n",
    "    \"\"\"\n",
    "    Create lag and rolling features for time series ML.\n",
    "    \n",
    "    Parameters:\n",
    "    - series: 1D array of values\n",
    "    - lags: number of lag features\n",
    "    - rolling_windows: list of window sizes for rolling stats\n",
    "    \n",
    "    Returns:\n",
    "    - DataFrame with features\n",
    "    \"\"\"\n",
    "    df = pd.DataFrame({'y': series})\n",
    "    \n",
    "    # Lag features\n",
    "    for i in range(1, lags + 1):\n",
    "        df[f'lag_{i}'] = df['y'].shift(i)\n",
    "    \n",
    "    # Rolling statistics\n",
    "    for w in rolling_windows:\n",
    "        df[f'rolling_mean_{w}'] = df['y'].shift(1).rolling(window=w).mean()\n",
    "        df[f'rolling_std_{w}'] = df['y'].shift(1).rolling(window=w).std()\n",
    "        df[f'rolling_min_{w}'] = df['y'].shift(1).rolling(window=w).min()\n",
    "        df[f'rolling_max_{w}'] = df['y'].shift(1).rolling(window=w).max()\n",
    "    \n",
    "    # Target: next value\n",
    "    df['target'] = df['y']\n",
    "    \n",
    "    # Drop rows with NaN\n",
    "    df = df.dropna()\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Create features from price data\n",
    "features_df = create_features(price, lags=7, rolling_windows=[7, 14, 30])\n",
    "print(f\"Feature DataFrame shape: {features_df.shape}\")\n",
    "print(f\"\\nFeatures created:\")\n",
    "print(features_df.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display sample of features\n",
    "print(\"\\nSample of engineered features:\")\n",
    "display_cols = ['y', 'lag_1', 'lag_2', 'rolling_mean_7', 'rolling_std_7', 'target']\n",
    "print(features_df[display_cols].head(10).round(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4: Random Forest with Time Series Cross-Validation\n",
    "\n",
    "**Task:** Train Random Forest and evaluate using TimeSeriesSplit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data\n",
    "feature_cols = [c for c in features_df.columns if c not in ['y', 'target']]\n",
    "X = features_df[feature_cols].values\n",
    "y = features_df['target'].values\n",
    "\n",
    "print(f\"Features: {len(feature_cols)}\")\n",
    "print(f\"Samples: {len(X)}\")\n",
    "\n",
    "# Time Series Cross-Validation\n",
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "\n",
    "# Store results\n",
    "cv_results = []\n",
    "\n",
    "print(\"\\nTime Series Cross-Validation\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for fold, (train_idx, test_idx) in enumerate(tscv.split(X)):\n",
    "    X_train, X_test = X[train_idx], X[test_idx]\n",
    "    y_train, y_test = y[train_idx], y[test_idx]\n",
    "    \n",
    "    # Train model\n",
    "    rf = RandomForestRegressor(n_estimators=100, random_state=42, n_jobs=-1)\n",
    "    rf.fit(X_train, y_train)\n",
    "    \n",
    "    # Predict\n",
    "    y_pred = rf.predict(X_test)\n",
    "    \n",
    "    # Metrics\n",
    "    rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    \n",
    "    # Direction accuracy\n",
    "    y_test_diff = np.diff(y_test)\n",
    "    y_pred_diff = np.diff(y_pred)\n",
    "    dir_acc = np.mean(np.sign(y_test_diff) == np.sign(y_pred_diff)) * 100\n",
    "    \n",
    "    cv_results.append({'fold': fold+1, 'rmse': rmse, 'mae': mae, 'dir_acc': dir_acc})\n",
    "    print(f\"Fold {fold+1}: Train={len(train_idx):5d}, Test={len(test_idx):4d} | RMSE={rmse:10.2f}, Dir.Acc={dir_acc:.1f}%\")\n",
    "\n",
    "# Summary\n",
    "results_df = pd.DataFrame(cv_results)\n",
    "print(\"\\n\" + \"-\"*60)\n",
    "print(f\"Average RMSE: {results_df['rmse'].mean():.2f} ± {results_df['rmse'].std():.2f}\")\n",
    "print(f\"Average Direction Accuracy: {results_df['dir_acc'].mean():.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Importance Analysis\n",
    "rf_final = RandomForestRegressor(n_estimators=100, random_state=42, n_jobs=-1)\n",
    "rf_final.fit(X, y)\n",
    "\n",
    "importance_df = pd.DataFrame({\n",
    "    'feature': feature_cols,\n",
    "    'importance': rf_final.feature_importances_\n",
    "}).sort_values('importance', ascending=True)\n",
    "\n",
    "# Plot top 15 features\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "top_features = importance_df.tail(15)\n",
    "colors = [COLORS['blue'] if 'lag' in f else COLORS['orange'] for f in top_features['feature']]\n",
    "ax.barh(top_features['feature'], top_features['importance'], color=colors)\n",
    "ax.set_xlabel('Importance')\n",
    "ax.set_title('Top 15 Feature Importances\\n(Blue=Lag, Orange=Rolling)', fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nInsight: Lag features (especially lag_1) are most important.\")\n",
    "print(\"Rolling statistics add value for capturing trends.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 5: Simple LSTM Implementation\n",
    "\n",
    "**Task:** Build a basic LSTM model for price prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try to import TensorFlow\n",
    "try:\n",
    "    import tensorflow as tf\n",
    "    from tensorflow.keras.models import Sequential\n",
    "    from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "    from tensorflow.keras.callbacks import EarlyStopping\n",
    "    HAS_TF = True\n",
    "    print(f\"TensorFlow version: {tf.__version__}\")\n",
    "except ImportError:\n",
    "    HAS_TF = False\n",
    "    print(\"TensorFlow not installed. LSTM exercises will be skipped.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if HAS_TF:\n",
    "    # Prepare data for LSTM\n",
    "    def create_sequences(data, seq_length):\n",
    "        \"\"\"Create sequences for LSTM input.\"\"\"\n",
    "        X, y = [], []\n",
    "        for i in range(seq_length, len(data)):\n",
    "            X.append(data[i-seq_length:i, 0])\n",
    "            y.append(data[i, 0])\n",
    "        return np.array(X), np.array(y)\n",
    "    \n",
    "    # Scale data\n",
    "    scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "    price_scaled = scaler.fit_transform(price.reshape(-1, 1))\n",
    "    \n",
    "    # Create sequences\n",
    "    SEQ_LENGTH = 30\n",
    "    X_lstm, y_lstm = create_sequences(price_scaled, SEQ_LENGTH)\n",
    "    \n",
    "    # Reshape for LSTM [samples, timesteps, features]\n",
    "    X_lstm = X_lstm.reshape((X_lstm.shape[0], X_lstm.shape[1], 1))\n",
    "    \n",
    "    # Train/test split (time-based)\n",
    "    train_size = int(len(X_lstm) * 0.8)\n",
    "    X_train_lstm = X_lstm[:train_size]\n",
    "    y_train_lstm = y_lstm[:train_size]\n",
    "    X_test_lstm = X_lstm[train_size:]\n",
    "    y_test_lstm = y_lstm[train_size:]\n",
    "    \n",
    "    print(f\"LSTM Input shape: {X_lstm.shape}\")\n",
    "    print(f\"Training samples: {len(X_train_lstm)}\")\n",
    "    print(f\"Test samples: {len(X_test_lstm)}\")\n",
    "else:\n",
    "    print(\"Skipping LSTM - TensorFlow not available\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if HAS_TF:\n",
    "    # Build LSTM model\n",
    "    model = Sequential([\n",
    "        LSTM(50, return_sequences=True, input_shape=(SEQ_LENGTH, 1)),\n",
    "        Dropout(0.2),\n",
    "        LSTM(50, return_sequences=False),\n",
    "        Dropout(0.2),\n",
    "        Dense(25),\n",
    "        Dense(1)\n",
    "    ])\n",
    "    \n",
    "    model.compile(optimizer='adam', loss='mse')\n",
    "    model.summary()\n",
    "    \n",
    "    # Early stopping\n",
    "    early_stop = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "    \n",
    "    # Train\n",
    "    print(\"\\nTraining LSTM...\")\n",
    "    history = model.fit(\n",
    "        X_train_lstm, y_train_lstm,\n",
    "        epochs=50,\n",
    "        batch_size=32,\n",
    "        validation_split=0.1,\n",
    "        callbacks=[early_stop],\n",
    "        verbose=1\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if HAS_TF:\n",
    "    # Evaluate LSTM\n",
    "    y_pred_lstm = model.predict(X_test_lstm, verbose=0)\n",
    "    \n",
    "    # Inverse transform\n",
    "    y_test_actual = scaler.inverse_transform(y_test_lstm.reshape(-1, 1)).flatten()\n",
    "    y_pred_actual = scaler.inverse_transform(y_pred_lstm).flatten()\n",
    "    \n",
    "    # Metrics\n",
    "    rmse_lstm = np.sqrt(mean_squared_error(y_test_actual, y_pred_actual))\n",
    "    mae_lstm = mean_absolute_error(y_test_actual, y_pred_actual)\n",
    "    \n",
    "    # Direction accuracy\n",
    "    y_test_diff = np.diff(y_test_actual)\n",
    "    y_pred_diff = np.diff(y_pred_actual)\n",
    "    dir_acc_lstm = np.mean(np.sign(y_test_diff) == np.sign(y_pred_diff)) * 100\n",
    "    \n",
    "    print(\"LSTM Results\")\n",
    "    print(\"=\" * 40)\n",
    "    print(f\"RMSE: {rmse_lstm:.2f}\")\n",
    "    print(f\"MAE: {mae_lstm:.2f}\")\n",
    "    print(f\"Direction Accuracy: {dir_acc_lstm:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if HAS_TF:\n",
    "    # Plot predictions\n",
    "    fig, axes = plt.subplots(2, 1, figsize=(14, 8))\n",
    "    \n",
    "    # Full test period\n",
    "    axes[0].plot(y_test_actual, color=COLORS['blue'], label='Actual', linewidth=1)\n",
    "    axes[0].plot(y_pred_actual, color=COLORS['red'], label='LSTM Predicted', linewidth=1, alpha=0.8)\n",
    "    axes[0].set_title('LSTM Predictions vs Actual (Test Set)', fontweight='bold')\n",
    "    axes[0].legend()\n",
    "    axes[0].set_ylabel('Price ($)')\n",
    "    \n",
    "    # Last 100 days detail\n",
    "    axes[1].plot(y_test_actual[-100:], color=COLORS['blue'], label='Actual', linewidth=1.5)\n",
    "    axes[1].plot(y_pred_actual[-100:], color=COLORS['red'], label='LSTM Predicted', linewidth=1.5, alpha=0.8)\n",
    "    axes[1].set_title('Last 100 Days (Detail)', fontweight='bold')\n",
    "    axes[1].legend()\n",
    "    axes[1].set_xlabel('Days')\n",
    "    axes[1].set_ylabel('Price ($)')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 6: Model Comparison\n",
    "\n",
    "**Task:** Compare Random Forest vs LSTM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Model Comparison: Random Forest vs LSTM\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"{'Metric':<25} {'Random Forest':>15} {'LSTM':>15}\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "# Use last fold for RF comparison\n",
    "rf_rmse = results_df['rmse'].iloc[-1]\n",
    "rf_dir = results_df['dir_acc'].iloc[-1]\n",
    "\n",
    "print(f\"{'RMSE':<25} {rf_rmse:>15.2f}\", end='')\n",
    "if HAS_TF:\n",
    "    print(f\" {rmse_lstm:>15.2f}\")\n",
    "else:\n",
    "    print(f\" {'N/A':>15}\")\n",
    "\n",
    "print(f\"{'Direction Accuracy (%)':<25} {rf_dir:>15.1f}\", end='')\n",
    "if HAS_TF:\n",
    "    print(f\" {dir_acc_lstm:>15.1f}\")\n",
    "else:\n",
    "    print(f\" {'N/A':>15}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"\\nKey Insights:\")\n",
    "print(\"• Random Forest: Fast, interpretable, good with engineered features\")\n",
    "print(\"• LSTM: Better for complex patterns, needs more data and tuning\")\n",
    "print(\"• Direction accuracy ~50% suggests markets are nearly efficient\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Practice Problems\n",
    "\n",
    "### Problem 1: Hurst Exponent Interpretation\n",
    "\n",
    "You estimate Hurst exponent for three series:\n",
    "- Series A: H = 0.72\n",
    "- Series B: H = 0.48  \n",
    "- Series C: H = 0.31\n",
    "\n",
    "**Question:** For which series would momentum strategies work best?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Problem 1 Solution\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "series = {'A': 0.72, 'B': 0.48, 'C': 0.31}\n",
    "\n",
    "for name, h in series.items():\n",
    "    if h > 0.5:\n",
    "        behavior = \"Persistent (trending)\"\n",
    "        strategy = \"Momentum ✓\"\n",
    "    elif h < 0.5:\n",
    "        behavior = \"Anti-persistent (mean-reverting)\"\n",
    "        strategy = \"Mean-reversion\"\n",
    "    else:\n",
    "        behavior = \"Random walk\"\n",
    "        strategy = \"None (unpredictable)\"\n",
    "    \n",
    "    print(f\"Series {name}: H = {h} → {behavior}\")\n",
    "    print(f\"           Best strategy: {strategy}\\n\")\n",
    "\n",
    "print(\"Answer: Series A (H = 0.72) - strongest persistence\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 2: Feature Selection\n",
    "\n",
    "You have these features:\n",
    "- lag_1, lag_2, lag_3\n",
    "- rolling_mean_7, rolling_std_7\n",
    "- future_return (tomorrow's return)\n",
    "\n",
    "**Question:** Which feature should NOT be included in training?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Problem 2 Solution\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "print(\"Answer: future_return should NOT be included!\")\n",
    "print(\"\\nReason: DATA LEAKAGE\")\n",
    "print(\"• future_return contains information from the future\")\n",
    "print(\"• Including it would artificially inflate model performance\")\n",
    "print(\"• In real trading, we don't know tomorrow's return today\")\n",
    "print(\"\\nValid features use only past information:\")\n",
    "print(\"• lag_1, lag_2, lag_3 ✓ (past values)\")\n",
    "print(\"• rolling_mean_7, rolling_std_7 ✓ (past statistics)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 3: LSTM Architecture\n",
    "\n",
    "Your LSTM has:\n",
    "- Input: sequences of 60 days\n",
    "- Output: predicting price 5 days ahead\n",
    "\n",
    "**Question:** What shape should the output layer have?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Problem 3 Solution\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "print(\"For multi-step prediction (5 days ahead):\")\n",
    "print(\"\\nOption 1: Direct Multi-Output\")\n",
    "print(\"  Dense(5) - one output per day\")\n",
    "print(\"  Predicts: [day+1, day+2, day+3, day+4, day+5]\")\n",
    "print(\"\\nOption 2: Recursive\")\n",
    "print(\"  Dense(1) - predict day+1\")\n",
    "print(\"  Feed prediction back as input, repeat 5 times\")\n",
    "print(\"\\nOption 3: Sequence-to-Sequence\")\n",
    "print(\"  LSTM encoder → LSTM decoder\")\n",
    "print(\"  Output sequence of length 5\")\n",
    "\n",
    "print(\"\\nRecommendation: Direct Multi-Output (Dense(5)) is simplest\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "1. **Long Memory Detection**\n",
    "   - Hurst exponent: H > 0.5 = trending, H < 0.5 = mean-reverting\n",
    "   - Returns: ~0.5 (no memory), Volatility: ~0.7-0.8 (long memory)\n",
    "\n",
    "2. **Feature Engineering**\n",
    "   - Lag features capture autocorrelation\n",
    "   - Rolling statistics capture trends\n",
    "   - Always use shifted values to avoid leakage\n",
    "\n",
    "3. **Time Series Cross-Validation**\n",
    "   - Never use standard k-fold\n",
    "   - TimeSeriesSplit maintains temporal order\n",
    "   - Walk-forward validation for realistic evaluation\n",
    "\n",
    "4. **Model Selection**\n",
    "   - Random Forest: Fast, interpretable, good baseline\n",
    "   - LSTM: Complex patterns, more data needed\n",
    "   - Direction accuracy near 50% suggests market efficiency\n",
    "\n",
    "### Practical Workflow\n",
    "1. Test for long memory (Hurst exponent)\n",
    "2. Engineer appropriate features\n",
    "3. Use time series cross-validation\n",
    "4. Compare multiple models\n",
    "5. Evaluate with appropriate metrics (RMSE, direction accuracy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}