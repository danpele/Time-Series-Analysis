{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/danpele/Time-Series-Analysis/blob/main/chapter8_seminar_notebook.ipynb)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 8 Seminar: Modern Extensions - Practice\n",
    "\n",
    "**Course:** Time Series Analysis and Forecasting  \n",
    "**Program:** Bachelor program, Faculty of Cybernetics, Statistics and Economic Informatics, Bucharest University of Economic Studies, Romania  \n",
    "**Academic Year:** 2025-2026\n",
    "\n",
    "---\n",
    "\n",
    "## Seminar Objectives\n",
    "\n",
    "In this practical seminar, you will:\n",
    "1. Estimate the Hurst exponent and identify long memory\n",
    "2. Apply feature engineering for time series ML\n",
    "3. Build and evaluate Random Forest models\n",
    "4. Implement simple LSTM networks\n",
    "5. Compare classical vs ML approaches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages (for Colab)\n",
    "import sys\n",
    "if 'google.colab' in sys.modules:\n",
    "    !pip install nolds arch yfinance -q\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "try:\n",
    "    import nolds\n",
    "    HAS_NOLDS = True\n",
    "except ImportError:\n",
    "    HAS_NOLDS = False\n",
    "    print(\"nolds not installed - using manual Hurst calculation\")\n",
    "\n",
    "try:\n",
    "    import yfinance as yf\n",
    "    HAS_YF = True\n",
    "except ImportError:\n",
    "    HAS_YF = False\n",
    "\n",
    "plt.rcParams['figure.figsize'] = (12, 5)\n",
    "plt.rcParams['font.size'] = 11\n",
    "plt.rcParams['axes.facecolor'] = 'none'\n",
    "plt.rcParams['figure.facecolor'] = 'none'\n",
    "plt.rcParams['axes.grid'] = False\n",
    "plt.rcParams['axes.spines.top'] = False\n",
    "plt.rcParams['axes.spines.right'] = False\n",
    "plt.rcParams['legend.frameon'] = False\n",
    "\n",
    "COLORS = {'blue': '#1A3A6E', 'red': '#DC3545', 'green': '#2E7D32', 'orange': '#E67E22', 'gray': '#666666'}\n",
    "\n",
    "print(\"Setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1: Hurst Exponent Estimation\n",
    "\n",
    "**Task:** Estimate the Hurst exponent for different types of series and interpret the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hurst_rs(series, min_window=10):\n",
    "    \"\"\"Calculate Hurst exponent using R/S analysis.\"\"\"\n",
    "    n = len(series)\n",
    "    max_k = int(np.floor(n / min_window))\n",
    "    \n",
    "    rs_values = []\n",
    "    n_values = []\n",
    "    \n",
    "    for k in range(2, max_k + 1):\n",
    "        size = n // k\n",
    "        rs_k = []\n",
    "        \n",
    "        for i in range(k):\n",
    "            subseries = series[i * size:(i + 1) * size]\n",
    "            mean_sub = np.mean(subseries)\n",
    "            cumdev = np.cumsum(subseries - mean_sub)\n",
    "            R = np.max(cumdev) - np.min(cumdev)\n",
    "            S = np.std(subseries, ddof=1)\n",
    "            if S > 0:\n",
    "                rs_k.append(R / S)\n",
    "        \n",
    "        if rs_k:\n",
    "            rs_values.append(np.mean(rs_k))\n",
    "            n_values.append(size)\n",
    "    \n",
    "    log_n = np.log(n_values)\n",
    "    log_rs = np.log(rs_values)\n",
    "    \n",
    "    slope, _ = np.polyfit(log_n, log_rs, 1)\n",
    "    return slope\n",
    "\n",
    "# Generate different types of series\n",
    "np.random.seed(42)\n",
    "n = 1000\n",
    "\n",
    "# 1. Random walk (H ≈ 0.5)\n",
    "random_walk = np.cumsum(np.random.randn(n))\n",
    "\n",
    "# 2. Mean-reverting (H < 0.5)\n",
    "mean_rev = np.zeros(n)\n",
    "for i in range(1, n):\n",
    "    mean_rev[i] = -0.5 * mean_rev[i-1] + np.random.randn()\n",
    "\n",
    "# 3. Trending/Persistent (H > 0.5)\n",
    "persistent = np.zeros(n)\n",
    "for i in range(1, n):\n",
    "    persistent[i] = 0.7 * persistent[i-1] + np.sign(persistent[i-1]) * 0.3 + np.random.randn() * 0.5\n",
    "persistent = np.cumsum(persistent)\n",
    "\n",
    "print(\"Hurst Exponent Estimation\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "series_dict = {\n",
    "    'Random Walk': random_walk,\n",
    "    'Mean-Reverting': mean_rev,\n",
    "    'Persistent': persistent\n",
    "}\n",
    "\n",
    "for name, series in series_dict.items():\n",
    "    h = hurst_rs(series)\n",
    "    if HAS_NOLDS:\n",
    "        h_nolds = nolds.hurst_rs(series)\n",
    "        print(f\"{name:20s}: H = {h:.3f} (manual), {h_nolds:.3f} (nolds)\")\n",
    "    else:\n",
    "        print(f\"{name:20s}: H = {h:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Visualize the three types of series\nfig, axes = plt.subplots(1, 3, figsize=(15, 5))\n\nh_vals = {}\nfor ax, (name, series) in zip(axes, series_dict.items()):\n    h = hurst_rs(series)\n    h_vals[name] = h\n    \n    if h < 0.5:\n        color = COLORS['green']\n        behavior = 'Mean-reverting'\n    elif h > 0.5:\n        color = COLORS['red']\n        behavior = 'Trending'\n    else:\n        color = COLORS['gray']\n        behavior = 'Random Walk'\n    \n    ax.plot(series, color=color, linewidth=0.8, label=f'H = {h:.2f}')\n    ax.set_title(f\"{name}\\n({behavior})\", fontweight='bold')\n    ax.set_xlabel('Time')\n    ax.legend(loc='upper center', bbox_to_anchor=(0.5, -0.12), ncol=1, frameon=False)\n\nplt.tight_layout()\nplt.subplots_adjust(bottom=0.18)\nplt.show()\n\nprint(\"\\nInterpretation Guide:\")\nprint(\"  H < 0.5: Anti-persistent (mean-reverting) - good for mean-reversion strategies\")\nprint(\"  H = 0.5: Random walk (no memory) - unpredictable\")\nprint(\"  H > 0.5: Persistent (trending) - good for momentum strategies\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2: Long Memory in Real Financial Data\n",
    "\n",
    "**Task:** Download Bitcoin data and test for long memory in returns vs volatility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Get Bitcoin data\nif HAS_YF:\n    btc = yf.download('BTC-USD', start='2020-01-01', end='2024-12-31', progress=False)\n    # Handle MultiIndex columns from newer yfinance versions\n    if isinstance(btc.columns, pd.MultiIndex):\n        btc.columns = btc.columns.droplevel(1)\n    # Ensure we get a Series and extract values\n    close_prices = btc['Close'].squeeze()\n    price = close_prices.dropna().values\n    returns = np.diff(np.log(price)) * 100  # Log returns in %\nelse:\n    # Simulate if no yfinance\n    np.random.seed(123)\n    n = 1000\n    returns = np.random.randn(n) * 3  # Simulate returns\n    # Add volatility clustering\n    vol = np.ones(n)\n    for i in range(1, n):\n        vol[i] = 0.1 + 0.85 * vol[i-1] + 0.1 * returns[i-1]**2\n    returns = returns * np.sqrt(vol)\n    price = 10000 * np.exp(np.cumsum(returns / 100))\n\nprint(f\"Data loaded: {len(returns)} observations\")\n\n# Calculate volatility proxies\nabs_returns = np.abs(returns)\nsquared_returns = returns ** 2\n\nprint(\"\\nLong Memory Analysis: Returns vs Volatility\")\nprint(\"=\" * 60)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare Hurst exponents\n",
    "h_returns = hurst_rs(returns)\n",
    "h_abs = hurst_rs(abs_returns)\n",
    "h_sq = hurst_rs(squared_returns)\n",
    "\n",
    "print(f\"{'Series':<25} {'Hurst':>10} {'Memory Type':>20}\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "for name, h in [('Returns', h_returns), ('|Returns|', h_abs), ('Returns²', h_sq)]:\n",
    "    if h < 0.45:\n",
    "        mem_type = 'Anti-persistent'\n",
    "    elif h > 0.55:\n",
    "        mem_type = 'Long memory (persistent)'\n",
    "    else:\n",
    "        mem_type = 'Short memory (~random)'\n",
    "    print(f\"{name:<25} {h:>10.3f} {mem_type:>20}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"KEY FINDING: Volatility has long memory, returns do not!\")\n",
    "print(\"This is a stylized fact used in FIGARCH models.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Visualize ACF decay comparison\nfrom statsmodels.tsa.stattools import acf\n\nmax_lag = 100\nacf_returns = acf(returns, nlags=max_lag, fft=True)\nacf_abs = acf(abs_returns, nlags=max_lag, fft=True)\n\nfig, axes = plt.subplots(1, 2, figsize=(14, 5))\n\n# Returns ACF (fast decay)\naxes[0].bar(range(max_lag+1), acf_returns, color=COLORS['blue'], alpha=0.7, width=0.8, label='ACF values')\naxes[0].axhline(y=1.96/np.sqrt(len(returns)), color='red', linestyle='--', alpha=0.5, label='95% confidence')\naxes[0].axhline(y=-1.96/np.sqrt(len(returns)), color='red', linestyle='--', alpha=0.5)\naxes[0].set_title(f'ACF of Returns (H = {h_returns:.3f})\\nFast Decay = Short Memory', fontweight='bold')\naxes[0].set_xlabel('Lag')\naxes[0].set_xlim(-1, max_lag)\naxes[0].legend(loc='upper center', bbox_to_anchor=(0.5, -0.12), ncol=2, frameon=False)\n\n# Absolute returns ACF (slow decay)\naxes[1].bar(range(max_lag+1), acf_abs, color=COLORS['red'], alpha=0.7, width=0.8, label='ACF values')\naxes[1].axhline(y=1.96/np.sqrt(len(abs_returns)), color='red', linestyle='--', alpha=0.5, label='95% confidence')\naxes[1].axhline(y=-1.96/np.sqrt(len(abs_returns)), color='red', linestyle='--', alpha=0.5)\naxes[1].set_title(f'ACF of |Returns| (H = {h_abs:.3f})\\nSlow Decay = Long Memory', fontweight='bold')\naxes[1].set_xlabel('Lag')\naxes[1].set_xlim(-1, max_lag)\naxes[1].legend(loc='upper center', bbox_to_anchor=(0.5, -0.12), ncol=2, frameon=False)\n\nplt.tight_layout()\nplt.subplots_adjust(bottom=0.18)\nplt.show()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3: Feature Engineering for Time Series ML\n",
    "\n",
    "**Task:** Create lag features and rolling statistics for ML models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def create_features(series, lags=7, rolling_windows=[7, 14, 30]):\n    \"\"\"\n    Create lag and rolling features for time series ML.\n    \n    Parameters:\n    - series: 1D array of values (returns, not prices!)\n    - lags: number of lag features\n    - rolling_windows: list of window sizes for rolling stats\n    \n    Returns:\n    - DataFrame with features\n    \"\"\"\n    df = pd.DataFrame({'y': series})\n    \n    # Lag features\n    for i in range(1, lags + 1):\n        df[f'lag_{i}'] = df['y'].shift(i)\n    \n    # Rolling statistics (on past data only - shift by 1!)\n    for w in rolling_windows:\n        df[f'rolling_mean_{w}'] = df['y'].shift(1).rolling(window=w).mean()\n        df[f'rolling_std_{w}'] = df['y'].shift(1).rolling(window=w).std()\n    \n    # Target: current value (we predict y from lagged features)\n    df['target'] = df['y']\n    \n    # Drop rows with NaN\n    df = df.dropna()\n    \n    return df\n\n# IMPORTANT: Use RETURNS, not prices!\n# Predicting price levels with ML is meaningless\nfeatures_df = create_features(returns, lags=7, rolling_windows=[7, 14, 30])\nprint(f\"Feature DataFrame shape: {features_df.shape}\")\nprint(f\"\\nFeatures created:\")\nprint([c for c in features_df.columns if c not in ['y', 'target']])"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Display sample of features\nprint(\"Sample of engineered features (RETURNS data):\")\ndisplay_cols = ['y', 'lag_1', 'lag_2', 'rolling_mean_7', 'rolling_std_7', 'target']\nprint(features_df[display_cols].head(10).round(4))\n\nprint(\"\\nNote: We predict RETURNS, not prices!\")\nprint(\"- Returns are stationary (or nearly so)\")\nprint(\"- RMSE will be in percentage points\")\nprint(\"- Direction accuracy ~50% means market is efficient\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4: Random Forest with Time Series Cross-Validation\n",
    "\n",
    "**Task:** Train Random Forest and evaluate using TimeSeriesSplit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data\n",
    "feature_cols = [c for c in features_df.columns if c not in ['y', 'target']]\n",
    "X = features_df[feature_cols].values\n",
    "y = features_df['target'].values\n",
    "\n",
    "print(f\"Features: {len(feature_cols)}\")\n",
    "print(f\"Samples: {len(X)}\")\n",
    "\n",
    "# Time Series Cross-Validation\n",
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "\n",
    "# Store results\n",
    "cv_results = []\n",
    "\n",
    "print(\"\\nTime Series Cross-Validation\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for fold, (train_idx, test_idx) in enumerate(tscv.split(X)):\n",
    "    X_train, X_test = X[train_idx], X[test_idx]\n",
    "    y_train, y_test = y[train_idx], y[test_idx]\n",
    "    \n",
    "    # Train model\n",
    "    rf = RandomForestRegressor(n_estimators=100, random_state=42, n_jobs=-1)\n",
    "    rf.fit(X_train, y_train)\n",
    "    \n",
    "    # Predict\n",
    "    y_pred = rf.predict(X_test)\n",
    "    \n",
    "    # Metrics\n",
    "    rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    \n",
    "    # Direction accuracy\n",
    "    y_test_diff = np.diff(y_test)\n",
    "    y_pred_diff = np.diff(y_pred)\n",
    "    dir_acc = np.mean(np.sign(y_test_diff) == np.sign(y_pred_diff)) * 100\n",
    "    \n",
    "    cv_results.append({'fold': fold+1, 'rmse': rmse, 'mae': mae, 'dir_acc': dir_acc})\n",
    "    print(f\"Fold {fold+1}: Train={len(train_idx):5d}, Test={len(test_idx):4d} | RMSE={rmse:10.2f}, Dir.Acc={dir_acc:.1f}%\")\n",
    "\n",
    "# Summary\n",
    "results_df = pd.DataFrame(cv_results)\n",
    "print(\"\\n\" + \"-\"*60)\n",
    "print(f\"Average RMSE: {results_df['rmse'].mean():.2f} ± {results_df['rmse'].std():.2f}\")\n",
    "print(f\"Average Direction Accuracy: {results_df['dir_acc'].mean():.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Feature Importance Analysis\nrf_final = RandomForestRegressor(n_estimators=100, random_state=42, n_jobs=-1)\nrf_final.fit(X, y)\n\nimportance_df = pd.DataFrame({\n    'feature': feature_cols,\n    'importance': rf_final.feature_importances_\n}).sort_values('importance', ascending=True)\n\n# Plot top 15 features\nfig, ax = plt.subplots(figsize=(10, 6))\ntop_features = importance_df.tail(15)\ncolors = [COLORS['blue'] if 'lag' in f else COLORS['orange'] for f in top_features['feature']]\nbars = ax.barh(top_features['feature'], top_features['importance'], color=colors)\n\n# Create legend handles\nfrom matplotlib.patches import Patch\nlegend_elements = [Patch(facecolor=COLORS['blue'], label='Lag features'),\n                   Patch(facecolor=COLORS['orange'], label='Rolling features')]\nax.legend(handles=legend_elements, loc='upper center', bbox_to_anchor=(0.5, -0.08), ncol=2, frameon=False)\n\nax.set_xlabel('Importance')\nax.set_title('Top 15 Feature Importances', fontweight='bold')\nplt.tight_layout()\nplt.subplots_adjust(bottom=0.15)\nplt.show()\n\nprint(\"\\nInsight: Lag features (especially lag_1) are most important.\")\nprint(\"Rolling statistics add value for capturing trends.\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 5: Simple LSTM Implementation\n",
    "\n",
    "**Task:** Build a basic LSTM model for price prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try to import TensorFlow\n",
    "try:\n",
    "    import tensorflow as tf\n",
    "    from tensorflow.keras.models import Sequential\n",
    "    from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "    from tensorflow.keras.callbacks import EarlyStopping\n",
    "    HAS_TF = True\n",
    "    print(f\"TensorFlow version: {tf.__version__}\")\n",
    "except ImportError:\n",
    "    HAS_TF = False\n",
    "    print(\"TensorFlow not installed. LSTM exercises will be skipped.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "if HAS_TF:\n    # Prepare data for LSTM - use RETURNS, not prices!\n    def create_sequences(data, seq_length):\n        \"\"\"Create sequences for LSTM input.\"\"\"\n        X, y = [], []\n        for i in range(seq_length, len(data)):\n            X.append(data[i-seq_length:i, 0])\n            y.append(data[i, 0])\n        return np.array(X), np.array(y)\n    \n    # Scale RETURNS data\n    scaler = MinMaxScaler(feature_range=(-1, 1))\n    returns_scaled = scaler.fit_transform(returns.reshape(-1, 1))\n    \n    # Create sequences\n    SEQ_LENGTH = 30\n    X_lstm, y_lstm = create_sequences(returns_scaled, SEQ_LENGTH)\n    \n    # Reshape for LSTM [samples, timesteps, features]\n    X_lstm = X_lstm.reshape((X_lstm.shape[0], X_lstm.shape[1], 1))\n    \n    # Train/test split (time-based)\n    train_size = int(len(X_lstm) * 0.8)\n    X_train_lstm = X_lstm[:train_size]\n    y_train_lstm = y_lstm[:train_size]\n    X_test_lstm = X_lstm[train_size:]\n    y_test_lstm = y_lstm[train_size:]\n    \n    print(\"LSTM Data Preparation (using RETURNS)\")\n    print(\"=\" * 50)\n    print(f\"Input shape: {X_lstm.shape} (samples, timesteps, features)\")\n    print(f\"Training samples: {len(X_train_lstm)}\")\n    print(f\"Test samples: {len(X_test_lstm)}\")\nelse:\n    print(\"Skipping LSTM - TensorFlow not available\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if HAS_TF:\n",
    "    # Build LSTM model\n",
    "    model = Sequential([\n",
    "        LSTM(50, return_sequences=True, input_shape=(SEQ_LENGTH, 1)),\n",
    "        Dropout(0.2),\n",
    "        LSTM(50, return_sequences=False),\n",
    "        Dropout(0.2),\n",
    "        Dense(25),\n",
    "        Dense(1)\n",
    "    ])\n",
    "    \n",
    "    model.compile(optimizer='adam', loss='mse')\n",
    "    model.summary()\n",
    "    \n",
    "    # Early stopping\n",
    "    early_stop = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "    \n",
    "    # Train\n",
    "    print(\"\\nTraining LSTM...\")\n",
    "    history = model.fit(\n",
    "        X_train_lstm, y_train_lstm,\n",
    "        epochs=50,\n",
    "        batch_size=32,\n",
    "        validation_split=0.1,\n",
    "        callbacks=[early_stop],\n",
    "        verbose=1\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "if HAS_TF:\n    # Evaluate LSTM on returns\n    y_pred_lstm = model.predict(X_test_lstm, verbose=0)\n    \n    # Inverse transform to get returns in %\n    y_test_actual = scaler.inverse_transform(y_test_lstm.reshape(-1, 1)).flatten()\n    y_pred_actual = scaler.inverse_transform(y_pred_lstm).flatten()\n    \n    # Metrics\n    rmse_lstm = np.sqrt(mean_squared_error(y_test_actual, y_pred_actual))\n    mae_lstm = mean_absolute_error(y_test_actual, y_pred_actual)\n    \n    # Direction accuracy\n    y_test_diff = np.sign(y_test_actual)  # Sign of actual return\n    y_pred_diff = np.sign(y_pred_actual)  # Sign of predicted return\n    dir_acc_lstm = np.mean(y_test_diff == y_pred_diff) * 100\n    \n    print(\"LSTM Results (predicting RETURNS)\")\n    print(\"=\" * 50)\n    print(f\"RMSE: {rmse_lstm:.4f} (percentage points)\")\n    print(f\"MAE: {mae_lstm:.4f} (percentage points)\")\n    print(f\"Direction Accuracy: {dir_acc_lstm:.1f}%\")\n    print(f\"\\nNote: Direction accuracy ~50% is expected for efficient markets\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "if HAS_TF:\n    # Plot predictions (returns)\n    fig, axes = plt.subplots(2, 1, figsize=(14, 8))\n    \n    # Last 200 days\n    n_plot = min(200, len(y_test_actual))\n    axes[0].plot(y_test_actual[-n_plot:], color=COLORS['blue'], label='Actual Returns', linewidth=1)\n    axes[0].plot(y_pred_actual[-n_plot:], color=COLORS['red'], label='LSTM Predicted', linewidth=1, alpha=0.7)\n    axes[0].axhline(y=0, color='gray', linestyle='--', alpha=0.5)\n    axes[0].set_title(f'LSTM: Actual vs Predicted Returns (last {n_plot} days)', fontweight='bold')\n    axes[0].set_ylabel('Return (%)')\n    axes[0].legend(loc='upper center', bbox_to_anchor=(0.5, -0.08), ncol=2, frameon=False)\n    \n    # Scatter plot\n    axes[1].scatter(y_test_actual, y_pred_actual, alpha=0.4, s=15, color=COLORS['blue'], label='Predictions')\n    axes[1].plot([y_test_actual.min(), y_test_actual.max()], \n                 [y_test_actual.min(), y_test_actual.max()], 'r--', linewidth=2, label='Perfect prediction')\n    axes[1].axhline(y=0, color='gray', linestyle=':', alpha=0.5)\n    axes[1].axvline(x=0, color='gray', linestyle=':', alpha=0.5)\n    axes[1].set_xlabel('Actual Return (%)')\n    axes[1].set_ylabel('Predicted Return (%)')\n    axes[1].set_title('LSTM: Scatter Plot', fontweight='bold')\n    axes[1].legend(loc='upper center', bbox_to_anchor=(0.5, -0.12), ncol=2, frameon=False)\n    \n    plt.tight_layout()\n    plt.subplots_adjust(bottom=0.12, hspace=0.35)\n    plt.show()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 6: Model Comparison\n",
    "\n",
    "**Task:** Compare Random Forest vs LSTM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "print(\"Model Comparison: Random Forest vs LSTM (predicting RETURNS)\")\nprint(\"=\" * 60)\nprint(f\"{'Metric':<25} {'Random Forest':>15} {'LSTM':>15}\")\nprint(\"-\" * 60)\n\n# Use last fold for RF comparison\nrf_rmse = results_df['rmse'].iloc[-1]\nrf_dir = results_df['dir_acc'].iloc[-1]\n\nprint(f\"{'RMSE (% points)':<25} {rf_rmse:>15.4f}\", end='')\nif HAS_TF:\n    print(f\" {rmse_lstm:>15.4f}\")\nelse:\n    print(f\" {'N/A':>15}\")\n\nprint(f\"{'Direction Accuracy (%)':<25} {rf_dir:>15.1f}\", end='')\nif HAS_TF:\n    print(f\" {dir_acc_lstm:>15.1f}\")\nelse:\n    print(f\" {'N/A':>15}\")\n\nprint(\"\\n\" + \"=\" * 60)\nprint(\"\\nKey Insights:\")\nprint(\"• RMSE ~3-4% is typical for daily return predictions\")\nprint(\"• Direction accuracy ~50% confirms market efficiency\")\nprint(\"• Models struggle to beat naive (predict 0) for returns\")\nprint(\"• This is EXPECTED - if returns were predictable, arbitrage would eliminate it\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Practice Problems\n",
    "\n",
    "### Problem 1: Hurst Exponent Interpretation\n",
    "\n",
    "You estimate Hurst exponent for three series:\n",
    "- Series A: H = 0.72\n",
    "- Series B: H = 0.48  \n",
    "- Series C: H = 0.31\n",
    "\n",
    "**Question:** For which series would momentum strategies work best?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Problem 1 Solution\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "series = {'A': 0.72, 'B': 0.48, 'C': 0.31}\n",
    "\n",
    "for name, h in series.items():\n",
    "    if h > 0.5:\n",
    "        behavior = \"Persistent (trending)\"\n",
    "        strategy = \"Momentum ✓\"\n",
    "    elif h < 0.5:\n",
    "        behavior = \"Anti-persistent (mean-reverting)\"\n",
    "        strategy = \"Mean-reversion\"\n",
    "    else:\n",
    "        behavior = \"Random walk\"\n",
    "        strategy = \"None (unpredictable)\"\n",
    "    \n",
    "    print(f\"Series {name}: H = {h} → {behavior}\")\n",
    "    print(f\"           Best strategy: {strategy}\\n\")\n",
    "\n",
    "print(\"Answer: Series A (H = 0.72) - strongest persistence\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 2: Feature Selection\n",
    "\n",
    "You have these features:\n",
    "- lag_1, lag_2, lag_3\n",
    "- rolling_mean_7, rolling_std_7\n",
    "- future_return (tomorrow's return)\n",
    "\n",
    "**Question:** Which feature should NOT be included in training?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Problem 2 Solution\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "print(\"Answer: future_return should NOT be included!\")\n",
    "print(\"\\nReason: DATA LEAKAGE\")\n",
    "print(\"• future_return contains information from the future\")\n",
    "print(\"• Including it would artificially inflate model performance\")\n",
    "print(\"• In real trading, we don't know tomorrow's return today\")\n",
    "print(\"\\nValid features use only past information:\")\n",
    "print(\"• lag_1, lag_2, lag_3 ✓ (past values)\")\n",
    "print(\"• rolling_mean_7, rolling_std_7 ✓ (past statistics)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 3: LSTM Architecture\n",
    "\n",
    "Your LSTM has:\n",
    "- Input: sequences of 60 days\n",
    "- Output: predicting price 5 days ahead\n",
    "\n",
    "**Question:** What shape should the output layer have?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Problem 3 Solution\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "print(\"For multi-step prediction (5 days ahead):\")\n",
    "print(\"\\nOption 1: Direct Multi-Output\")\n",
    "print(\"  Dense(5) - one output per day\")\n",
    "print(\"  Predicts: [day+1, day+2, day+3, day+4, day+5]\")\n",
    "print(\"\\nOption 2: Recursive\")\n",
    "print(\"  Dense(1) - predict day+1\")\n",
    "print(\"  Feed prediction back as input, repeat 5 times\")\n",
    "print(\"\\nOption 3: Sequence-to-Sequence\")\n",
    "print(\"  LSTM encoder → LSTM decoder\")\n",
    "print(\"  Output sequence of length 5\")\n",
    "\n",
    "print(\"\\nRecommendation: Direct Multi-Output (Dense(5)) is simplest\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "1. **Long Memory Detection**\n",
    "   - Hurst exponent: H > 0.5 = trending, H < 0.5 = mean-reverting\n",
    "   - Returns: ~0.5 (no memory), Volatility: ~0.7-0.8 (long memory)\n",
    "\n",
    "2. **Feature Engineering**\n",
    "   - Lag features capture autocorrelation\n",
    "   - Rolling statistics capture trends\n",
    "   - Always use shifted values to avoid leakage\n",
    "\n",
    "3. **Time Series Cross-Validation**\n",
    "   - Never use standard k-fold\n",
    "   - TimeSeriesSplit maintains temporal order\n",
    "   - Walk-forward validation for realistic evaluation\n",
    "\n",
    "4. **Model Selection**\n",
    "   - Random Forest: Fast, interpretable, good baseline\n",
    "   - LSTM: Complex patterns, more data needed\n",
    "   - Direction accuracy near 50% suggests market efficiency\n",
    "\n",
    "### Practical Workflow\n",
    "1. Test for long memory (Hurst exponent)\n",
    "2. Engineer appropriate features\n",
    "3. Use time series cross-validation\n",
    "4. Compare multiple models\n",
    "5. Evaluate with appropriate metrics (RMSE, direction accuracy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}