{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/danpele/Time-Series-Analysis/blob/main/chapter10_lecture_notebook.ipynb)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-1",
   "metadata": {},
   "source": [
    "# Chapter 10: Comprehensive Review\n",
    "\n",
    "**Complete Time Series Analysis with Real Data**\n",
    "\n",
    "**Course:** Time Series Analysis and Forecasting  \n",
    "**Program:** Bachelor program, Faculty of Cybernetics, Statistics and Economic Informatics, Bucharest University of Economic Studies, Romania  \n",
    "**Academic Year:** 2025-2026\n",
    "\n",
    "---\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "This comprehensive review demonstrates the complete time series analysis workflow using **real data**:\n",
    "\n",
    "1. **Case Study 1: S&P 500** - Financial data analysis with ARIMA-GARCH\n",
    "2. **Case Study 2: Air Passengers** - Classical seasonal data with SARIMA and Prophet\n",
    "3. **Case Study 3: US Retail Sales** - Economic data with structural breaks\n",
    "\n",
    "We will apply ALL methods covered in the course:\n",
    "- Stationarity testing (ADF, KPSS)\n",
    "- ARIMA/SARIMA modeling\n",
    "- GARCH for volatility\n",
    "- Prophet and TBATS\n",
    "- Model comparison and evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-2",
   "metadata": {},
   "source": [
    "## Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages (for Colab)\n",
    "import sys\n",
    "if 'google.colab' in sys.modules:\n",
    "    !pip install prophet arch statsmodels -q\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Statistical tests and models\n",
    "from statsmodels.tsa.stattools import adfuller, kpss\n",
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "\n",
    "# GARCH\n",
    "try:\n",
    "    from arch import arch_model\n",
    "    HAS_ARCH = True\n",
    "except ImportError:\n",
    "    HAS_ARCH = False\n",
    "    print(\"arch not installed. Install with: pip install arch\")\n",
    "\n",
    "# Prophet\n",
    "try:\n",
    "    from prophet import Prophet\n",
    "    HAS_PROPHET = True\n",
    "except ImportError:\n",
    "    HAS_PROPHET = False\n",
    "    print(\"Prophet not installed. Install with: pip install prophet\")\n",
    "\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "\n",
    "# Plotting style\n",
    "plt.rcParams['figure.figsize'] = (14, 5)\n",
    "plt.rcParams['font.size'] = 11\n",
    "plt.rcParams['axes.facecolor'] = 'none'\n",
    "plt.rcParams['figure.facecolor'] = 'none'\n",
    "plt.rcParams['axes.grid'] = False\n",
    "plt.rcParams['axes.spines.top'] = False\n",
    "plt.rcParams['axes.spines.right'] = False\n",
    "plt.rcParams['legend.frameon'] = False\n",
    "\n",
    "COLORS = {'blue': '#1A3A6E', 'red': '#DC3545', 'green': '#2E7D32', 'orange': '#E67E22', 'gray': '#666666'}\n",
    "\n",
    "print(\"Setup complete!\")\n",
    "print(f\"ARCH/GARCH available: {HAS_ARCH}\")\n",
    "print(f\"Prophet available: {HAS_PROPHET}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-4",
   "metadata": {},
   "source": [
    "## Real Data Loading Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sp500_data():\n",
    "    \"\"\"S&P 500 simulated data based on real patterns (2019-2024)\"\"\"\n",
    "    np.random.seed(42)\n",
    "    dates = pd.date_range('2019-01-01', '2024-01-01', freq='B')\n",
    "    n = len(dates)\n",
    "    \n",
    "    price = 2500\n",
    "    prices = [price]\n",
    "    \n",
    "    for i in range(1, n):\n",
    "        date = dates[i]\n",
    "        # COVID crash\n",
    "        if pd.Timestamp('2020-02-20') <= date <= pd.Timestamp('2020-03-23'):\n",
    "            drift, vol = -0.015, 0.04\n",
    "        # COVID recovery\n",
    "        elif pd.Timestamp('2020-03-24') <= date <= pd.Timestamp('2020-08-01'):\n",
    "            drift, vol = 0.003, 0.025\n",
    "        # 2022 bear market\n",
    "        elif pd.Timestamp('2022-01-01') <= date <= pd.Timestamp('2022-10-01'):\n",
    "            drift, vol = -0.0005, 0.015\n",
    "        else:\n",
    "            drift, vol = 0.0003, 0.01\n",
    "        \n",
    "        ret = drift + vol * np.random.randn()\n",
    "        price = prices[-1] * (1 + ret)\n",
    "        prices.append(price)\n",
    "    \n",
    "    df = pd.DataFrame({'ds': dates, 'price': prices})\n",
    "    df['returns'] = df['price'].pct_change() * 100\n",
    "    return df\n",
    "\n",
    "\n",
    "def get_air_passengers():\n",
    "    \"\"\"Classic Air Passengers dataset (1949-1960)\"\"\"\n",
    "    data = [\n",
    "        112, 118, 132, 129, 121, 135, 148, 148, 136, 119, 104, 118,\n",
    "        115, 126, 141, 135, 125, 149, 170, 170, 158, 133, 114, 140,\n",
    "        145, 150, 178, 163, 172, 178, 199, 199, 184, 162, 146, 166,\n",
    "        171, 180, 193, 181, 183, 218, 230, 242, 209, 191, 172, 194,\n",
    "        196, 196, 236, 235, 229, 243, 264, 272, 237, 211, 180, 201,\n",
    "        204, 188, 235, 227, 234, 264, 302, 293, 259, 229, 203, 229,\n",
    "        242, 233, 267, 269, 270, 315, 364, 347, 312, 274, 237, 278,\n",
    "        284, 277, 317, 313, 318, 374, 413, 405, 355, 306, 271, 306,\n",
    "        315, 301, 356, 348, 355, 422, 465, 467, 404, 347, 305, 336,\n",
    "        340, 318, 362, 348, 363, 435, 491, 505, 404, 359, 310, 337,\n",
    "        360, 342, 406, 396, 420, 472, 548, 559, 463, 407, 362, 405,\n",
    "        417, 391, 419, 461, 472, 535, 622, 606, 508, 461, 390, 432\n",
    "    ]\n",
    "    dates = pd.date_range('1949-01-01', periods=len(data), freq='MS')\n",
    "    return pd.DataFrame({'ds': dates, 'y': data})\n",
    "\n",
    "\n",
    "def get_retail_sales():\n",
    "    \"\"\"US Retail Sales from FRED (2018-2023)\"\"\"\n",
    "    data = [\n",
    "        457.6, 459.1, 468.2, 469.2, 473.9, 477.6, 482.1, 483.0, 473.7, 476.2, 477.9, 502.7,\n",
    "        455.6, 459.8, 472.0, 470.5, 479.3, 480.7, 485.9, 488.6, 479.9, 483.6, 481.7, 516.0,\n",
    "        461.2, 461.5, 414.7, 384.9, 476.4, 509.3, 516.1, 521.7, 527.0, 524.7, 519.6, 553.3,\n",
    "        510.6, 507.4, 560.1, 561.1, 567.0, 574.0, 582.0, 585.0, 581.0, 596.1, 595.6, 630.1,\n",
    "        581.9, 587.8, 631.5, 613.8, 629.3, 633.0, 631.8, 638.7, 625.5, 641.0, 633.7, 671.9,\n",
    "        620.6, 624.0, 670.2, 656.5, 666.3, 670.1, 673.2, 679.3, 668.6, 686.1, 672.3, 724.5\n",
    "    ]\n",
    "    dates = pd.date_range('2018-01-01', periods=len(data), freq='MS')\n",
    "    return pd.DataFrame({'ds': dates, 'y': data})\n",
    "\n",
    "\n",
    "print(\"Data loading functions defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-6",
   "metadata": {},
   "source": [
    "---\n",
    "# Case Study 1: S&P 500 Financial Analysis\n",
    "\n",
    "Financial time series require special treatment:\n",
    "- Returns (not prices) for modeling\n",
    "- GARCH for volatility clustering\n",
    "- Often weak predictability in mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load S&P 500 data\n",
    "sp500 = get_sp500_data()\n",
    "\n",
    "print(\"S&P 500 Data Overview\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Period: {sp500['ds'].min().date()} to {sp500['ds'].max().date()}\")\n",
    "print(f\"Observations: {len(sp500)} trading days\")\n",
    "print(f\"\\nPrice Statistics:\")\n",
    "print(f\"  Min: ${sp500['price'].min():.2f}\")\n",
    "print(f\"  Max: ${sp500['price'].max():.2f}\")\n",
    "print(f\"\\nReturn Statistics:\")\n",
    "print(f\"  Mean: {sp500['returns'].mean():.4f}%\")\n",
    "print(f\"  Std: {sp500['returns'].std():.4f}%\")\n",
    "print(f\"  Skewness: {sp500['returns'].skew():.4f}\")\n",
    "print(f\"  Kurtosis: {sp500['returns'].kurtosis():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize prices and returns\n",
    "fig, axes = plt.subplots(2, 1, figsize=(14, 8))\n",
    "\n",
    "# Prices\n",
    "axes[0].plot(sp500['ds'], sp500['price'], color=COLORS['blue'], linewidth=1)\n",
    "axes[0].axvspan(pd.Timestamp('2020-02-20'), pd.Timestamp('2020-03-23'),\n",
    "                alpha=0.3, color=COLORS['red'], label='COVID-19 Crash')\n",
    "axes[0].set_title('S&P 500 Daily Prices (2019-2024)', fontweight='bold')\n",
    "axes[0].set_ylabel('Price')\n",
    "axes[0].legend(loc='upper left')\n",
    "\n",
    "# Returns\n",
    "axes[1].plot(sp500['ds'], sp500['returns'], color=COLORS['green'], linewidth=0.5)\n",
    "axes[1].axhline(y=0, color='black', linewidth=0.5, alpha=0.3)\n",
    "axes[1].set_title('S&P 500 Daily Returns (%)', fontweight='bold')\n",
    "axes[1].set_xlabel('Date')\n",
    "axes[1].set_ylabel('Return (%)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Key Observations:\")\n",
    "print(\"- COVID-19 crash (Feb-Mar 2020): Sharp decline\")\n",
    "print(\"- Volatility clustering: Large returns followed by large returns\")\n",
    "print(\"- Returns appear stationary, prices do not\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-9",
   "metadata": {},
   "source": [
    "## Step 1: Stationarity Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-10",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_stationarity(series, name):\n",
    "    \"\"\"Run ADF and KPSS tests\"\"\"\n",
    "    print(f\"\\nStationarity Tests for {name}\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    # ADF Test\n",
    "    adf_result = adfuller(series.dropna(), autolag='AIC')\n",
    "    print(f\"ADF Test:\")\n",
    "    print(f\"  Statistic: {adf_result[0]:.4f}\")\n",
    "    print(f\"  p-value: {adf_result[1]:.4f}\")\n",
    "    print(f\"  Conclusion: {'Stationary' if adf_result[1] < 0.05 else 'Non-stationary'}\")\n",
    "    \n",
    "    # KPSS Test\n",
    "    kpss_result = kpss(series.dropna(), regression='c', nlags='auto')\n",
    "    print(f\"\\nKPSS Test:\")\n",
    "    print(f\"  Statistic: {kpss_result[0]:.4f}\")\n",
    "    print(f\"  p-value: {kpss_result[1]:.4f}\")\n",
    "    print(f\"  Conclusion: {'Stationary' if kpss_result[1] > 0.05 else 'Non-stationary'}\")\n",
    "    \n",
    "    return adf_result[1] < 0.05 and kpss_result[1] > 0.05\n",
    "\n",
    "# Test prices\n",
    "prices_stationary = test_stationarity(sp500['price'], 'Prices')\n",
    "\n",
    "# Test returns\n",
    "returns_stationary = test_stationarity(sp500['returns'].dropna(), 'Returns')\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"CONCLUSION: Use RETURNS for modeling (stationary)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-11",
   "metadata": {},
   "source": [
    "## Step 2: ACF/PACF Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-12",
   "metadata": {},
   "outputs": [],
   "source": [
    "returns = sp500['returns'].dropna()\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 8))\n",
    "\n",
    "# ACF/PACF for returns\n",
    "plot_acf(returns, ax=axes[0, 0], lags=30, alpha=0.05)\n",
    "axes[0, 0].set_title('ACF: Returns', fontweight='bold')\n",
    "\n",
    "plot_pacf(returns, ax=axes[0, 1], lags=30, alpha=0.05)\n",
    "axes[0, 1].set_title('PACF: Returns', fontweight='bold')\n",
    "\n",
    "# ACF/PACF for squared returns (volatility)\n",
    "plot_acf(returns**2, ax=axes[1, 0], lags=30, alpha=0.05)\n",
    "axes[1, 0].set_title('ACF: Squared Returns (Volatility)', fontweight='bold')\n",
    "\n",
    "plot_pacf(returns**2, ax=axes[1, 1], lags=30, alpha=0.05)\n",
    "axes[1, 1].set_title('PACF: Squared Returns (Volatility)', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Key Findings:\")\n",
    "print(\"- Returns: Near white noise (weak autocorrelation)\")\n",
    "print(\"- Squared returns: Strong persistence → GARCH needed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-13",
   "metadata": {},
   "source": [
    "## Step 3: ARIMA-GARCH Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-14",
   "metadata": {},
   "outputs": [],
   "source": [
    "if HAS_ARCH:\n",
    "    # Fit GARCH(1,1) model\n",
    "    print(\"Fitting GARCH(1,1) Model\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    model = arch_model(returns, vol='Garch', p=1, q=1, mean='AR', lags=1)\n",
    "    results = model.fit(disp='off')\n",
    "    \n",
    "    print(results.summary())\n",
    "else:\n",
    "    print(\"ARCH package not available. Install with: pip install arch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-15",
   "metadata": {},
   "outputs": [],
   "source": [
    "if HAS_ARCH:\n",
    "    # Plot conditional volatility\n",
    "    fig, axes = plt.subplots(2, 1, figsize=(14, 8))\n",
    "    \n",
    "    # Returns with volatility bands\n",
    "    cond_vol = results.conditional_volatility\n",
    "    axes[0].plot(sp500['ds'][1:], returns, color=COLORS['blue'], linewidth=0.5, alpha=0.7)\n",
    "    axes[0].plot(sp500['ds'][1:], 2*cond_vol, color=COLORS['red'], linewidth=1, label='+2σ')\n",
    "    axes[0].plot(sp500['ds'][1:], -2*cond_vol, color=COLORS['red'], linewidth=1, label='-2σ')\n",
    "    axes[0].axhline(y=0, color='black', linewidth=0.5, alpha=0.3)\n",
    "    axes[0].set_title('S&P 500 Returns with GARCH(1,1) Volatility Bands', fontweight='bold')\n",
    "    axes[0].set_ylabel('Return (%)')\n",
    "    axes[0].legend()\n",
    "    \n",
    "    # Conditional volatility\n",
    "    axes[1].fill_between(sp500['ds'][1:], 0, cond_vol, color=COLORS['orange'], alpha=0.7)\n",
    "    axes[1].set_title('GARCH(1,1) Conditional Volatility', fontweight='bold')\n",
    "    axes[1].set_xlabel('Date')\n",
    "    axes[1].set_ylabel('Volatility (σ)')\n",
    "    \n",
    "    # Mark COVID\n",
    "    axes[1].axvspan(pd.Timestamp('2020-02-20'), pd.Timestamp('2020-04-30'),\n",
    "                    alpha=0.3, color=COLORS['red'])\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"GARCH Model Interpretation:\")\n",
    "    print(f\"- α (ARCH): {results.params['alpha[1]']:.4f}\")\n",
    "    print(f\"- β (GARCH): {results.params['beta[1]']:.4f}\")\n",
    "    print(f\"- Persistence (α+β): {results.params['alpha[1]'] + results.params['beta[1]']:.4f}\")\n",
    "    print(\"\\nHigh persistence indicates volatility shocks are long-lasting.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-16",
   "metadata": {},
   "source": [
    "---\n",
    "# Case Study 2: Air Passengers with Seasonality\n",
    "\n",
    "The classic dataset demonstrating:\n",
    "- Trend + Seasonality\n",
    "- Multiplicative pattern\n",
    "- SARIMA vs Prophet comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Air Passengers\n",
    "air = get_air_passengers()\n",
    "\n",
    "print(\"Air Passengers Data Overview\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Period: {air['ds'].min().strftime('%Y-%m')} to {air['ds'].max().strftime('%Y-%m')}\")\n",
    "print(f\"Observations: {len(air)} months\")\n",
    "print(f\"\\nStatistics:\")\n",
    "print(f\"  Mean: {air['y'].mean():.1f}\")\n",
    "print(f\"  Std: {air['y'].std():.1f}\")\n",
    "print(f\"  Min: {air['y'].min():.0f} ({air.loc[air['y'].idxmin(), 'ds'].strftime('%Y-%m')})\")\n",
    "print(f\"  Max: {air['y'].max():.0f} ({air.loc[air['y'].idxmax(), 'ds'].strftime('%Y-%m')})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize with decomposition\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# Original series\n",
    "axes[0, 0].plot(air['ds'], air['y'], color=COLORS['blue'], linewidth=1.5)\n",
    "axes[0, 0].set_title('Air Passengers (1949-1960)', fontweight='bold')\n",
    "axes[0, 0].set_ylabel('Passengers (thousands)')\n",
    "\n",
    "# Decomposition\n",
    "air_series = pd.Series(air['y'].values, index=air['ds'])\n",
    "decomposition = seasonal_decompose(air_series, model='multiplicative', period=12)\n",
    "\n",
    "axes[0, 1].plot(air['ds'], decomposition.trend, color=COLORS['green'], linewidth=2)\n",
    "axes[0, 1].set_title('Trend Component', fontweight='bold')\n",
    "\n",
    "axes[1, 0].plot(air['ds'], decomposition.seasonal, color=COLORS['orange'], linewidth=1)\n",
    "axes[1, 0].axhline(y=1, color='black', linewidth=0.5, linestyle='--')\n",
    "axes[1, 0].set_title('Seasonal Component (Multiplicative)', fontweight='bold')\n",
    "\n",
    "axes[1, 1].plot(air['ds'], decomposition.resid, color=COLORS['red'], linewidth=1)\n",
    "axes[1, 1].axhline(y=1, color='black', linewidth=0.5, linestyle='--')\n",
    "axes[1, 1].set_title('Residual Component', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Multiplicative Model: Y = Trend × Seasonal × Residual\")\n",
    "print(\"Peak season: July-August (vacation travel)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-19",
   "metadata": {},
   "source": [
    "## SARIMA Model Fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data\n",
    "y = air['y'].values\n",
    "train_size = 120  # 10 years training\n",
    "train, test = y[:train_size], y[train_size:]\n",
    "test_dates = air['ds'][train_size:]\n",
    "\n",
    "# Fit SARIMA model on log-transformed data\n",
    "log_train = np.log(train)\n",
    "\n",
    "print(\"Fitting SARIMA(2,1,1)(0,1,1)12 Model\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "sarima_model = SARIMAX(log_train, order=(2, 1, 1), seasonal_order=(0, 1, 1, 12))\n",
    "sarima_results = sarima_model.fit(disp=False)\n",
    "\n",
    "print(sarima_results.summary().tables[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forecast\n",
    "sarima_forecast_log = sarima_results.get_forecast(steps=len(test))\n",
    "sarima_forecast = np.exp(sarima_forecast_log.predicted_mean)\n",
    "sarima_ci = np.exp(sarima_forecast_log.conf_int())\n",
    "\n",
    "# Metrics\n",
    "sarima_rmse = np.sqrt(mean_squared_error(test, sarima_forecast))\n",
    "sarima_mape = np.mean(np.abs((test - sarima_forecast) / test)) * 100\n",
    "\n",
    "print(f\"\\nSARIMA Forecast Performance:\")\n",
    "print(f\"  RMSE: {sarima_rmse:.2f}\")\n",
    "print(f\"  MAPE: {sarima_mape:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-22",
   "metadata": {},
   "source": [
    "## Prophet Model for Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-23",
   "metadata": {},
   "outputs": [],
   "source": [
    "if HAS_PROPHET:\n",
    "    # Prepare data for Prophet\n",
    "    train_prophet = pd.DataFrame({\n",
    "        'ds': air['ds'][:train_size],\n",
    "        'y': train\n",
    "    })\n",
    "    \n",
    "    # Fit Prophet\n",
    "    print(\"Fitting Prophet Model\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    prophet_model = Prophet(\n",
    "        yearly_seasonality=True,\n",
    "        weekly_seasonality=False,\n",
    "        daily_seasonality=False,\n",
    "        seasonality_mode='multiplicative'\n",
    "    )\n",
    "    prophet_model.fit(train_prophet)\n",
    "    \n",
    "    # Forecast\n",
    "    future = prophet_model.make_future_dataframe(periods=len(test), freq='MS')\n",
    "    prophet_forecast_df = prophet_model.predict(future)\n",
    "    prophet_forecast = prophet_forecast_df['yhat'].iloc[-len(test):].values\n",
    "    \n",
    "    # Metrics\n",
    "    prophet_rmse = np.sqrt(mean_squared_error(test, prophet_forecast))\n",
    "    prophet_mape = np.mean(np.abs((test - prophet_forecast) / test)) * 100\n",
    "    \n",
    "    print(f\"\\nProphet Forecast Performance:\")\n",
    "    print(f\"  RMSE: {prophet_rmse:.2f}\")\n",
    "    print(f\"  MAPE: {prophet_mape:.2f}%\")\n",
    "else:\n",
    "    print(\"Prophet not available\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare SARIMA and Prophet\n",
    "fig, axes = plt.subplots(2, 1, figsize=(14, 8))\n",
    "\n",
    "# Forecasts\n",
    "axes[0].plot(air['ds'][:train_size], train, color=COLORS['blue'], linewidth=1, label='Training')\n",
    "axes[0].plot(test_dates, test, color=COLORS['blue'], linewidth=1.5, label='Actual')\n",
    "axes[0].plot(test_dates, sarima_forecast, color=COLORS['green'], linewidth=1.5,\n",
    "             linestyle='--', label=f'SARIMA (RMSE={sarima_rmse:.1f})')\n",
    "\n",
    "if HAS_PROPHET:\n",
    "    axes[0].plot(test_dates, prophet_forecast, color=COLORS['orange'], linewidth=1.5,\n",
    "                 linestyle=':', label=f'Prophet (RMSE={prophet_rmse:.1f})')\n",
    "\n",
    "axes[0].axvline(x=air['ds'].iloc[train_size], color='black', linestyle=':', alpha=0.5)\n",
    "axes[0].set_title('Air Passengers: SARIMA vs Prophet Forecast', fontweight='bold')\n",
    "axes[0].set_ylabel('Passengers (thousands)')\n",
    "axes[0].legend(loc='upper left')\n",
    "\n",
    "# Forecast errors\n",
    "sarima_errors = test - sarima_forecast\n",
    "axes[1].bar(test_dates, sarima_errors, width=20, alpha=0.7, color=COLORS['green'], label='SARIMA')\n",
    "\n",
    "if HAS_PROPHET:\n",
    "    prophet_errors = test - prophet_forecast\n",
    "    axes[1].bar(test_dates + pd.Timedelta(days=10), prophet_errors, width=20, \n",
    "                alpha=0.7, color=COLORS['orange'], label='Prophet')\n",
    "\n",
    "axes[1].axhline(y=0, color='black', linewidth=0.5)\n",
    "axes[1].set_title('Forecast Errors', fontweight='bold')\n",
    "axes[1].set_xlabel('Date')\n",
    "axes[1].set_ylabel('Error')\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-25",
   "metadata": {},
   "source": [
    "---\n",
    "# Case Study 3: US Retail Sales with Structural Break\n",
    "\n",
    "Real-world challenge: COVID-19 created a massive structural break in economic data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Retail Sales\n",
    "retail = get_retail_sales()\n",
    "\n",
    "print(\"US Retail Sales Data Overview\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Period: {retail['ds'].min().strftime('%Y-%m')} to {retail['ds'].max().strftime('%Y-%m')}\")\n",
    "print(f\"Observations: {len(retail)} months\")\n",
    "print(f\"\\nKey Events:\")\n",
    "print(f\"  - COVID-19 Impact: March-April 2020\")\n",
    "print(f\"  - Minimum: ${retail['y'].min():.1f}B (April 2020)\")\n",
    "print(f\"  - Maximum: ${retail['y'].max():.1f}B (December 2023)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize with COVID impact\n",
    "fig, ax = plt.subplots(figsize=(14, 6))\n",
    "\n",
    "ax.plot(retail['ds'], retail['y'], color=COLORS['blue'], linewidth=1.5)\n",
    "ax.axvspan(pd.Timestamp('2020-03-01'), pd.Timestamp('2020-05-01'),\n",
    "           alpha=0.3, color=COLORS['red'], label='COVID-19 Impact')\n",
    "\n",
    "# Pre-COVID trend\n",
    "pre_covid = retail[retail['ds'] < '2020-03-01']\n",
    "z1 = np.polyfit(range(len(pre_covid)), pre_covid['y'], 1)\n",
    "ax.plot(pre_covid['ds'], np.polyval(z1, range(len(pre_covid))),\n",
    "        color=COLORS['gray'], linewidth=2, linestyle='--', label='Pre-COVID Trend')\n",
    "\n",
    "# Post-COVID trend\n",
    "post_covid = retail[retail['ds'] >= '2020-06-01']\n",
    "z2 = np.polyfit(range(len(post_covid)), post_covid['y'], 1)\n",
    "ax.plot(post_covid['ds'], np.polyval(z2, range(len(post_covid))),\n",
    "        color=COLORS['green'], linewidth=2, linestyle='--', label='Post-COVID Trend')\n",
    "\n",
    "ax.set_title('US Retail Sales (2018-2023): COVID-19 Structural Break', fontweight='bold')\n",
    "ax.set_xlabel('Date')\n",
    "ax.set_ylabel('Sales ($ billions)')\n",
    "ax.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nChallenge: Traditional ARIMA struggles with structural breaks.\")\n",
    "print(\"Solution: Prophet's changepoint detection or use post-COVID data only.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-28",
   "metadata": {},
   "outputs": [],
   "source": [
    "if HAS_PROPHET:\n",
    "    # Prophet handles structural breaks well\n",
    "    retail_prophet = retail.copy()\n",
    "    \n",
    "    # Split data\n",
    "    train_retail = retail_prophet.iloc[:-12]\n",
    "    test_retail = retail_prophet.iloc[-12:]\n",
    "    \n",
    "    # Fit Prophet with flexible changepoints\n",
    "    print(\"Fitting Prophet with Changepoint Detection\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    prophet_retail = Prophet(\n",
    "        changepoint_prior_scale=0.1,  # More flexible for COVID\n",
    "        seasonality_mode='multiplicative',\n",
    "        yearly_seasonality=True\n",
    "    )\n",
    "    prophet_retail.fit(train_retail)\n",
    "    \n",
    "    # Forecast\n",
    "    future_retail = prophet_retail.make_future_dataframe(periods=12, freq='MS')\n",
    "    forecast_retail = prophet_retail.predict(future_retail)\n",
    "    \n",
    "    # Evaluate\n",
    "    pred_retail = forecast_retail['yhat'].iloc[-12:].values\n",
    "    retail_rmse = np.sqrt(mean_squared_error(test_retail['y'], pred_retail))\n",
    "    retail_mape = np.mean(np.abs((test_retail['y'].values - pred_retail) / test_retail['y'].values)) * 100\n",
    "    \n",
    "    print(f\"\\nProphet Forecast Performance:\")\n",
    "    print(f\"  RMSE: ${retail_rmse:.2f}B\")\n",
    "    print(f\"  MAPE: {retail_mape:.2f}%\")\n",
    "    \n",
    "    # Plot\n",
    "    fig = prophet_retail.plot_components(forecast_retail)\n",
    "    plt.suptitle('Prophet Components: US Retail Sales', fontweight='bold', y=1.02)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-29",
   "metadata": {},
   "source": [
    "---\n",
    "# Summary: Model Selection Guide"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-30",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\"\"\n",
    "╔══════════════════════════════════════════════════════════════════════╗\n",
    "║                    TIME SERIES MODEL SELECTION GUIDE                  ║\n",
    "╠══════════════════════════════════════════════════════════════════════╣\n",
    "║                                                                      ║\n",
    "║  DATA TYPE              RECOMMENDED MODEL         ALTERNATIVES       ║\n",
    "║  ──────────────────────────────────────────────────────────────────  ║\n",
    "║                                                                      ║\n",
    "║  Financial returns      ARIMA-GARCH               EGARCH, GJR-GARCH  ║\n",
    "║  (volatility)                                                        ║\n",
    "║                                                                      ║\n",
    "║  Single seasonality     SARIMA                    ETS, Prophet       ║\n",
    "║  (monthly, quarterly)                                                ║\n",
    "║                                                                      ║\n",
    "║  Multiple seasonality   Prophet, TBATS            Dynamic regression ║\n",
    "║  (daily+weekly+annual)                                               ║\n",
    "║                                                                      ║\n",
    "║  Structural breaks      Prophet                   Piecewise models   ║\n",
    "║  (COVID, regime change)                                              ║\n",
    "║                                                                      ║\n",
    "║  Multiple time series   VAR, VECM                 Factor models      ║\n",
    "║  (interdependencies)                                                 ║\n",
    "║                                                                      ║\n",
    "╠══════════════════════════════════════════════════════════════════════╣\n",
    "║                         WORKFLOW STEPS                               ║\n",
    "║  1. Visualize and explore the data                                   ║\n",
    "║  2. Test for stationarity (ADF, KPSS)                                ║\n",
    "║  3. Apply transformations if needed (log, diff)                      ║\n",
    "║  4. Identify patterns (ACF/PACF, decomposition)                      ║\n",
    "║  5. Fit candidate models                                             ║\n",
    "║  6. Check diagnostics (residuals, Ljung-Box)                         ║\n",
    "║  7. Compare with out-of-sample forecast                              ║\n",
    "║  8. Select best model for the task                                   ║\n",
    "╚══════════════════════════════════════════════════════════════════════╝\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final comparison table\n",
    "print(\"\\nCourse Case Studies Summary\")\n",
    "print(\"=\"*70)\n",
    "print(f\"{'Dataset':<20} {'Best Model':<20} {'RMSE':<15} {'MAPE (%)':<10}\")\n",
    "print(\"-\"*70)\n",
    "print(f\"{'S&P 500 Returns':<20} {'ARIMA-GARCH(1,1)':<20} {'N/A (vol)':<15} {'N/A':<10}\")\n",
    "print(f\"{'Air Passengers':<20} {'SARIMA(2,1,1)':<20} {sarima_rmse:<15.2f} {sarima_mape:<10.2f}\")\n",
    "\n",
    "if HAS_PROPHET:\n",
    "    print(f\"{'US Retail Sales':<20} {'Prophet':<20} {retail_rmse:<15.2f} {retail_mape:<10.2f}\")\n",
    "\n",
    "print(\"-\"*70)\n",
    "print(\"\\nKey Takeaways:\")\n",
    "print(\"• Financial data: Focus on volatility (GARCH), not mean prediction\")\n",
    "print(\"• Seasonal data: SARIMA works well for clean, single-season data\")\n",
    "print(\"• Structural breaks: Prophet's flexibility handles regime changes\")\n",
    "print(\"• Always validate with out-of-sample testing!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
