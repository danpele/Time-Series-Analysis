{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/danpele/Time-Series-Analysis/blob/main/chapter10_lecture_notebook.ipynb)\n\n---",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "# Chapter 10: Comprehensive Review\n\n**Complete Time Series Analysis Workflow**\n\n**Course:** Time Series Analysis and Forecasting  \n**Program:** Bachelor program, Faculty of Cybernetics, Statistics and Economic Informatics, Bucharest University of Economic Studies, Romania  \n**Academic Year:** 2025-2026\n\n---\n\n## Learning Objectives\n\nThis comprehensive review demonstrates the complete time series analysis workflow using **simulated data based on real-world patterns**:\n\n1. **Case Study 1: Bitcoin** - Cryptocurrency volatility analysis with ARIMA-GARCH\n2. **Case Study 2: Sunspots** - Long-cycle seasonal data (11-year Schwabe cycle)\n3. **Case Study 3: US Unemployment** - Economic data with COVID-19 structural break\n\nWe will apply ALL methods covered in the course:\n- Stationarity testing (ADF, KPSS)\n- ARIMA/SARIMA modeling\n- GARCH for volatility clustering\n- Prophet with changepoint detection\n- Fourier terms for long seasonality\n- Model comparison and evaluation\n\n**Note:** Data is simulated to replicate real-world patterns (regime changes, cycles, structural breaks) for educational purposes.",
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": "## Setup and Imports",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# Install required packages (for Colab)\nimport sys\nif 'google.colab' in sys.modules:\n    !pip install prophet arch statsmodels -q\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Statistical tests and models\nfrom statsmodels.tsa.stattools import adfuller, kpss\nfrom statsmodels.graphics.tsaplots import plot_acf, plot_pacf\nfrom statsmodels.tsa.arima.model import ARIMA\nfrom statsmodels.tsa.statespace.sarimax import SARIMAX\nfrom statsmodels.tsa.seasonal import seasonal_decompose\n\n# GARCH\ntry:\n    from arch import arch_model\n    HAS_ARCH = True\nexcept ImportError:\n    HAS_ARCH = False\n    print(\"arch not installed. Install with: pip install arch\")\n\n# Prophet\ntry:\n    from prophet import Prophet\n    HAS_PROPHET = True\nexcept ImportError:\n    HAS_PROPHET = False\n    print(\"Prophet not installed. Install with: pip install prophet\")\n\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error\n\n# Plotting style - CONSISTENT WITH OTHER CHAPTERS\nplt.rcParams['figure.figsize'] = (12, 5)\nplt.rcParams['font.size'] = 11\nplt.rcParams['axes.facecolor'] = 'none'\nplt.rcParams['figure.facecolor'] = 'none'\nplt.rcParams['axes.grid'] = False\nplt.rcParams['axes.spines.top'] = False\nplt.rcParams['axes.spines.right'] = False\nplt.rcParams['legend.frameon'] = False\n\nCOLORS = {'blue': '#1A3A6E', 'red': '#DC3545', 'green': '#2E7D32', 'orange': '#E67E22', 'gray': '#666666'}\n\nprint(\"Setup complete!\")\nprint(f\"ARCH/GARCH available: {HAS_ARCH}\")\nprint(f\"Prophet available: {HAS_PROPHET}\")",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "## Simulated Data Loading Functions\n\nThese functions generate data that mimics real-world patterns for educational purposes.",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "def get_bitcoin_data():\n    \"\"\"Simulated Bitcoin daily prices with regime-based volatility (2019-2024)\n    \n    Mimics real Bitcoin behavior: COVID crash, 2021 bull run, 2022 crypto winter.\n    \"\"\"\n    np.random.seed(42)\n    dates = pd.date_range('2019-01-01', '2024-01-01', freq='D')\n    n = len(dates)\n    \n    price = 3700  # Bitcoin starting price Jan 2019\n    prices = [price]\n    \n    for i in range(1, n):\n        date = dates[i]\n        \n        # 2019: Recovery from crypto winter\n        if date < pd.Timestamp('2020-01-01'):\n            drift, vol = 0.0008, 0.035\n        # Early 2020: Pre-COVID stability\n        elif date < pd.Timestamp('2020-03-01'):\n            drift, vol = 0.001, 0.03\n        # COVID crash (March 2020)\n        elif date < pd.Timestamp('2020-04-01'):\n            drift, vol = -0.02, 0.08\n        # 2020-2021: Bull run to ATH\n        elif date < pd.Timestamp('2021-11-01'):\n            drift, vol = 0.003, 0.04\n        # 2022: Crypto winter/bear market\n        elif date < pd.Timestamp('2023-01-01'):\n            drift, vol = -0.002, 0.045\n        # 2023: Recovery\n        else:\n            drift, vol = 0.0015, 0.025\n        \n        ret = drift + vol * np.random.randn()\n        price = max(prices[-1] * (1 + ret), 1000)  # Floor at $1000\n        prices.append(price)\n    \n    df = pd.DataFrame({'ds': dates, 'price': prices})\n    df['returns'] = df['price'].pct_change() * 100\n    return df\n\n\ndef get_sunspot_data():\n    \"\"\"Simulated monthly sunspot numbers with 11-year Schwabe cycle (1990-2023)\n    \n    Mimics real sunspot behavior: asymmetric cycles, varying amplitudes.\n    \"\"\"\n    np.random.seed(123)\n    dates = pd.date_range('1990-01-01', '2023-12-01', freq='MS')\n    n = len(dates)\n    \n    # 11-year (132 months) solar cycle\n    cycle_period = 132\n    sunspots = []\n    \n    for i in range(n):\n        # Base cycle (asymmetric: rapid rise, slow decline)\n        phase = (i % cycle_period) / cycle_period\n        if phase < 0.4:  # Rising phase (40% of cycle)\n            cycle_value = 150 * np.sin(np.pi * phase / 0.4) ** 1.5\n        else:  # Declining phase (60% of cycle)\n            cycle_value = 150 * np.sin(np.pi * (1 - (phase - 0.4) / 0.6)) ** 0.8\n        \n        # Add variations between cycles\n        cycle_num = i // cycle_period\n        cycle_amplitude = [1.0, 0.7, 1.2][cycle_num % 3]\n        cycle_value *= cycle_amplitude\n        \n        # Add noise\n        noise = np.random.exponential(10) * np.random.choice([-1, 1])\n        sunspots.append(max(0, cycle_value + noise))\n    \n    return pd.DataFrame({'ds': dates, 'y': sunspots})\n\n\ndef get_unemployment_data():\n    \"\"\"Simulated US Unemployment Rate with COVID-19 structural break (2015-2023)\n    \n    Mimics real unemployment: pre-COVID decline, April 2020 spike to 14.7%, recovery.\n    \"\"\"\n    np.random.seed(456)\n    dates = pd.date_range('2015-01-01', '2023-12-01', freq='MS')\n    n = len(dates)\n    \n    unemployment = []\n    \n    for i in range(n):\n        date = dates[i]\n        \n        # Pre-COVID trend (declining unemployment)\n        if date < pd.Timestamp('2020-03-01'):\n            base = 5.5 - (i / 60) * 2  # From 5.5% to ~3.5%\n            noise = np.random.normal(0, 0.1)\n            unemployment.append(max(3.5, base + noise))\n        # COVID spike (March-April 2020)\n        elif date < pd.Timestamp('2020-05-01'):\n            if date.month == 3:\n                unemployment.append(4.4)\n            else:\n                unemployment.append(14.7)  # Peak\n        # Rapid recovery (May-Dec 2020)\n        elif date < pd.Timestamp('2021-01-01'):\n            months_since_peak = (date.year - 2020) * 12 + date.month - 4\n            rate = 14.7 - months_since_peak * 1.2\n            unemployment.append(max(6.7, rate + np.random.normal(0, 0.2)))\n        # Continued recovery (2021-2022)\n        elif date < pd.Timestamp('2023-01-01'):\n            months_since_2021 = (date.year - 2021) * 12 + date.month\n            rate = 6.7 - months_since_2021 * 0.15\n            unemployment.append(max(3.5, rate + np.random.normal(0, 0.1)))\n        # Stabilization (2023)\n        else:\n            unemployment.append(3.5 + np.random.normal(0, 0.1))\n    \n    return pd.DataFrame({'ds': dates, 'y': unemployment})\n\n\nprint(\"Data loading functions defined (simulated data):\")\nprint(\"- Bitcoin: Extreme volatility, regime changes\")\nprint(\"- Sunspots: 11-year (132-month) seasonal cycle\")\nprint(\"- US Unemployment: COVID-19 structural break\")",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "---\n# Case Study 1: Bitcoin Volatility Analysis\n\nCryptocurrency data is characterized by:\n- **Extreme volatility** (much higher than traditional assets)\n- **Regime changes** (bull markets, crypto winters)\n- **Fat tails** (more extreme events than normal distribution)\n- **Volatility clustering** (GARCH is essential)",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# Load Bitcoin data\nbtc = get_bitcoin_data()\n\nprint(\"Bitcoin Data Overview (Simulated)\")\nprint(\"=\"*50)\nprint(f\"Period: {btc['ds'].min().date()} to {btc['ds'].max().date()}\")\nprint(f\"Observations: {len(btc)} days\")\nprint(f\"\\nPrice Statistics:\")\nprint(f\"  Min: ${btc['price'].min():,.2f}\")\nprint(f\"  Max: ${btc['price'].max():,.2f}\")\nprint(f\"\\nReturn Statistics (Daily %):\")\nprint(f\"  Mean: {btc['returns'].mean():.4f}%\")\nprint(f\"  Std: {btc['returns'].std():.4f}%\")\nprint(f\"  Skewness: {btc['returns'].skew():.4f}\")\nprint(f\"  Kurtosis: {btc['returns'].kurtosis():.4f}\")\nprint(f\"\\nNote: High kurtosis indicates fat tails (extreme events)\")",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "# Visualize Bitcoin prices and returns\nfig, axes = plt.subplots(2, 1, figsize=(12, 8))\n\n# Prices with regime annotations\naxes[0].plot(btc['ds'], btc['price'], color=COLORS['blue'], linewidth=1)\naxes[0].axvspan(pd.Timestamp('2020-03-01'), pd.Timestamp('2020-04-01'),\n                alpha=0.3, color=COLORS['red'], label='COVID-19 Crash')\naxes[0].axvspan(pd.Timestamp('2021-11-01'), pd.Timestamp('2022-12-31'),\n                alpha=0.2, color=COLORS['orange'], label='Crypto Winter 2022')\naxes[0].set_title('Bitcoin Daily Prices (2019-2024) - Simulated', fontweight='bold')\naxes[0].set_ylabel('Price (USD)')\naxes[0].set_yscale('log')  # Log scale for better visualization\naxes[0].legend(loc='upper center', bbox_to_anchor=(0.5, -0.12), ncol=2, frameon=False)\n\n# Returns\naxes[1].plot(btc['ds'], btc['returns'], color=COLORS['green'], linewidth=0.5)\naxes[1].axhline(y=0, color='black', linewidth=0.5, alpha=0.3)\naxes[1].set_title('Bitcoin Daily Returns (%)', fontweight='bold')\naxes[1].set_xlabel('Date')\naxes[1].set_ylabel('Return (%)')\n\nplt.tight_layout()\nplt.subplots_adjust(hspace=0.35)\nplt.show()\n\nprint(\"Key Observations:\")\nprint(\"- COVID-19 crash (March 2020): ~50% drop in weeks\")\nprint(\"- 2021 bull run to all-time highs (~$69K)\")\nprint(\"- 2022 crypto winter: Extended decline\")\nprint(\"- Clear volatility clustering: Large moves cluster together\")",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "## Step 1: Stationarity Testing",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "def test_stationarity(series, name):\n    \"\"\"Run ADF and KPSS tests\"\"\"\n    print(f\"\\nStationarity Tests for {name}\")\n    print(\"-\" * 40)\n    \n    # ADF Test (H0: unit root exists = non-stationary)\n    adf_result = adfuller(series.dropna(), autolag='AIC')\n    print(f\"ADF Test:\")\n    print(f\"  Statistic: {adf_result[0]:.4f}\")\n    print(f\"  p-value: {adf_result[1]:.4f}\")\n    print(f\"  Conclusion: {'Stationary' if adf_result[1] < 0.05 else 'Non-stationary'}\")\n    \n    # KPSS Test (H0: stationary)\n    kpss_result = kpss(series.dropna(), regression='c', nlags='auto')\n    print(f\"\\nKPSS Test:\")\n    print(f\"  Statistic: {kpss_result[0]:.4f}\")\n    print(f\"  p-value: {kpss_result[1]:.4f}\")\n    print(f\"  Conclusion: {'Stationary' if kpss_result[1] > 0.05 else 'Non-stationary'}\")\n    \n    return adf_result[1] < 0.05 and kpss_result[1] > 0.05\n\n# Test prices (expect non-stationary)\nprices_stationary = test_stationarity(btc['price'], 'Bitcoin Prices')\n\n# Test returns (expect stationary)\nreturns_stationary = test_stationarity(btc['returns'].dropna(), 'Bitcoin Returns')\n\nprint(\"\\n\" + \"=\"*50)\nprint(\"CONCLUSION: Use RETURNS for mean modeling (stationary)\")\nprint(\"But volatility of returns is NOT constant -> GARCH needed!\")",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "## Step 2: ACF/PACF Analysis",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "returns = btc['returns'].dropna()\n\nfig, axes = plt.subplots(2, 2, figsize=(12, 8))\n\n# ACF/PACF for returns (mean equation)\nplot_acf(returns, ax=axes[0, 0], lags=30, alpha=0.05)\naxes[0, 0].set_title('ACF: Returns (Mean)', fontweight='bold')\n\nplot_pacf(returns, ax=axes[0, 1], lags=30, alpha=0.05, method='ywm')\naxes[0, 1].set_title('PACF: Returns (Mean)', fontweight='bold')\n\n# ACF/PACF for squared returns (volatility)\nplot_acf(returns**2, ax=axes[1, 0], lags=30, alpha=0.05)\naxes[1, 0].set_title('ACF: Squared Returns (Volatility)', fontweight='bold')\n\nplot_pacf(returns**2, ax=axes[1, 1], lags=30, alpha=0.05, method='ywm')\naxes[1, 1].set_title('PACF: Squared Returns (Volatility)', fontweight='bold')\n\nplt.tight_layout()\nplt.show()\n\nprint(\"Key Findings:\")\nprint(\"- Returns: Weak autocorrelation (hard to predict mean)\")\nprint(\"- Squared returns: STRONG persistence -> GARCH essential!\")\nprint(\"- This is typical of financial returns: 'weak mean, strong variance'\")",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "## Step 3: ARIMA-GARCH Model\n\n**GARCH(1,1) Model:**\n$$\\sigma_t^2 = \\omega + \\alpha \\epsilon_{t-1}^2 + \\beta \\sigma_{t-1}^2$$\n\nWhere:\n- $\\omega$ = constant (long-run variance component)\n- $\\alpha$ = ARCH effect (reaction to recent shock)\n- $\\beta$ = GARCH effect (persistence of volatility)\n- $\\alpha + \\beta$ = persistence (how long shocks affect volatility)",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "if HAS_ARCH:\n    # Fit GARCH(1,1) model with AR(1) mean\n    print(\"Fitting GARCH(1,1) Model for Bitcoin\")\n    print(\"=\"*50)\n    \n    model = arch_model(returns, vol='Garch', p=1, q=1, mean='AR', lags=1)\n    results = model.fit(disp='off')\n    \n    print(results.summary())\nelse:\n    print(\"ARCH package not available. Install with: pip install arch\")",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "if HAS_ARCH:\n    # Plot conditional volatility\n    fig, axes = plt.subplots(2, 1, figsize=(12, 8))\n    \n    # Returns with volatility bands\n    cond_vol = results.conditional_volatility\n    dates = btc['ds'][1:]\n    \n    axes[0].plot(dates, returns, color=COLORS['blue'], linewidth=0.5, alpha=0.7, label='Returns')\n    axes[0].plot(dates, 2*cond_vol, color=COLORS['red'], linewidth=1, label='+2\\u03c3')\n    axes[0].plot(dates, -2*cond_vol, color=COLORS['red'], linewidth=1, label='-2\\u03c3')\n    axes[0].axhline(y=0, color='black', linewidth=0.5, alpha=0.3)\n    axes[0].set_title('Bitcoin Returns with GARCH(1,1) Volatility Bands', fontweight='bold')\n    axes[0].set_ylabel('Return (%)')\n    axes[0].legend(loc='upper center', bbox_to_anchor=(0.5, -0.12), ncol=3, frameon=False)\n    \n    # Conditional volatility\n    axes[1].fill_between(dates, 0, cond_vol, color=COLORS['orange'], alpha=0.7, label='Conditional Volatility')\n    axes[1].axvspan(pd.Timestamp('2020-03-01'), pd.Timestamp('2020-04-30'),\n                    alpha=0.3, color=COLORS['red'], label='COVID Crash')\n    axes[1].set_title('GARCH(1,1) Conditional Volatility', fontweight='bold')\n    axes[1].set_xlabel('Date')\n    axes[1].set_ylabel('Volatility (\\u03c3)')\n    axes[1].legend(loc='upper center', bbox_to_anchor=(0.5, -0.15), ncol=2, frameon=False)\n    \n    plt.tight_layout()\n    plt.subplots_adjust(hspace=0.35)\n    plt.show()\n    \n    # Interpret parameters\n    alpha = results.params.get('alpha[1]', 0)\n    beta = results.params.get('beta[1]', 0)\n    \n    print(\"\\nGARCH Model Interpretation:\")\n    print(f\"- \\u03b1 (ARCH): {alpha:.4f} - Reaction to recent shocks\")\n    print(f\"- \\u03b2 (GARCH): {beta:.4f} - Persistence of past volatility\")\n    print(f\"- Persistence (\\u03b1+\\u03b2): {alpha + beta:.4f}\")\n    print(\"\\nHigh persistence indicates volatility shocks are long-lasting.\")\n    if alpha + beta > 0.95:\n        print(\"Warning: Very high persistence - consider IGARCH or long-memory models.\")",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "---\n# Case Study 2: Sunspot Cycle Analysis\n\nSunspots demonstrate:\n- **Very long seasonality** (11-year Schwabe cycle = 132 months)\n- **Asymmetric pattern** (rapid rise, slow decline)\n- **Variable cycle amplitude**\n- Challenge: Standard SARIMA cannot handle 132-month seasonality",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# Load Sunspot data\nsunspots = get_sunspot_data()\n\nprint(\"Sunspot Data Overview (Simulated)\")\nprint(\"=\"*50)\nprint(f\"Period: {sunspots['ds'].min().strftime('%Y-%m')} to {sunspots['ds'].max().strftime('%Y-%m')}\")\nprint(f\"Observations: {len(sunspots)} months\")\nprint(f\"\\nStatistics:\")\nprint(f\"  Mean: {sunspots['y'].mean():.1f}\")\nprint(f\"  Std: {sunspots['y'].std():.1f}\")\nprint(f\"  Min: {sunspots['y'].min():.0f}\")\nprint(f\"  Max: {sunspots['y'].max():.0f}\")\nprint(f\"\\nKey Feature: 11-year (132-month) Schwabe solar cycle\")",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "# Visualize sunspot data\nfig, axes = plt.subplots(2, 1, figsize=(12, 8))\n\n# Time series\naxes[0].plot(sunspots['ds'], sunspots['y'], color=COLORS['blue'], linewidth=1, label='Sunspot Count')\naxes[0].set_title('Monthly Sunspot Numbers (1990-2023) - Simulated', fontweight='bold')\naxes[0].set_ylabel('Sunspot Count')\n\n# Mark solar cycles\nfor cycle_start in [1996, 2008, 2019]:\n    axes[0].axvline(pd.Timestamp(f'{cycle_start}-01-01'), color=COLORS['red'], \n                    linestyle='--', alpha=0.5, linewidth=1)\naxes[0].text(pd.Timestamp('1997-01-01'), sunspots['y'].max()*0.9, 'Cycle 23', fontsize=10)\naxes[0].text(pd.Timestamp('2009-01-01'), sunspots['y'].max()*0.9, 'Cycle 24', fontsize=10)\naxes[0].text(pd.Timestamp('2020-01-01'), sunspots['y'].max()*0.9, 'Cycle 25', fontsize=10)\n\n# Rolling statistics\nwindow = 24  # 2-year window\nrolling_mean = sunspots['y'].rolling(window=window).mean()\nrolling_std = sunspots['y'].rolling(window=window).std()\n\naxes[1].plot(sunspots['ds'], rolling_mean, color=COLORS['green'], \n             linewidth=2, label=f'{window}-month Rolling Mean')\naxes[1].fill_between(sunspots['ds'], rolling_mean - rolling_std, rolling_mean + rolling_std,\n                     color=COLORS['green'], alpha=0.2, label='\\u00b11 Std Dev')\naxes[1].set_title('Rolling Statistics', fontweight='bold')\naxes[1].set_xlabel('Date')\naxes[1].set_ylabel('Sunspot Count')\naxes[1].legend(loc='upper center', bbox_to_anchor=(0.5, -0.15), ncol=2, frameon=False)\n\nplt.tight_layout()\nplt.subplots_adjust(hspace=0.35)\nplt.show()\n\nprint(\"Key Observations:\")\nprint(\"- Clear 11-year periodicity (solar cycle)\")\nprint(\"- Asymmetric shape: rapid rise, gradual decline\")\nprint(\"- Cycle amplitudes vary (Cycle 24 was weaker)\")",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "## Decomposition and Spectral Analysis",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# Seasonal decomposition with period=132 (11 years)\nfig, axes = plt.subplots(2, 2, figsize=(12, 10))\n\n# Use custom period\nsunspot_series = pd.Series(sunspots['y'].values, index=sunspots['ds'])\n\n# FFT to find dominant period\nfrom scipy.fft import fft, fftfreq\n\ny = sunspots['y'].values - sunspots['y'].mean()\nn = len(y)\nyf = np.abs(fft(y))[:n//2]\nxf = fftfreq(n, 1)[:n//2]  # Monthly frequency\n\n# Convert to period in months\nperiod_months = 1 / xf[1:]  # Skip zero frequency\npower = yf[1:]\n\n# Plot spectrum\naxes[0, 0].plot(sunspots['ds'], sunspots['y'], color=COLORS['blue'], linewidth=0.8)\naxes[0, 0].set_title('Original Sunspot Series', fontweight='bold')\naxes[0, 0].set_ylabel('Sunspot Count')\n\naxes[0, 1].plot(period_months[:200], power[:200], color=COLORS['orange'], linewidth=1, label='Power Spectrum')\naxes[0, 1].axvline(x=132, color=COLORS['red'], linestyle='--', \n                   label='11-year cycle (132 months)')\naxes[0, 1].set_title('Power Spectrum (Periodogram)', fontweight='bold')\naxes[0, 1].set_xlabel('Period (months)')\naxes[0, 1].set_ylabel('Spectral Power')\naxes[0, 1].set_xlim([0, 200])\naxes[0, 1].legend(loc='upper center', bbox_to_anchor=(0.5, -0.15), ncol=2, frameon=False)\n\n# ACF showing long cycle\nplot_acf(sunspots['y'], ax=axes[1, 0], lags=150, alpha=0.05)\naxes[1, 0].axvline(x=66, color=COLORS['red'], linestyle='--', alpha=0.5)\naxes[1, 0].axvline(x=132, color=COLORS['red'], linestyle='--', alpha=0.5)\naxes[1, 0].set_title('ACF: Shows 132-month Periodicity', fontweight='bold')\n\nplot_pacf(sunspots['y'], ax=axes[1, 1], lags=50, alpha=0.05, method='ywm')\naxes[1, 1].set_title('PACF', fontweight='bold')\n\nplt.tight_layout()\nplt.show()\n\nprint(\"Spectral Analysis confirms:\")\nprint(\"- Dominant period around 132 months (11 years)\")\nprint(\"- ACF shows peak correlation at lag 132\")\nprint(\"- Challenge: Standard SARIMA can't handle such long seasonality\")",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "## Modeling Long Seasonality with Fourier Terms\n\nFor very long seasonal periods (132 months), we use **Fourier terms** instead of traditional SARIMA:\n\n$$y_t = \\sum_{k=1}^{K} \\left[ a_k \\sin\\left(\\frac{2\\pi k t}{m}\\right) + b_k \\cos\\left(\\frac{2\\pi k t}{m}\\right) \\right] + \\epsilon_t$$\n\nWhere $m$ = 132 (seasonal period) and $K$ = number of harmonics",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "def add_fourier_terms(df, period, K):\n    \"\"\"Add Fourier terms for seasonality\"\"\"\n    t = np.arange(len(df))\n    for k in range(1, K + 1):\n        df[f'sin_{k}'] = np.sin(2 * np.pi * k * t / period)\n        df[f'cos_{k}'] = np.cos(2 * np.pi * k * t / period)\n    return df\n\n# Prepare data with Fourier terms\nsunspot_model = sunspots.copy()\nsunspot_model = add_fourier_terms(sunspot_model, period=132, K=3)  # 3 harmonics\n\n# Train/test split\ntrain_size = len(sunspots) - 36  # Hold out last 3 years\ntrain = sunspot_model.iloc[:train_size]\ntest = sunspot_model.iloc[train_size:]\n\nprint(f\"Training: {train_size} months\")\nprint(f\"Testing: {len(test)} months\")\nprint(f\"\\nFourier terms added: sin_1, cos_1, sin_2, cos_2, sin_3, cos_3\")",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "# Fit ARIMA with Fourier regressors\nexog_cols = ['sin_1', 'cos_1', 'sin_2', 'cos_2', 'sin_3', 'cos_3']\n\nprint(\"Fitting ARIMA(2,0,1) with Fourier Regressors\")\nprint(\"=\"*50)\n\nmodel = SARIMAX(train['y'], \n                exog=train[exog_cols],\n                order=(2, 0, 1),\n                enforce_stationarity=False,\n                enforce_invertibility=False)\nresults = model.fit(disp=False)\n\nprint(results.summary().tables[1])",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "# Forecast\nforecast = results.get_forecast(steps=len(test), exog=test[exog_cols])\nforecast_mean = forecast.predicted_mean\nforecast_ci = forecast.conf_int()\n\n# Metrics\nrmse = np.sqrt(mean_squared_error(test['y'], forecast_mean))\nmape = np.mean(np.abs((test['y'].values - forecast_mean.values) / (test['y'].values + 1))) * 100\n\nprint(f\"\\nForecast Performance:\")\nprint(f\"  RMSE: {rmse:.2f}\")\nprint(f\"  MAPE: {mape:.2f}%\")\n\n# Plot forecast\nfig, ax = plt.subplots(figsize=(12, 6))\n\nax.plot(train['ds'], train['y'], color=COLORS['blue'], linewidth=1, label='Training')\nax.plot(test['ds'], test['y'], color=COLORS['blue'], linewidth=1.5, label='Actual')\nax.plot(test['ds'], forecast_mean, color=COLORS['red'], linewidth=1.5, \n        linestyle='--', label=f'Forecast (RMSE={rmse:.1f})')\nax.fill_between(test['ds'], forecast_ci.iloc[:, 0], forecast_ci.iloc[:, 1],\n                color=COLORS['red'], alpha=0.2, label='95% CI')\nax.axvline(x=train['ds'].iloc[-1], color='black', linestyle=':', alpha=0.5)\n\nax.set_title('Sunspot Forecast: ARIMA with Fourier Terms', fontweight='bold')\nax.set_xlabel('Date')\nax.set_ylabel('Sunspot Count')\nax.legend(loc='upper center', bbox_to_anchor=(0.5, -0.12), ncol=4, frameon=False)\n\nplt.tight_layout()\nplt.show()\n\nprint(\"\\nNote: Fourier terms capture the 11-year cycle effectively\")\nprint(\"without needing SARIMA(p,d,q)(P,D,Q)132 which would be computationally infeasible.\")",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "---\n# Case Study 3: US Unemployment with Structural Break\n\nThe COVID-19 pandemic created an unprecedented structural break:\n- Pre-COVID: 3.5% (50-year low)\n- Peak: 14.7% (April 2020)\n- Recovery: Back to ~3.5% by 2023\n\nThis challenges traditional time series models.",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# Load unemployment data\nunemp = get_unemployment_data()\n\nprint(\"US Unemployment Data Overview (Simulated)\")\nprint(\"=\"*50)\nprint(f\"Period: {unemp['ds'].min().strftime('%Y-%m')} to {unemp['ds'].max().strftime('%Y-%m')}\")\nprint(f\"Observations: {len(unemp)} months\")\nprint(f\"\\nKey Statistics:\")\nprint(f\"  Pre-COVID Min: {unemp[unemp['ds'] < '2020-03-01']['y'].min():.1f}% (Feb 2020)\")\nprint(f\"  COVID Peak: {unemp['y'].max():.1f}% (April 2020)\")\nprint(f\"  Latest: {unemp['y'].iloc[-1]:.1f}%\")\nprint(f\"\\nStructural Break: March-April 2020\")",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "# Visualize unemployment with COVID impact\nfig, axes = plt.subplots(2, 1, figsize=(12, 8))\n\n# Main plot\naxes[0].plot(unemp['ds'], unemp['y'], color=COLORS['blue'], linewidth=1.5, label='Unemployment Rate')\naxes[0].axvspan(pd.Timestamp('2020-03-01'), pd.Timestamp('2020-05-01'),\n                alpha=0.3, color=COLORS['red'], label='COVID-19 Shock')\n\n# Pre-COVID trend\npre_covid = unemp[unemp['ds'] < '2020-03-01']\nz1 = np.polyfit(range(len(pre_covid)), pre_covid['y'], 1)\ntrend_pre = np.polyval(z1, range(len(pre_covid)))\naxes[0].plot(pre_covid['ds'], trend_pre, color=COLORS['gray'], linewidth=2, \n             linestyle='--', label='Pre-COVID Trend')\n\n# Extend pre-COVID trend (counterfactual)\nfuture_len = len(unemp) - len(pre_covid)\ntrend_counter = np.polyval(z1, range(len(pre_covid), len(unemp)))\naxes[0].plot(unemp['ds'][len(pre_covid):], trend_counter, color=COLORS['gray'], \n             linewidth=1, linestyle=':', alpha=0.5, label='Counterfactual')\n\naxes[0].set_title('US Unemployment Rate (2015-2023): COVID-19 Structural Break - Simulated', fontweight='bold')\naxes[0].set_ylabel('Unemployment Rate (%)')\naxes[0].set_ylim([0, 16])\naxes[0].legend(loc='upper center', bbox_to_anchor=(0.5, -0.12), ncol=4, frameon=False)\n\n# Month-over-month change\nunemp_change = unemp['y'].diff()\ncolors_bar = [COLORS['green'] if x < 0 else COLORS['red'] for x in unemp_change]\naxes[1].bar(unemp['ds'], unemp_change, color=colors_bar, width=20, alpha=0.7)\naxes[1].axhline(y=0, color='black', linewidth=0.5)\naxes[1].set_title('Month-over-Month Change', fontweight='bold')\naxes[1].set_xlabel('Date')\naxes[1].set_ylabel('Change (pp)')\n\nplt.tight_layout()\nplt.subplots_adjust(hspace=0.35)\nplt.show()\n\nprint(\"Key Observations:\")\nprint(\"- April 2020: Largest single-month increase in history (+10.3pp)\")\nprint(\"- Recovery was rapid but took ~3 years to return to pre-COVID levels\")\nprint(\"- Traditional ARIMA would struggle with such an extreme outlier\")",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "## Prophet Model for Structural Breaks\n\nProphet handles structural breaks through **changepoint detection**:\n- Automatically identifies where the trend changes\n- Adjustable `changepoint_prior_scale` controls flexibility\n- Can add known changepoints (like COVID)",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "if HAS_PROPHET:\n    # Prepare data for Prophet\n    unemp_prophet = unemp.copy()\n    \n    # Train/test split (hold out last 12 months)\n    train_unemp = unemp_prophet.iloc[:-12]\n    test_unemp = unemp_prophet.iloc[-12:]\n    \n    print(\"Fitting Prophet Model with Changepoint Detection\")\n    print(\"=\"*50)\n    \n    # Model with flexible changepoints and specified COVID changepoint\n    prophet_model = Prophet(\n        changepoint_prior_scale=0.5,  # More flexible for extreme changes\n        yearly_seasonality=False,\n        weekly_seasonality=False,\n        daily_seasonality=False,\n        changepoints=['2020-03-01', '2020-04-01', '2020-06-01']  # COVID events\n    )\n    prophet_model.fit(train_unemp)\n    \n    # Forecast\n    future = prophet_model.make_future_dataframe(periods=12, freq='MS')\n    forecast = prophet_model.predict(future)\n    \n    # Extract test predictions\n    pred_test = forecast['yhat'].iloc[-12:].values\n    \n    # Metrics\n    rmse = np.sqrt(mean_squared_error(test_unemp['y'], pred_test))\n    mape = np.mean(np.abs((test_unemp['y'].values - pred_test) / test_unemp['y'].values)) * 100\n    \n    print(f\"\\nProphet Forecast Performance:\")\n    print(f\"  RMSE: {rmse:.2f}\")\n    print(f\"  MAPE: {mape:.2f}%\")\nelse:\n    print(\"Prophet not available. Install with: pip install prophet\")",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "if HAS_PROPHET:\n    # Plot Prophet results\n    fig, axes = plt.subplots(2, 1, figsize=(12, 8))\n    \n    # Forecast plot\n    axes[0].plot(unemp['ds'], unemp['y'], color=COLORS['blue'], linewidth=1.5, label='Actual')\n    axes[0].plot(forecast['ds'], forecast['yhat'], color=COLORS['orange'], \n                 linewidth=1.5, linestyle='--', label='Prophet Forecast')\n    axes[0].fill_between(forecast['ds'], forecast['yhat_lower'], forecast['yhat_upper'],\n                        color=COLORS['orange'], alpha=0.2, label='95% CI')\n    axes[0].axvline(x=train_unemp['ds'].iloc[-1], color='black', linestyle=':', alpha=0.5)\n    \n    # Mark changepoints\n    for i, cp in enumerate(prophet_model.changepoints):\n        if i == 0:\n            axes[0].axvline(x=cp, color=COLORS['red'], linestyle='--', alpha=0.5, linewidth=1, label='Changepoints')\n        else:\n            axes[0].axvline(x=cp, color=COLORS['red'], linestyle='--', alpha=0.5, linewidth=1)\n    \n    axes[0].set_title('Prophet Model with Changepoint Detection', fontweight='bold')\n    axes[0].set_ylabel('Unemployment Rate (%)')\n    axes[0].legend(loc='upper center', bbox_to_anchor=(0.5, -0.12), ncol=4, frameon=False)\n    \n    # Trend component\n    axes[1].plot(forecast['ds'], forecast['trend'], color=COLORS['green'], linewidth=2, label='Trend')\n    for i, cp in enumerate(prophet_model.changepoints):\n        if i == 0:\n            axes[1].axvline(x=cp, color=COLORS['red'], linestyle='--', alpha=0.5, linewidth=1, label='Changepoints')\n        else:\n            axes[1].axvline(x=cp, color=COLORS['red'], linestyle='--', alpha=0.5, linewidth=1)\n    axes[1].set_title('Prophet Trend Component (with Changepoints)', fontweight='bold')\n    axes[1].set_xlabel('Date')\n    axes[1].set_ylabel('Trend')\n    axes[1].legend(loc='upper center', bbox_to_anchor=(0.5, -0.15), ncol=2, frameon=False)\n    \n    plt.tight_layout()\n    plt.subplots_adjust(hspace=0.35)\n    plt.show()\n    \n    print(\"\\nProphet Changepoints detected:\")\n    for i, cp in enumerate(prophet_model.changepoints):\n        print(f\"  {i+1}. {cp.strftime('%Y-%m-%d')}\")\n    print(\"\\nRed dashed lines show where Prophet detected trend changes.\")",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "## ARIMA Comparison (Post-COVID only)\n\nFor comparison, let's fit ARIMA to post-COVID data only:",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "# Post-COVID data only (from June 2020)\npost_covid = unemp[unemp['ds'] >= '2020-06-01'].copy()\npost_covid_train = post_covid.iloc[:-12]\npost_covid_test = post_covid.iloc[-12:]\n\nprint(\"Fitting ARIMA to Post-COVID Data Only\")\nprint(\"=\"*50)\n\narima_model = ARIMA(post_covid_train['y'], order=(2, 1, 1))\narima_results = arima_model.fit()\n\n# Forecast\narima_forecast = arima_results.get_forecast(steps=12)\narima_pred = arima_forecast.predicted_mean\narima_ci = arima_forecast.conf_int()\n\n# Metrics\narima_rmse = np.sqrt(mean_squared_error(post_covid_test['y'], arima_pred))\narima_mape = np.mean(np.abs((post_covid_test['y'].values - arima_pred.values) / post_covid_test['y'].values)) * 100\n\nprint(f\"\\nARIMA(2,1,1) Performance (Post-COVID):\")\nprint(f\"  RMSE: {arima_rmse:.2f}\")\nprint(f\"  MAPE: {arima_mape:.2f}%\")",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "# Compare Prophet vs ARIMA\nfig, ax = plt.subplots(figsize=(12, 6))\n\nax.plot(unemp['ds'], unemp['y'], color=COLORS['blue'], linewidth=1.5, label='Actual')\n\nif HAS_PROPHET:\n    ax.plot(test_unemp['ds'], pred_test, color=COLORS['orange'], linewidth=2,\n            linestyle='--', label=f'Prophet (RMSE={rmse:.2f})')\n\nax.plot(post_covid_test['ds'], arima_pred, color=COLORS['green'], linewidth=2,\n        linestyle=':', label=f'ARIMA Post-COVID (RMSE={arima_rmse:.2f})')\n\nax.axvline(x=train_unemp['ds'].iloc[-1], color='black', linestyle=':', alpha=0.5)\nax.axvspan(pd.Timestamp('2020-03-01'), pd.Timestamp('2020-05-01'),\n           alpha=0.2, color=COLORS['red'], label='COVID Shock')\n\nax.set_title('Model Comparison: US Unemployment Forecast', fontweight='bold')\nax.set_xlabel('Date')\nax.set_ylabel('Unemployment Rate (%)')\nax.legend(loc='upper center', bbox_to_anchor=(0.5, -0.12), ncol=4, frameon=False)\n\nplt.tight_layout()\nplt.show()\n\nprint(\"\\nComparison:\")\nprint(\"- Prophet handles the full series including structural break\")\nprint(\"- ARIMA requires careful selection of training period\")\nprint(\"- For extreme events, domain knowledge + flexible models are key\")",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "---\n# Summary: Model Selection Guide\n\n## Decision Flowchart\n\n```\nTime Series Data\n    |\n    v\n[Is data stationary?] --No--> Apply differencing/transformations\n    |\n    Yes\n    v\n[Financial returns?] --Yes--> ARIMA-GARCH\n    |\n    No\n    v\n[Seasonality present?]\n    |         |\n   Yes        No\n    |         |\n    v         v\n[Long season?]   ARIMA\n(>12 periods)\n    |    |\n   Yes   No\n    |    |\n    v    v\nFourier  SARIMA\nTerms\n    |\n    v\n[Structural breaks?] --Yes--> Prophet/Piecewise models\n    |\n    No\n    v\n[Multiple seasonality?] --Yes--> Prophet/TBATS\n    |\n    No\n    v\nSelect based on AIC/BIC and out-of-sample performance\n```",
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "print(\"\"\"\n\\u2554\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2557\n\\u2551                    TIME SERIES MODEL SELECTION GUIDE                  \\u2551\n\\u2560\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2563\n\\u2551                                                                      \\u2551\n\\u2551  DATA TYPE              RECOMMENDED MODEL         SPECIAL CASE       \\u2551\n\\u2551  \\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500\\u2500  \\u2551\n\\u2551                                                                      \\u2551\n\\u2551  Crypto/Financial       ARIMA-GARCH               Extreme volatility \\u2551\n\\u2551  (Bitcoin, stocks)      Focus on volatility       clustering         \\u2551\n\\u2551                                                                      \\u2551\n\\u2551  Long seasonality       ARIMA + Fourier           Period > 12        \\u2551\n\\u2551  (Sunspots, climate)    terms as regressors       (computationally   \\u2551\n\\u2551                                                   feasible)          \\u2551\n\\u2551                                                                      \\u2551\n\\u2551  Structural breaks      Prophet with              Changepoint        \\u2551\n\\u2551  (COVID, crises)        changepoints              detection          \\u2551\n\\u2551                                                                      \\u2551\n\\u2551  Standard seasonality   SARIMA                    Period \\u2264 12        \\u2551\n\\u2551  (monthly, quarterly)                                                \\u2551\n\\u2551                                                                      \\u2551\n\\u2551  Multiple seasonality   Prophet, TBATS            Daily + weekly +   \\u2551\n\\u2551  (hourly data)                                    annual             \\u2551\n\\u2551                                                                      \\u2551\n\\u2560\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2563\n\\u2551                         WORKFLOW STEPS                               \\u2551\n\\u2551  1. Visualize and explore the data                                   \\u2551\n\\u2551  2. Test for stationarity (ADF, KPSS)                                \\u2551\n\\u2551  3. Apply transformations if needed (log, diff)                      \\u2551\n\\u2551  4. Identify patterns (ACF/PACF, decomposition)                      \\u2551\n\\u2551  5. Fit candidate models                                             \\u2551\n\\u2551  6. Check diagnostics (residuals, Ljung-Box)                         \\u2551\n\\u2551  7. Compare with out-of-sample forecast                              \\u2551\n\\u2551  8. Select best model for the task                                   \\u2551\n\\u255a\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u255d\n\"\"\")",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "# Final summary table\nprint(\"\\nChapter 10 Case Studies Summary\")\nprint(\"=\"*75)\nprint(f\"{'Dataset':<20} {'Challenge':<25} {'Best Model':<20} {'Key Insight'}\")\nprint(\"-\"*75)\nprint(f\"{'Bitcoin':<20} {'Extreme volatility':<25} {'ARIMA-GARCH(1,1)':<20} {'Focus on volatility, not mean'}\")\nprint(f\"{'Sunspots':<20} {'132-month cycle':<25} {'ARIMA + Fourier':<20} {'Long seasonality handling'}\")\nprint(f\"{'US Unemployment':<20} {'COVID structural break':<25} {'Prophet':<20} {'Changepoint detection'}\")\nprint(\"-\"*75)\n\nprint(\"\\n\" + \"=\"*75)\nprint(\"KEY TAKEAWAYS\")\nprint(\"=\"*75)\nprint(\"\"\"\n1. VOLATILITY MODELING (Bitcoin)\n   - Financial returns show weak mean predictability but strong volatility patterns\n   - GARCH captures volatility clustering: large moves followed by large moves\n   - Always examine squared returns for ARCH effects\n\n2. LONG SEASONALITY (Sunspots)\n   - When seasonal period > 12, traditional SARIMA is computationally infeasible\n   - Fourier terms as regressors provide an elegant solution\n   - Use spectral analysis to identify the true period\n\n3. STRUCTURAL BREAKS (Unemployment)\n   - Extreme events (COVID, financial crises) require flexible models\n   - Prophet's changepoint detection handles regime changes\n   - Consider using post-break data only for ARIMA\n\n4. GENERAL PRINCIPLES\n   - Always start with visualization and stationarity testing\n   - Match the model to the data characteristics\n   - Validate with out-of-sample forecasting\n   - Simple models often work better than complex ones\n\"\"\")",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": "---\n\n## Practice Exercises\n\n1. **Bitcoin Analysis**: Try fitting EGARCH or GJR-GARCH to capture asymmetric volatility (negative returns increase volatility more than positive returns).\n\n2. **Sunspot Forecasting**: Experiment with different numbers of Fourier harmonics (K=1, 2, 4, 6). How does it affect forecast accuracy?\n\n3. **Unemployment Modeling**: Add COVID-19 as a regressor (dummy variable) to an ARIMA model. Compare with Prophet.\n\n4. **Model Diagnostics**: For each case study, examine the residuals using Ljung-Box test and QQ-plots.\n\n---\n\n**End of Chapter 10: Comprehensive Review**\n\n*Course: Time Series Analysis and Forecasting*  \n*Bucharest University of Economic Studies*",
      "metadata": {}
    }
  ]
}
