{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/danpele/Time-Series-Analysis/blob/main/chapter10_lecture_notebook.ipynb)\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Chapter 10: Comprehensive Review\n",
        "\n",
        "**Complete Time Series Analysis with Real Data**\n",
        "\n",
        "**Course:** Time Series Analysis and Forecasting  \n",
        "**Program:** Bachelor program, Faculty of Cybernetics, Statistics and Economic Informatics, Bucharest University of Economic Studies, Romania  \n",
        "**Academic Year:** 2025-2026\n",
        "\n",
        "---\n",
        "\n",
        "## Learning Objectives\n",
        "\n",
        "This comprehensive review demonstrates the complete time series analysis workflow using **real data** with proper **train/validation/test** methodology:\n",
        "\n",
        "1. **Case Study 1: Bitcoin** - ARIMA-GARCH with proper volatility forecasting evaluation\n",
        "2. **Case Study 2: Sunspots** - Long-cycle seasonal data with cross-validation\n",
        "3. **Case Study 3: US Unemployment** - Structural breaks with Prophet\n",
        "4. **Case Study 4: Multivariate Analysis** - VAR model with Granger causality\n",
        "\n",
        "### Key Methodology: Train/Validation/Test Split\n",
        "\n",
        "```\n",
        "|<------ Training ------>|<-- Validation -->|<---- Test ---->|\n",
        "|                        |                  |                |\n",
        "|  Fit model parameters  | Select hyperpar. | Final evaluate |\n",
        "|                        | Compare models   | Report metrics |\n",
        "```\n",
        "\n",
        "**Why this matters:**\n",
        "- **Training set**: Used to estimate model parameters\n",
        "- **Validation set**: Used to select hyperparameters and compare models\n",
        "- **Test set**: Held out until the very end for unbiased final evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup and Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install required packages (for Colab)\n",
        "import sys\n",
        "if 'google.colab' in sys.modules:\n",
        "    !pip install prophet arch statsmodels yfinance pandas-datareader -q\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Statistical tests and models\n",
        "from statsmodels.tsa.stattools import adfuller, kpss, grangercausalitytests\n",
        "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
        "from statsmodels.tsa.arima.model import ARIMA\n",
        "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
        "from statsmodels.tsa.api import VAR\n",
        "from statsmodels.tsa.seasonal import seasonal_decompose\n",
        "\n",
        "# GARCH\n",
        "try:\n",
        "    from arch import arch_model\n",
        "    HAS_ARCH = True\n",
        "except ImportError:\n",
        "    HAS_ARCH = False\n",
        "    print(\"arch not installed. Install with: pip install arch\")\n",
        "\n",
        "# Prophet\n",
        "try:\n",
        "    from prophet import Prophet\n",
        "    HAS_PROPHET = True\n",
        "except ImportError:\n",
        "    HAS_PROPHET = False\n",
        "    print(\"Prophet not installed. Install with: pip install prophet\")\n",
        "\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
        "\n",
        "# Plotting style - CONSISTENT WITH OTHER CHAPTERS\n",
        "plt.rcParams['figure.figsize'] = (12, 5)\n",
        "plt.rcParams['font.size'] = 11\n",
        "plt.rcParams['axes.facecolor'] = 'none'\n",
        "plt.rcParams['figure.facecolor'] = 'none'\n",
        "plt.rcParams['axes.grid'] = False\n",
        "plt.rcParams['axes.spines.top'] = False\n",
        "plt.rcParams['axes.spines.right'] = False\n",
        "plt.rcParams['legend.frameon'] = False\n",
        "\n",
        "COLORS = {'blue': '#1A3A6E', 'red': '#DC3545', 'green': '#2E7D32', \n",
        "          'orange': '#E67E22', 'gray': '#666666', 'purple': '#8E44AD'}\n",
        "\n",
        "print(\"Setup complete!\")\n",
        "print(f\"ARCH/GARCH available: {HAS_ARCH}\")\n",
        "print(f\"Prophet available: {HAS_PROPHET}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Helper Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def test_stationarity(series, name):\n",
        "    \"\"\"Run ADF and KPSS tests for stationarity\"\"\"\n",
        "    print(f\"\\nStationarity Tests for {name}\")\n",
        "    print(\"-\" * 50)\n",
        "    \n",
        "    # ADF Test (H0: unit root exists = non-stationary)\n",
        "    adf_result = adfuller(series.dropna(), autolag='AIC')\n",
        "    adf_stationary = adf_result[1] < 0.05\n",
        "    print(f\"ADF Test:\")\n",
        "    print(f\"  Statistic: {adf_result[0]:.4f}\")\n",
        "    print(f\"  p-value: {adf_result[1]:.6f}\")\n",
        "    print(f\"  Conclusion: {'STATIONARY' if adf_stationary else 'NON-STATIONARY'}\")\n",
        "    \n",
        "    # KPSS Test (H0: stationary)\n",
        "    kpss_result = kpss(series.dropna(), regression='c', nlags='auto')\n",
        "    kpss_stationary = kpss_result[1] > 0.05\n",
        "    print(f\"\\nKPSS Test:\")\n",
        "    print(f\"  Statistic: {kpss_result[0]:.4f}\")\n",
        "    print(f\"  p-value: {kpss_result[1]:.4f}\")\n",
        "    print(f\"  Conclusion: {'STATIONARY' if kpss_stationary else 'NON-STATIONARY'}\")\n",
        "    \n",
        "    both_agree = adf_stationary and kpss_stationary\n",
        "    print(f\"\\nOverall: {'STATIONARY' if both_agree else 'NON-STATIONARY or INCONCLUSIVE'}\")\n",
        "    return both_agree\n",
        "\n",
        "\n",
        "def calc_metrics(actual, predicted, name=\"\"):\n",
        "    \"\"\"Calculate forecast accuracy metrics\"\"\"\n",
        "    actual = np.array(actual)\n",
        "    predicted = np.array(predicted)\n",
        "    \n",
        "    rmse = np.sqrt(mean_squared_error(actual, predicted))\n",
        "    mae = mean_absolute_error(actual, predicted)\n",
        "    # Avoid division by zero in MAPE\n",
        "    mape = np.mean(np.abs((actual - predicted) / (actual + 1e-10))) * 100\n",
        "    \n",
        "    if name:\n",
        "        print(f\"\\n{name} Metrics:\")\n",
        "        print(f\"  RMSE: {rmse:.4f}\")\n",
        "        print(f\"  MAE:  {mae:.4f}\")\n",
        "        print(f\"  MAPE: {mape:.2f}%\")\n",
        "    \n",
        "    return {'rmse': rmse, 'mae': mae, 'mape': mape}\n",
        "\n",
        "\n",
        "def time_series_split(data, train_pct=0.7, val_pct=0.15, test_pct=0.15):\n",
        "    \"\"\"Split time series data into train/validation/test sets\n",
        "    \n",
        "    IMPORTANT: For time series, we NEVER shuffle data!\n",
        "    We preserve temporal order.\n",
        "    \"\"\"\n",
        "    n = len(data)\n",
        "    train_end = int(n * train_pct)\n",
        "    val_end = int(n * (train_pct + val_pct))\n",
        "    \n",
        "    train = data.iloc[:train_end].copy()\n",
        "    val = data.iloc[train_end:val_end].copy()\n",
        "    test = data.iloc[val_end:].copy()\n",
        "    \n",
        "    print(f\"Data Split (Total: {n} observations):\")\n",
        "    print(f\"  Training:   {len(train):4d} ({100*len(train)/n:.1f}%)\")\n",
        "    print(f\"  Validation: {len(val):4d} ({100*len(val)/n:.1f}%)\")\n",
        "    print(f\"  Test:       {len(test):4d} ({100*len(test)/n:.1f}%)\")\n",
        "    \n",
        "    return train, val, test\n",
        "\n",
        "\n",
        "print(\"Helper functions defined!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Real Data Loading Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_bitcoin_data():\n",
        "    \"\"\"Fetch REAL Bitcoin daily prices from Yahoo Finance (2019-2024)\"\"\"\n",
        "    try:\n",
        "        import yfinance as yf\n",
        "        btc = yf.download('BTC-USD', start='2019-01-01', end='2024-01-01', progress=False)\n",
        "        df = pd.DataFrame({\n",
        "            'ds': btc.index,\n",
        "            'price': btc['Close'].values.flatten()\n",
        "        }).reset_index(drop=True)\n",
        "        df['returns'] = df['price'].pct_change() * 100\n",
        "        print(f\"Bitcoin: Loaded {len(df)} days of REAL data from Yahoo Finance\")\n",
        "        return df\n",
        "    except Exception as e:\n",
        "        print(f\"Could not load Bitcoin data: {e}\")\n",
        "        print(\"Using backup simulated data...\")\n",
        "        np.random.seed(42)\n",
        "        dates = pd.date_range('2019-01-01', '2024-01-01', freq='D')\n",
        "        price = 3700\n",
        "        prices = [price]\n",
        "        for i in range(1, len(dates)):\n",
        "            date = dates[i]\n",
        "            if date < pd.Timestamp('2020-03-01'):\n",
        "                drift, vol = 0.001, 0.03\n",
        "            elif date < pd.Timestamp('2020-04-01'):\n",
        "                drift, vol = -0.02, 0.08\n",
        "            elif date < pd.Timestamp('2021-11-01'):\n",
        "                drift, vol = 0.003, 0.04\n",
        "            elif date < pd.Timestamp('2023-01-01'):\n",
        "                drift, vol = -0.002, 0.045\n",
        "            else:\n",
        "                drift, vol = 0.0015, 0.025\n",
        "            ret = drift + vol * np.random.randn()\n",
        "            price = max(prices[-1] * (1 + ret), 1000)\n",
        "            prices.append(price)\n",
        "        df = pd.DataFrame({'ds': dates, 'price': prices})\n",
        "        df['returns'] = df['price'].pct_change() * 100\n",
        "        return df\n",
        "\n",
        "\n",
        "def get_sunspot_data():\n",
        "    \"\"\"Fetch REAL monthly sunspot numbers from statsmodels (1749-present)\"\"\"\n",
        "    try:\n",
        "        import statsmodels.api as sm\n",
        "        sunspots = sm.datasets.sunspots.load_pandas().data\n",
        "        sunspots = sunspots[sunspots['YEAR'] >= 1900].copy()\n",
        "        sunspots['ds'] = pd.to_datetime(sunspots['YEAR'].astype(int), format='%Y')\n",
        "        sunspots = sunspots.rename(columns={'SUNACTIVITY': 'y'})\n",
        "        sunspots = sunspots[['ds', 'y']].reset_index(drop=True)\n",
        "        print(f\"Sunspots: Loaded {len(sunspots)} years of REAL data from statsmodels\")\n",
        "        return sunspots\n",
        "    except Exception as e:\n",
        "        print(f\"Could not load sunspot data: {e}\")\n",
        "        print(\"Using backup simulated data...\")\n",
        "        np.random.seed(123)\n",
        "        dates = pd.date_range('1900-01-01', '2023-12-01', freq='YS')\n",
        "        cycle_period = 11\n",
        "        sunspots = []\n",
        "        for i in range(len(dates)):\n",
        "            phase = (i % cycle_period) / cycle_period\n",
        "            cycle_value = 150 * np.sin(np.pi * phase) ** 1.5\n",
        "            cycle_num = i // cycle_period\n",
        "            cycle_amplitude = [1.0, 0.8, 1.1, 0.9, 1.2, 0.7, 1.0, 0.85, 1.15, 0.95, 1.1][cycle_num % 11]\n",
        "            cycle_value *= cycle_amplitude\n",
        "            noise = np.random.normal(0, 15)\n",
        "            sunspots.append(max(0, cycle_value + noise))\n",
        "        return pd.DataFrame({'ds': dates, 'y': sunspots})\n",
        "\n",
        "\n",
        "def get_unemployment_data():\n",
        "    \"\"\"Fetch REAL US Unemployment Rate from FRED (2010-2023)\"\"\"\n",
        "    try:\n",
        "        import pandas_datareader as pdr\n",
        "        unemp = pdr.get_data_fred('UNRATE', start='2010-01-01', end='2023-12-31')\n",
        "        df = pd.DataFrame({\n",
        "            'ds': unemp.index,\n",
        "            'y': unemp['UNRATE'].values\n",
        "        }).reset_index(drop=True)\n",
        "        print(f\"US Unemployment: Loaded {len(df)} months of REAL data from FRED\")\n",
        "        return df\n",
        "    except Exception as e:\n",
        "        print(f\"Could not load unemployment data: {e}\")\n",
        "        print(\"Using backup simulated data...\")\n",
        "        np.random.seed(456)\n",
        "        dates = pd.date_range('2010-01-01', '2023-12-01', freq='MS')\n",
        "        unemployment = []\n",
        "        for i, date in enumerate(dates):\n",
        "            if date < pd.Timestamp('2020-03-01'):\n",
        "                base = 9.5 - (i / 120) * 6\n",
        "                unemployment.append(max(3.5, base + np.random.normal(0, 0.1)))\n",
        "            elif date < pd.Timestamp('2020-05-01'):\n",
        "                unemployment.append(4.4 if date.month == 3 else 14.7)\n",
        "            elif date < pd.Timestamp('2022-01-01'):\n",
        "                months_since = (date.year - 2020) * 12 + date.month - 4\n",
        "                rate = 14.7 - months_since * 0.5\n",
        "                unemployment.append(max(4.0, rate + np.random.normal(0, 0.2)))\n",
        "            else:\n",
        "                unemployment.append(3.6 + np.random.normal(0, 0.1))\n",
        "        return pd.DataFrame({'ds': dates, 'y': unemployment})\n",
        "\n",
        "\n",
        "def get_multivariate_economic_data():\n",
        "    \"\"\"Fetch multiple economic indicators from FRED for VAR analysis\"\"\"\n",
        "    try:\n",
        "        import pandas_datareader as pdr\n",
        "        \n",
        "        # Fetch multiple series\n",
        "        start, end = '2000-01-01', '2023-12-31'\n",
        "        \n",
        "        gdp = pdr.get_data_fred('GDPC1', start=start, end=end)  # Real GDP (quarterly)\n",
        "        unemp = pdr.get_data_fred('UNRATE', start=start, end=end)  # Unemployment (monthly)\n",
        "        inflation = pdr.get_data_fred('CPIAUCSL', start=start, end=end)  # CPI (monthly)\n",
        "        fed_rate = pdr.get_data_fred('FEDFUNDS', start=start, end=end)  # Fed Funds Rate (monthly)\n",
        "        \n",
        "        # Convert to quarterly and merge\n",
        "        unemp_q = unemp.resample('Q').mean()\n",
        "        inflation_q = inflation.resample('Q').last()\n",
        "        fed_q = fed_rate.resample('Q').mean()\n",
        "        \n",
        "        # Calculate inflation rate (YoY %)\n",
        "        inflation_q['inflation'] = inflation_q['CPIAUCSL'].pct_change(4) * 100\n",
        "        \n",
        "        # Calculate GDP growth (YoY %)\n",
        "        gdp['gdp_growth'] = gdp['GDPC1'].pct_change(4) * 100\n",
        "        \n",
        "        # Merge all series\n",
        "        df = pd.DataFrame({\n",
        "            'gdp_growth': gdp['gdp_growth'],\n",
        "            'unemployment': unemp_q['UNRATE'],\n",
        "            'inflation': inflation_q['inflation'],\n",
        "            'fed_rate': fed_q['FEDFUNDS']\n",
        "        }).dropna()\n",
        "        \n",
        "        print(f\"Economic Data: Loaded {len(df)} quarters of REAL data from FRED\")\n",
        "        print(f\"  Variables: GDP Growth, Unemployment, Inflation, Fed Funds Rate\")\n",
        "        return df\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"Could not load economic data: {e}\")\n",
        "        print(\"Using backup simulated data...\")\n",
        "        np.random.seed(789)\n",
        "        n = 96  # 24 years quarterly\n",
        "        dates = pd.date_range('2000-01-01', periods=n, freq='Q')\n",
        "        \n",
        "        # Simulate correlated economic variables\n",
        "        gdp_growth = np.random.randn(n) * 0.5 + 2.5\n",
        "        unemployment = 5 + np.cumsum(np.random.randn(n) * 0.2)\n",
        "        unemployment = np.clip(unemployment, 3, 15)\n",
        "        inflation = 2 + np.random.randn(n) * 0.5 + 0.3 * gdp_growth\n",
        "        fed_rate = np.clip(2 + inflation * 0.5 + np.random.randn(n) * 0.3, 0, 10)\n",
        "        \n",
        "        # Add 2008 crisis and COVID shock\n",
        "        for i, date in enumerate(dates):\n",
        "            if pd.Timestamp('2008-06-01') <= date <= pd.Timestamp('2009-06-01'):\n",
        "                gdp_growth[i] = -3 + np.random.randn() * 1\n",
        "                unemployment[i] = 8 + (i - 34) * 0.3\n",
        "            if pd.Timestamp('2020-01-01') <= date <= pd.Timestamp('2020-06-01'):\n",
        "                gdp_growth[i] = -8 + np.random.randn() * 2\n",
        "                unemployment[i] = 10 + np.random.randn() * 2\n",
        "        \n",
        "        return pd.DataFrame({\n",
        "            'gdp_growth': gdp_growth,\n",
        "            'unemployment': unemployment,\n",
        "            'inflation': inflation,\n",
        "            'fed_rate': fed_rate\n",
        "        }, index=dates)\n",
        "\n",
        "\n",
        "print(\"Data loading functions defined!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "# Case Study 1: Bitcoin Volatility Analysis\n",
        "\n",
        "## Objective\n",
        "Model and forecast Bitcoin's volatility using GARCH models.\n",
        "\n",
        "## Why GARCH?\n",
        "- Financial returns are typically **unpredictable in mean** but **predictable in variance**\n",
        "- Volatility clustering: large changes follow large changes\n",
        "- GARCH captures this time-varying volatility\n",
        "\n",
        "## Methodology\n",
        "1. **Training (70%)**: Estimate GARCH model parameters\n",
        "2. **Validation (15%)**: Compare GARCH(1,1) vs GARCH(2,1) vs GJR-GARCH\n",
        "3. **Test (15%)**: Final out-of-sample evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load Bitcoin data\n",
        "btc = get_bitcoin_data()\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"BITCOIN DATA OVERVIEW\")\n",
        "print(\"=\"*60)\n",
        "print(f\"Period: {btc['ds'].min().date()} to {btc['ds'].max().date()}\")\n",
        "print(f\"Observations: {len(btc)} days\")\n",
        "print(f\"\\nPrice Statistics:\")\n",
        "print(f\"  Min:  ${btc['price'].min():,.2f}\")\n",
        "print(f\"  Max:  ${btc['price'].max():,.2f}\")\n",
        "print(f\"  Mean: ${btc['price'].mean():,.2f}\")\n",
        "print(f\"\\nReturn Statistics (Daily %):\")\n",
        "print(f\"  Mean:     {btc['returns'].mean():.4f}%\")\n",
        "print(f\"  Std Dev:  {btc['returns'].std():.4f}%\")\n",
        "print(f\"  Skewness: {btc['returns'].skew():.4f} (negative = left tail)\")\n",
        "print(f\"  Kurtosis: {btc['returns'].kurtosis():.4f} (>3 = fat tails)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize the data\n",
        "fig, axes = plt.subplots(2, 2, figsize=(12, 8))\n",
        "\n",
        "# Price series\n",
        "axes[0, 0].plot(btc['ds'], btc['price'], color=COLORS['blue'], linewidth=0.8)\n",
        "axes[0, 0].set_title('Bitcoin Price (Log Scale)', fontweight='bold')\n",
        "axes[0, 0].set_ylabel('Price (USD)')\n",
        "axes[0, 0].set_yscale('log')\n",
        "\n",
        "# Returns\n",
        "axes[0, 1].plot(btc['ds'], btc['returns'], color=COLORS['green'], linewidth=0.5)\n",
        "axes[0, 1].axhline(y=0, color='black', linewidth=0.5, alpha=0.3)\n",
        "axes[0, 1].set_title('Daily Returns (%)', fontweight='bold')\n",
        "axes[0, 1].set_ylabel('Return (%)')\n",
        "\n",
        "# Returns histogram\n",
        "axes[1, 0].hist(btc['returns'].dropna(), bins=100, color=COLORS['blue'], \n",
        "                alpha=0.7, density=True, label='Returns')\n",
        "# Normal distribution overlay\n",
        "x = np.linspace(-20, 20, 100)\n",
        "from scipy.stats import norm\n",
        "axes[1, 0].plot(x, norm.pdf(x, btc['returns'].mean(), btc['returns'].std()),\n",
        "                color=COLORS['red'], linewidth=2, label='Normal')\n",
        "axes[1, 0].set_title('Return Distribution vs Normal', fontweight='bold')\n",
        "axes[1, 0].set_xlabel('Return (%)')\n",
        "axes[1, 0].legend(loc='upper center', bbox_to_anchor=(0.5, -0.15), ncol=2)\n",
        "\n",
        "# Squared returns (proxy for volatility)\n",
        "axes[1, 1].plot(btc['ds'], btc['returns']**2, color=COLORS['orange'], linewidth=0.5)\n",
        "axes[1, 1].set_title('Squared Returns (Volatility Proxy)', fontweight='bold')\n",
        "axes[1, 1].set_xlabel('Date')\n",
        "axes[1, 1].set_ylabel('Return² (%²)')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nKey Observations:\")\n",
        "print(\"1. Returns show VOLATILITY CLUSTERING (periods of high/low volatility)\")\n",
        "print(\"2. Distribution has FAT TAILS (more extreme events than normal)\")\n",
        "print(\"3. Squared returns show strong persistence -> GARCH is appropriate\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 1: Stationarity Testing\n",
        "print(\"STEP 1: STATIONARITY TESTING\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Test prices (expect non-stationary)\n",
        "test_stationarity(btc['price'], 'Bitcoin Prices')\n",
        "\n",
        "# Test returns (expect stationary)\n",
        "returns = btc['returns'].dropna()\n",
        "test_stationarity(returns, 'Bitcoin Returns')\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"CONCLUSION: Use RETURNS (not prices) for modeling\")\n",
        "print(\"=\"*60)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 2: Train/Validation/Test Split\n",
        "print(\"\\nSTEP 2: DATA SPLITTING\")\n",
        "print(\"=\"*60)\n",
        "print(\"\\nFor time series, we ALWAYS preserve temporal order!\")\n",
        "print(\"Never shuffle - the future cannot predict the past.\\n\")\n",
        "\n",
        "# Prepare returns data\n",
        "btc_returns = btc[['ds', 'returns']].dropna().reset_index(drop=True)\n",
        "\n",
        "# Split: 70% train, 15% validation, 15% test\n",
        "train_btc, val_btc, test_btc = time_series_split(btc_returns, 0.70, 0.15, 0.15)\n",
        "\n",
        "print(f\"\\nDate Ranges:\")\n",
        "print(f\"  Training:   {train_btc['ds'].min().date()} to {train_btc['ds'].max().date()}\")\n",
        "print(f\"  Validation: {val_btc['ds'].min().date()} to {val_btc['ds'].max().date()}\")\n",
        "print(f\"  Test:       {test_btc['ds'].min().date()} to {test_btc['ds'].max().date()}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 3: Model Selection on Validation Set\n",
        "print(\"\\nSTEP 3: MODEL SELECTION (using Validation Set)\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "if HAS_ARCH:\n",
        "    # Combine train+val for model comparison, but evaluate on val only\n",
        "    train_returns = train_btc['returns'].values\n",
        "    val_returns = val_btc['returns'].values\n",
        "    \n",
        "    # Models to compare\n",
        "    models_to_test = {\n",
        "        'GARCH(1,1)': {'vol': 'Garch', 'p': 1, 'q': 1},\n",
        "        'GARCH(2,1)': {'vol': 'Garch', 'p': 2, 'q': 1},\n",
        "        'GJR-GARCH(1,1)': {'vol': 'Garch', 'p': 1, 'o': 1, 'q': 1},\n",
        "        'EGARCH(1,1)': {'vol': 'EGARCH', 'p': 1, 'q': 1}\n",
        "    }\n",
        "    \n",
        "    results_comparison = {}\n",
        "    \n",
        "    print(\"\\nComparing GARCH variants on VALIDATION set:\")\n",
        "    print(\"-\" * 60)\n",
        "    \n",
        "    for name, params in models_to_test.items():\n",
        "        try:\n",
        "            # Fit on training data\n",
        "            model = arch_model(train_returns, mean='AR', lags=1, **params)\n",
        "            res = model.fit(disp='off', show_warning=False)\n",
        "            \n",
        "            # Forecast on validation period\n",
        "            forecast = res.forecast(horizon=len(val_returns), reindex=False)\n",
        "            vol_forecast = np.sqrt(forecast.variance.values[-1, :])\n",
        "            \n",
        "            # Calculate realized volatility (|returns| as proxy)\n",
        "            realized_vol = np.abs(val_returns)\n",
        "            \n",
        "            # Metrics: MAE of volatility forecast\n",
        "            mae = np.mean(np.abs(realized_vol - vol_forecast))\n",
        "            \n",
        "            results_comparison[name] = {\n",
        "                'AIC': res.aic,\n",
        "                'BIC': res.bic,\n",
        "                'Val_MAE': mae,\n",
        "                'model': res\n",
        "            }\n",
        "            \n",
        "            print(f\"{name:20s} AIC: {res.aic:10.2f}  BIC: {res.bic:10.2f}  Val MAE: {mae:.4f}\")\n",
        "        except Exception as e:\n",
        "            print(f\"{name:20s} Failed: {e}\")\n",
        "    \n",
        "    # Select best model based on validation MAE\n",
        "    best_model_name = min(results_comparison, key=lambda x: results_comparison[x]['Val_MAE'])\n",
        "    print(f\"\\n>>> BEST MODEL (lowest validation MAE): {best_model_name}\")\n",
        "else:\n",
        "    print(\"ARCH package not available\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 4: Final Model - Refit on Train+Val, Evaluate on Test\n",
        "print(\"\\nSTEP 4: FINAL EVALUATION (Test Set)\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "if HAS_ARCH:\n",
        "    # Combine training and validation for final model\n",
        "    trainval_returns = np.concatenate([train_returns, val_returns])\n",
        "    test_returns = test_btc['returns'].values\n",
        "    \n",
        "    print(f\"\\nRefitting {best_model_name} on Training+Validation data\")\n",
        "    print(f\"  Training+Validation: {len(trainval_returns)} observations\")\n",
        "    print(f\"  Test: {len(test_returns)} observations (HELD OUT until now!)\\n\")\n",
        "    \n",
        "    # Fit final model\n",
        "    best_params = models_to_test[best_model_name]\n",
        "    final_model = arch_model(trainval_returns, mean='AR', lags=1, **best_params)\n",
        "    final_results = final_model.fit(disp='off')\n",
        "    \n",
        "    # Forecast on test set\n",
        "    final_forecast = final_results.forecast(horizon=len(test_returns), reindex=False)\n",
        "    test_vol_forecast = np.sqrt(final_forecast.variance.values[-1, :])\n",
        "    \n",
        "    # Calculate test metrics\n",
        "    test_realized_vol = np.abs(test_returns)\n",
        "    test_mae = np.mean(np.abs(test_realized_vol - test_vol_forecast))\n",
        "    test_rmse = np.sqrt(np.mean((test_realized_vol - test_vol_forecast)**2))\n",
        "    \n",
        "    print(f\"TEST SET RESULTS (Unbiased Final Evaluation):\")\n",
        "    print(f\"  Volatility MAE:  {test_mae:.4f}\")\n",
        "    print(f\"  Volatility RMSE: {test_rmse:.4f}\")\n",
        "    \n",
        "    # Model summary\n",
        "    print(f\"\\n{best_model_name} Model Parameters:\")\n",
        "    print(final_results.summary().tables[1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize final results\n",
        "if HAS_ARCH:\n",
        "    fig, axes = plt.subplots(2, 1, figsize=(12, 8))\n",
        "    \n",
        "    # Plot 1: Conditional volatility on full training period\n",
        "    cond_vol = final_results.conditional_volatility\n",
        "    train_val_dates = pd.concat([train_btc['ds'], val_btc['ds']])\n",
        "    \n",
        "    axes[0].fill_between(train_val_dates, 0, cond_vol, \n",
        "                         color=COLORS['orange'], alpha=0.5, label='Training+Val Volatility')\n",
        "    axes[0].axvline(x=train_btc['ds'].iloc[-1], color='black', linestyle='--', \n",
        "                    linewidth=1, alpha=0.7, label='Train/Val Split')\n",
        "    axes[0].set_title(f'{best_model_name}: Conditional Volatility (Training Period)', fontweight='bold')\n",
        "    axes[0].set_ylabel('Volatility (σ)')\n",
        "    axes[0].legend(loc='upper center', bbox_to_anchor=(0.5, -0.12), ncol=2)\n",
        "    \n",
        "    # Plot 2: Test set - Forecast vs Realized\n",
        "    test_dates = test_btc['ds']\n",
        "    axes[1].bar(test_dates, test_realized_vol, color=COLORS['blue'], \n",
        "                alpha=0.5, width=2, label='Realized |Returns|')\n",
        "    axes[1].plot(test_dates, test_vol_forecast, color=COLORS['red'], \n",
        "                 linewidth=2, label=f'{best_model_name} Forecast')\n",
        "    axes[1].set_title('TEST SET: Volatility Forecast vs Realized', fontweight='bold')\n",
        "    axes[1].set_xlabel('Date')\n",
        "    axes[1].set_ylabel('Volatility')\n",
        "    axes[1].legend(loc='upper center', bbox_to_anchor=(0.5, -0.15), ncol=2)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.subplots_adjust(hspace=0.35)\n",
        "    plt.show()\n",
        "    \n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"CASE STUDY 1 SUMMARY: Bitcoin Volatility\")\n",
        "    print(\"=\"*60)\n",
        "    print(f\"Best Model: {best_model_name}\")\n",
        "    print(f\"Test MAE: {test_mae:.4f}\")\n",
        "    print(\"\\nKey Insight: Volatility is predictable even when returns are not!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "# Case Study 2: Sunspot Cycle Analysis\n",
        "\n",
        "## Objective\n",
        "Model the 11-year solar cycle using Fourier terms.\n",
        "\n",
        "## Challenge\n",
        "- Seasonal period of 11 years is too long for standard SARIMA\n",
        "- Solution: Use Fourier terms as exogenous regressors\n",
        "\n",
        "## Methodology\n",
        "1. **Training (70%)**: Fit ARIMA with Fourier terms\n",
        "2. **Validation (15%)**: Select optimal number of Fourier harmonics (K=1,2,3,4)\n",
        "3. **Test (15%)**: Final forecast evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load Sunspot data\n",
        "sunspots = get_sunspot_data()\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"SUNSPOT DATA OVERVIEW\")\n",
        "print(\"=\"*60)\n",
        "print(f\"Period: {sunspots['ds'].min().year} to {sunspots['ds'].max().year}\")\n",
        "print(f\"Observations: {len(sunspots)} years\")\n",
        "print(f\"\\nStatistics:\")\n",
        "print(f\"  Mean: {sunspots['y'].mean():.1f}\")\n",
        "print(f\"  Std:  {sunspots['y'].std():.1f}\")\n",
        "print(f\"  Min:  {sunspots['y'].min():.0f}\")\n",
        "print(f\"  Max:  {sunspots['y'].max():.0f}\")\n",
        "print(f\"\\nKnown Feature: ~11-year Schwabe solar cycle\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize and confirm the cycle\n",
        "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
        "\n",
        "# Time series\n",
        "axes[0].plot(sunspots['ds'], sunspots['y'], color=COLORS['blue'], linewidth=1)\n",
        "axes[0].set_title('Yearly Sunspot Numbers (1900-present)', fontweight='bold')\n",
        "axes[0].set_xlabel('Year')\n",
        "axes[0].set_ylabel('Sunspot Count')\n",
        "\n",
        "# ACF to confirm periodicity\n",
        "plot_acf(sunspots['y'], ax=axes[1], lags=40, alpha=0.05)\n",
        "axes[1].axvline(x=11, color=COLORS['red'], linestyle='--', alpha=0.7, label='11-year lag')\n",
        "axes[1].axvline(x=22, color=COLORS['red'], linestyle='--', alpha=0.7)\n",
        "axes[1].set_title('ACF: Confirms ~11-year Periodicity', fontweight='bold')\n",
        "axes[1].legend(loc='upper center', bbox_to_anchor=(0.5, -0.15), ncol=1)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"ACF shows clear peaks at lags 11 and 22, confirming the solar cycle.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 1: Train/Validation/Test Split\n",
        "print(\"\\nSTEP 1: DATA SPLITTING\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "train_sun, val_sun, test_sun = time_series_split(sunspots, 0.70, 0.15, 0.15)\n",
        "\n",
        "print(f\"\\nDate Ranges:\")\n",
        "print(f\"  Training:   {train_sun['ds'].min().year} to {train_sun['ds'].max().year}\")\n",
        "print(f\"  Validation: {val_sun['ds'].min().year} to {val_sun['ds'].max().year}\")\n",
        "print(f\"  Test:       {test_sun['ds'].min().year} to {test_sun['ds'].max().year}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 2: Add Fourier terms and select K\n",
        "print(\"\\nSTEP 2: MODEL SELECTION - Finding Optimal Number of Fourier Harmonics\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "def add_fourier_terms(df, period, K, start_idx=0):\n",
        "    \"\"\"Add K Fourier pairs (sin/cos) for given period\"\"\"\n",
        "    result = df.copy()\n",
        "    t = np.arange(start_idx, start_idx + len(df))\n",
        "    for k in range(1, K + 1):\n",
        "        result[f'sin_{k}'] = np.sin(2 * np.pi * k * t / period)\n",
        "        result[f'cos_{k}'] = np.cos(2 * np.pi * k * t / period)\n",
        "    return result\n",
        "\n",
        "# Test K = 1, 2, 3, 4 Fourier pairs\n",
        "results_K = {}\n",
        "\n",
        "print(\"\\nComparing Fourier harmonics K=1,2,3,4 on VALIDATION set:\")\n",
        "print(\"-\" * 60)\n",
        "\n",
        "for K in [1, 2, 3, 4]:\n",
        "    # Add Fourier terms\n",
        "    train_fourier = add_fourier_terms(train_sun, period=11, K=K, start_idx=0)\n",
        "    val_fourier = add_fourier_terms(val_sun, period=11, K=K, start_idx=len(train_sun))\n",
        "    \n",
        "    exog_cols = [f'sin_{k}' for k in range(1, K+1)] + [f'cos_{k}' for k in range(1, K+1)]\n",
        "    \n",
        "    # Fit ARIMA(2,0,1) with Fourier regressors\n",
        "    try:\n",
        "        model = SARIMAX(train_fourier['y'], \n",
        "                       exog=train_fourier[exog_cols],\n",
        "                       order=(2, 0, 1),\n",
        "                       enforce_stationarity=False,\n",
        "                       enforce_invertibility=False)\n",
        "        res = model.fit(disp=False)\n",
        "        \n",
        "        # Forecast on validation\n",
        "        forecast = res.get_forecast(steps=len(val_fourier), exog=val_fourier[exog_cols])\n",
        "        val_pred = forecast.predicted_mean.values\n",
        "        \n",
        "        # Metrics\n",
        "        rmse = np.sqrt(mean_squared_error(val_fourier['y'], val_pred))\n",
        "        mae = mean_absolute_error(val_fourier['y'], val_pred)\n",
        "        \n",
        "        results_K[K] = {'AIC': res.aic, 'BIC': res.bic, 'RMSE': rmse, 'MAE': mae, 'model': res}\n",
        "        print(f\"K={K}: AIC={res.aic:8.2f}  BIC={res.bic:8.2f}  Val RMSE={rmse:6.2f}  Val MAE={mae:6.2f}\")\n",
        "    except Exception as e:\n",
        "        print(f\"K={K}: Failed - {e}\")\n",
        "\n",
        "# Select best K\n",
        "best_K = min(results_K, key=lambda x: results_K[x]['RMSE'])\n",
        "print(f\"\\n>>> BEST: K={best_K} Fourier harmonics (lowest validation RMSE)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 3: Final Model - Refit on Train+Val, Evaluate on Test\n",
        "print(\"\\nSTEP 3: FINAL EVALUATION (Test Set)\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Combine training and validation\n",
        "trainval_sun = pd.concat([train_sun, val_sun]).reset_index(drop=True)\n",
        "\n",
        "# Add Fourier terms\n",
        "trainval_fourier = add_fourier_terms(trainval_sun, period=11, K=best_K, start_idx=0)\n",
        "test_fourier = add_fourier_terms(test_sun, period=11, K=best_K, start_idx=len(trainval_sun))\n",
        "\n",
        "exog_cols = [f'sin_{k}' for k in range(1, best_K+1)] + [f'cos_{k}' for k in range(1, best_K+1)]\n",
        "\n",
        "print(f\"\\nRefitting ARIMA(2,0,1) + {best_K} Fourier harmonics\")\n",
        "print(f\"  Training+Validation: {len(trainval_fourier)} years\")\n",
        "print(f\"  Test: {len(test_fourier)} years (HELD OUT until now!)\\n\")\n",
        "\n",
        "# Fit final model\n",
        "final_sun_model = SARIMAX(trainval_fourier['y'],\n",
        "                          exog=trainval_fourier[exog_cols],\n",
        "                          order=(2, 0, 1),\n",
        "                          enforce_stationarity=False,\n",
        "                          enforce_invertibility=False)\n",
        "final_sun_results = final_sun_model.fit(disp=False)\n",
        "\n",
        "# Forecast on test\n",
        "test_forecast = final_sun_results.get_forecast(steps=len(test_fourier), exog=test_fourier[exog_cols])\n",
        "test_pred = test_forecast.predicted_mean.values\n",
        "test_ci = test_forecast.conf_int()\n",
        "\n",
        "# Metrics\n",
        "test_metrics = calc_metrics(test_fourier['y'], test_pred, \"TEST SET\")\n",
        "\n",
        "print(f\"\\nModel Summary:\")\n",
        "print(final_sun_results.summary().tables[1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize final results\n",
        "fig, ax = plt.subplots(figsize=(12, 6))\n",
        "\n",
        "# Training data\n",
        "ax.plot(trainval_fourier['ds'], trainval_fourier['y'], \n",
        "        color=COLORS['blue'], linewidth=1, label='Training+Val')\n",
        "\n",
        "# Test actual\n",
        "ax.plot(test_fourier['ds'], test_fourier['y'], \n",
        "        color=COLORS['blue'], linewidth=2, label='Test Actual')\n",
        "\n",
        "# Test forecast\n",
        "ax.plot(test_fourier['ds'], test_pred, \n",
        "        color=COLORS['red'], linewidth=2, linestyle='--', \n",
        "        label=f'Forecast (RMSE={test_metrics[\"rmse\"]:.1f})')\n",
        "\n",
        "# Confidence interval\n",
        "ax.fill_between(test_fourier['ds'], test_ci.iloc[:, 0], test_ci.iloc[:, 1],\n",
        "                color=COLORS['red'], alpha=0.2, label='95% CI')\n",
        "\n",
        "# Mark train/test split\n",
        "ax.axvline(x=trainval_fourier['ds'].iloc[-1], color='black', \n",
        "           linestyle=':', linewidth=2, alpha=0.7)\n",
        "ax.text(trainval_fourier['ds'].iloc[-1], ax.get_ylim()[1]*0.9, \n",
        "        '  Test Start', fontsize=10)\n",
        "\n",
        "ax.set_title(f'Sunspot Forecast: ARIMA(2,0,1) + {best_K} Fourier Harmonics', fontweight='bold')\n",
        "ax.set_xlabel('Year')\n",
        "ax.set_ylabel('Sunspot Count')\n",
        "ax.legend(loc='upper center', bbox_to_anchor=(0.5, -0.12), ncol=4)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"CASE STUDY 2 SUMMARY: Sunspot Cycles\")\n",
        "print(\"=\"*60)\n",
        "print(f\"Best Model: ARIMA(2,0,1) + {best_K} Fourier harmonics\")\n",
        "print(f\"Test RMSE: {test_metrics['rmse']:.2f}\")\n",
        "print(f\"Test MAPE: {test_metrics['mape']:.2f}%\")\n",
        "print(\"\\nKey Insight: Fourier terms capture long seasonal patterns effectively!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "# Case Study 3: US Unemployment with Structural Break\n",
        "\n",
        "## Objective\n",
        "Handle the COVID-19 structural break using Prophet's changepoint detection.\n",
        "\n",
        "## Challenge\n",
        "- April 2020: Unemployment jumped from 3.5% to 14.7% (largest single-month increase ever)\n",
        "- Traditional ARIMA struggles with such extreme outliers\n",
        "\n",
        "## Methodology\n",
        "1. **Training (70%)**: Fit Prophet with changepoint detection\n",
        "2. **Validation (15%)**: Tune changepoint_prior_scale\n",
        "3. **Test (15%)**: Final evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load Unemployment data\n",
        "unemp = get_unemployment_data()\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"US UNEMPLOYMENT DATA OVERVIEW\")\n",
        "print(\"=\"*60)\n",
        "print(f\"Period: {unemp['ds'].min().strftime('%Y-%m')} to {unemp['ds'].max().strftime('%Y-%m')}\")\n",
        "print(f\"Observations: {len(unemp)} months\")\n",
        "print(f\"\\nKey Statistics:\")\n",
        "print(f\"  Pre-COVID Min (Feb 2020): {unemp[unemp['ds'] < '2020-03-01']['y'].min():.1f}%\")\n",
        "print(f\"  COVID Peak (Apr 2020):    {unemp['y'].max():.1f}%\")\n",
        "print(f\"  Latest:                   {unemp['y'].iloc[-1]:.1f}%\")\n",
        "print(f\"\\nStructural Break: March-April 2020\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize the structural break\n",
        "fig, axes = plt.subplots(2, 1, figsize=(12, 8))\n",
        "\n",
        "# Main plot\n",
        "axes[0].plot(unemp['ds'], unemp['y'], color=COLORS['blue'], linewidth=1.5)\n",
        "axes[0].axvspan(pd.Timestamp('2020-03-01'), pd.Timestamp('2020-05-01'),\n",
        "                alpha=0.3, color=COLORS['red'], label='COVID-19 Shock')\n",
        "axes[0].set_title('US Unemployment Rate: COVID-19 Structural Break', fontweight='bold')\n",
        "axes[0].set_ylabel('Unemployment Rate (%)')\n",
        "axes[0].legend(loc='upper center', bbox_to_anchor=(0.5, -0.12), ncol=1)\n",
        "\n",
        "# Month-over-month change\n",
        "unemp_change = unemp['y'].diff()\n",
        "colors_bar = [COLORS['green'] if x < 0 else COLORS['red'] for x in unemp_change]\n",
        "axes[1].bar(unemp['ds'], unemp_change, color=colors_bar, width=20, alpha=0.7)\n",
        "axes[1].axhline(y=0, color='black', linewidth=0.5)\n",
        "axes[1].set_title('Month-over-Month Change (pp)', fontweight='bold')\n",
        "axes[1].set_xlabel('Date')\n",
        "axes[1].set_ylabel('Change (pp)')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.subplots_adjust(hspace=0.35)\n",
        "plt.show()\n",
        "\n",
        "print(\"April 2020: +10.3 percentage points - largest monthly increase in history!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 1: Train/Validation/Test Split\n",
        "print(\"\\nSTEP 1: DATA SPLITTING\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "train_unemp, val_unemp, test_unemp = time_series_split(unemp, 0.70, 0.15, 0.15)\n",
        "\n",
        "print(f\"\\nDate Ranges:\")\n",
        "print(f\"  Training:   {train_unemp['ds'].min().strftime('%Y-%m')} to {train_unemp['ds'].max().strftime('%Y-%m')}\")\n",
        "print(f\"  Validation: {val_unemp['ds'].min().strftime('%Y-%m')} to {val_unemp['ds'].max().strftime('%Y-%m')}\")\n",
        "print(f\"  Test:       {test_unemp['ds'].min().strftime('%Y-%m')} to {test_unemp['ds'].max().strftime('%Y-%m')}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 2: Model Selection - Tune changepoint_prior_scale\n",
        "print(\"\\nSTEP 2: MODEL SELECTION - Tuning Prophet Hyperparameters\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "if HAS_PROPHET:\n",
        "    # Test different changepoint_prior_scale values\n",
        "    scales_to_test = [0.01, 0.05, 0.1, 0.3, 0.5]\n",
        "    results_prophet = {}\n",
        "    \n",
        "    print(\"\\nComparing changepoint_prior_scale on VALIDATION set:\")\n",
        "    print(\"-\" * 60)\n",
        "    print(\"(Higher scale = more flexible trend, can overfit)\")\n",
        "    print(\"(Lower scale = smoother trend, may underfit)\\n\")\n",
        "    \n",
        "    for scale in scales_to_test:\n",
        "        try:\n",
        "            # Fit on training data\n",
        "            model = Prophet(\n",
        "                changepoint_prior_scale=scale,\n",
        "                yearly_seasonality=False,\n",
        "                weekly_seasonality=False,\n",
        "                daily_seasonality=False\n",
        "            )\n",
        "            model.fit(train_unemp, suppress_logging=True)\n",
        "            \n",
        "            # Predict on validation\n",
        "            future_val = model.make_future_dataframe(periods=len(val_unemp), freq='MS')\n",
        "            forecast_val = model.predict(future_val)\n",
        "            val_pred = forecast_val['yhat'].iloc[-len(val_unemp):].values\n",
        "            \n",
        "            # Metrics\n",
        "            rmse = np.sqrt(mean_squared_error(val_unemp['y'], val_pred))\n",
        "            mae = mean_absolute_error(val_unemp['y'], val_pred)\n",
        "            \n",
        "            results_prophet[scale] = {'RMSE': rmse, 'MAE': mae, 'model': model}\n",
        "            print(f\"scale={scale:4.2f}: Val RMSE={rmse:6.3f}  Val MAE={mae:6.3f}\")\n",
        "        except Exception as e:\n",
        "            print(f\"scale={scale:4.2f}: Failed - {e}\")\n",
        "    \n",
        "    # Select best scale\n",
        "    best_scale = min(results_prophet, key=lambda x: results_prophet[x]['RMSE'])\n",
        "    print(f\"\\n>>> BEST: changepoint_prior_scale={best_scale} (lowest validation RMSE)\")\n",
        "else:\n",
        "    print(\"Prophet not available\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 3: Final Model - Refit on Train+Val, Evaluate on Test\n",
        "print(\"\\nSTEP 3: FINAL EVALUATION (Test Set)\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "if HAS_PROPHET:\n",
        "    # Combine training and validation\n",
        "    trainval_unemp = pd.concat([train_unemp, val_unemp]).reset_index(drop=True)\n",
        "    \n",
        "    print(f\"\\nRefitting Prophet with changepoint_prior_scale={best_scale}\")\n",
        "    print(f\"  Training+Validation: {len(trainval_unemp)} months\")\n",
        "    print(f\"  Test: {len(test_unemp)} months (HELD OUT until now!)\\n\")\n",
        "    \n",
        "    # Fit final model\n",
        "    final_prophet = Prophet(\n",
        "        changepoint_prior_scale=best_scale,\n",
        "        yearly_seasonality=False,\n",
        "        weekly_seasonality=False,\n",
        "        daily_seasonality=False\n",
        "    )\n",
        "    final_prophet.fit(trainval_unemp, suppress_logging=True)\n",
        "    \n",
        "    # Predict on test\n",
        "    future_test = final_prophet.make_future_dataframe(periods=len(test_unemp), freq='MS')\n",
        "    forecast_test = final_prophet.predict(future_test)\n",
        "    test_pred_prophet = forecast_test['yhat'].iloc[-len(test_unemp):].values\n",
        "    test_lower = forecast_test['yhat_lower'].iloc[-len(test_unemp):].values\n",
        "    test_upper = forecast_test['yhat_upper'].iloc[-len(test_unemp):].values\n",
        "    \n",
        "    # Metrics\n",
        "    test_prophet_metrics = calc_metrics(test_unemp['y'], test_pred_prophet, \"TEST SET\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize final results\n",
        "if HAS_PROPHET:\n",
        "    fig, axes = plt.subplots(2, 1, figsize=(12, 8))\n",
        "    \n",
        "    # Full forecast\n",
        "    axes[0].plot(unemp['ds'], unemp['y'], color=COLORS['blue'], \n",
        "                 linewidth=1.5, label='Actual')\n",
        "    axes[0].plot(forecast_test['ds'], forecast_test['yhat'], \n",
        "                 color=COLORS['orange'], linewidth=1.5, linestyle='--', label='Prophet Forecast')\n",
        "    axes[0].fill_between(forecast_test['ds'], forecast_test['yhat_lower'], \n",
        "                        forecast_test['yhat_upper'], color=COLORS['orange'], alpha=0.2)\n",
        "    axes[0].axvline(x=trainval_unemp['ds'].iloc[-1], color='black', \n",
        "                    linestyle=':', linewidth=2, alpha=0.7)\n",
        "    \n",
        "    # Mark detected changepoints\n",
        "    for cp in final_prophet.changepoints:\n",
        "        axes[0].axvline(x=cp, color=COLORS['red'], linestyle='--', alpha=0.3, linewidth=1)\n",
        "    \n",
        "    axes[0].set_title('Prophet Model with Changepoint Detection', fontweight='bold')\n",
        "    axes[0].set_ylabel('Unemployment Rate (%)')\n",
        "    axes[0].legend(loc='upper center', bbox_to_anchor=(0.5, -0.12), ncol=3)\n",
        "    \n",
        "    # Test period zoom\n",
        "    axes[1].plot(test_unemp['ds'], test_unemp['y'], \n",
        "                 color=COLORS['blue'], linewidth=2, marker='o', label='Test Actual')\n",
        "    axes[1].plot(test_unemp['ds'], test_pred_prophet, \n",
        "                 color=COLORS['red'], linewidth=2, marker='s', linestyle='--', \n",
        "                 label=f'Forecast (RMSE={test_prophet_metrics[\"rmse\"]:.3f})')\n",
        "    axes[1].fill_between(test_unemp['ds'], test_lower, test_upper,\n",
        "                        color=COLORS['red'], alpha=0.2, label='95% CI')\n",
        "    axes[1].set_title('TEST SET: Forecast vs Actual', fontweight='bold')\n",
        "    axes[1].set_xlabel('Date')\n",
        "    axes[1].set_ylabel('Unemployment Rate (%)')\n",
        "    axes[1].legend(loc='upper center', bbox_to_anchor=(0.5, -0.15), ncol=3)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.subplots_adjust(hspace=0.35)\n",
        "    plt.show()\n",
        "    \n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"CASE STUDY 3 SUMMARY: Unemployment with Structural Break\")\n",
        "    print(\"=\"*60)\n",
        "    print(f\"Best Model: Prophet (changepoint_prior_scale={best_scale})\")\n",
        "    print(f\"Test RMSE: {test_prophet_metrics['rmse']:.4f}\")\n",
        "    print(f\"Test MAPE: {test_prophet_metrics['mape']:.2f}%\")\n",
        "    print(f\"Changepoints detected: {len(final_prophet.changepoints)}\")\n",
        "    print(\"\\nKey Insight: Prophet adapts to structural breaks via changepoints!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "# Case Study 4: Multivariate Analysis (VAR Model)\n",
        "\n",
        "## Objective\n",
        "Model the relationships between multiple economic variables using Vector Autoregression (VAR).\n",
        "\n",
        "## Variables\n",
        "- **GDP Growth** (quarterly, YoY %)\n",
        "- **Unemployment Rate** (%)\n",
        "- **Inflation** (CPI, YoY %)\n",
        "- **Federal Funds Rate** (%)\n",
        "\n",
        "## Why VAR?\n",
        "- Captures **dynamic interdependencies** between multiple time series\n",
        "- Each variable is regressed on its own lags AND lags of all other variables\n",
        "- Enables **Granger causality** testing and **impulse response** analysis\n",
        "\n",
        "## Methodology\n",
        "1. **Training (70%)**: Fit VAR model\n",
        "2. **Validation (15%)**: Select optimal lag order\n",
        "3. **Test (15%)**: Multivariate forecast evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load multivariate economic data\n",
        "econ_data = get_multivariate_economic_data()\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"MULTIVARIATE ECONOMIC DATA OVERVIEW\")\n",
        "print(\"=\"*60)\n",
        "print(f\"Period: {econ_data.index.min().strftime('%Y-Q%q')} to {econ_data.index.max().strftime('%Y-Q%q')}\")\n",
        "print(f\"Observations: {len(econ_data)} quarters\")\n",
        "print(f\"\\nVariables:\")\n",
        "for col in econ_data.columns:\n",
        "    print(f\"  {col:15s}: Mean={econ_data[col].mean():6.2f}, Std={econ_data[col].std():5.2f}\")\n",
        "\n",
        "print(\"\\nCorrelation Matrix:\")\n",
        "print(econ_data.corr().round(2))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize all variables\n",
        "fig, axes = plt.subplots(2, 2, figsize=(12, 8))\n",
        "\n",
        "variables = ['gdp_growth', 'unemployment', 'inflation', 'fed_rate']\n",
        "titles = ['GDP Growth (YoY %)', 'Unemployment Rate (%)', 'Inflation (CPI YoY %)', 'Fed Funds Rate (%)']\n",
        "colors = [COLORS['blue'], COLORS['red'], COLORS['green'], COLORS['orange']]\n",
        "\n",
        "for ax, var, title, color in zip(axes.flat, variables, titles, colors):\n",
        "    ax.plot(econ_data.index, econ_data[var], color=color, linewidth=1)\n",
        "    ax.axhline(y=0, color='black', linewidth=0.5, alpha=0.3)\n",
        "    ax.axvspan(pd.Timestamp('2008-06-01'), pd.Timestamp('2009-06-01'),\n",
        "               alpha=0.2, color='gray', label='2008 Crisis')\n",
        "    ax.axvspan(pd.Timestamp('2020-01-01'), pd.Timestamp('2020-06-01'),\n",
        "               alpha=0.2, color='red', label='COVID')\n",
        "    ax.set_title(title, fontweight='bold')\n",
        "    ax.set_xlabel('Date')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"Key Observations:\")\n",
        "print(\"- GDP and Unemployment are negatively correlated (Okun's Law)\")\n",
        "print(\"- Fed Rate responds to inflation (Taylor Rule)\")\n",
        "print(\"- All variables affected by 2008 crisis and COVID\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 1: Stationarity Testing\n",
        "print(\"\\nSTEP 1: STATIONARITY TESTING\")\n",
        "print(\"=\"*60)\n",
        "print(\"VAR requires all variables to be stationary.\\n\")\n",
        "\n",
        "for var in econ_data.columns:\n",
        "    adf_result = adfuller(econ_data[var].dropna(), autolag='AIC')\n",
        "    status = \"STATIONARY\" if adf_result[1] < 0.05 else \"NON-STATIONARY\"\n",
        "    print(f\"{var:15s}: ADF p-value={adf_result[1]:.4f} -> {status}\")\n",
        "\n",
        "print(\"\\nNote: Some variables may need differencing for strict stationarity.\")\n",
        "print(\"For this example, we proceed with levels (common in macroeconomics).\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 2: Train/Validation/Test Split\n",
        "print(\"\\nSTEP 2: DATA SPLITTING\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "n = len(econ_data)\n",
        "train_end = int(n * 0.70)\n",
        "val_end = int(n * 0.85)\n",
        "\n",
        "train_econ = econ_data.iloc[:train_end]\n",
        "val_econ = econ_data.iloc[train_end:val_end]\n",
        "test_econ = econ_data.iloc[val_end:]\n",
        "\n",
        "print(f\"Data Split (Total: {n} quarters):\")\n",
        "print(f\"  Training:   {len(train_econ):3d} quarters ({train_econ.index.min().strftime('%Y-Q%q')} to {train_econ.index.max().strftime('%Y-Q%q')})\")\n",
        "print(f\"  Validation: {len(val_econ):3d} quarters ({val_econ.index.min().strftime('%Y-Q%q')} to {val_econ.index.max().strftime('%Y-Q%q')})\")\n",
        "print(f\"  Test:       {len(test_econ):3d} quarters ({test_econ.index.min().strftime('%Y-Q%q')} to {test_econ.index.max().strftime('%Y-Q%q')})\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 3: Model Selection - Find optimal lag order\n",
        "print(\"\\nSTEP 3: MODEL SELECTION - Finding Optimal VAR Lag Order\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Fit VAR to find optimal lag using information criteria\n",
        "var_model = VAR(train_econ)\n",
        "lag_selection = var_model.select_order(maxlags=8)\n",
        "\n",
        "print(\"\\nInformation Criteria for Different Lag Orders:\")\n",
        "print(lag_selection.summary())\n",
        "\n",
        "# Use BIC-selected lag (tends to be more parsimonious)\n",
        "optimal_lag = lag_selection.bic\n",
        "print(f\"\\n>>> Selected lag order (BIC): {optimal_lag}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Validate lag selection on validation set\n",
        "print(\"\\nValidating lag selection on VALIDATION set:\")\n",
        "print(\"-\" * 60)\n",
        "\n",
        "results_var = {}\n",
        "\n",
        "for lag in range(1, 6):\n",
        "    try:\n",
        "        # Fit on training\n",
        "        model = VAR(train_econ)\n",
        "        fitted = model.fit(lag)\n",
        "        \n",
        "        # Forecast validation period\n",
        "        forecast = fitted.forecast(train_econ.values[-lag:], steps=len(val_econ))\n",
        "        \n",
        "        # Calculate RMSE for each variable\n",
        "        rmse_total = 0\n",
        "        for i, var in enumerate(econ_data.columns):\n",
        "            rmse = np.sqrt(mean_squared_error(val_econ[var], forecast[:, i]))\n",
        "            rmse_total += rmse\n",
        "        \n",
        "        results_var[lag] = {'RMSE': rmse_total / len(econ_data.columns), 'model': fitted}\n",
        "        print(f\"Lag={lag}: Avg RMSE={rmse_total/len(econ_data.columns):.4f}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Lag={lag}: Failed - {e}\")\n",
        "\n",
        "best_lag = min(results_var, key=lambda x: results_var[x]['RMSE'])\n",
        "print(f\"\\n>>> BEST: Lag={best_lag} (lowest validation RMSE)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 4: Final Model - Refit on Train+Val, Evaluate on Test\n",
        "print(\"\\nSTEP 4: FINAL EVALUATION (Test Set)\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Combine training and validation\n",
        "trainval_econ = pd.concat([train_econ, val_econ])\n",
        "\n",
        "print(f\"\\nRefitting VAR({best_lag}) on Training+Validation data\")\n",
        "print(f\"  Training+Validation: {len(trainval_econ)} quarters\")\n",
        "print(f\"  Test: {len(test_econ)} quarters (HELD OUT until now!)\\n\")\n",
        "\n",
        "# Fit final model\n",
        "final_var = VAR(trainval_econ)\n",
        "final_var_fitted = final_var.fit(best_lag)\n",
        "\n",
        "# Forecast test period\n",
        "test_forecast = final_var_fitted.forecast(trainval_econ.values[-best_lag:], steps=len(test_econ))\n",
        "\n",
        "# Calculate metrics for each variable\n",
        "print(\"TEST SET RESULTS:\")\n",
        "print(\"-\" * 60)\n",
        "test_var_metrics = {}\n",
        "for i, var in enumerate(econ_data.columns):\n",
        "    metrics = calc_metrics(test_econ[var], test_forecast[:, i], f\"{var}\")\n",
        "    test_var_metrics[var] = metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Granger Causality Analysis\n",
        "print(\"\\nGRANGER CAUSALITY ANALYSIS\")\n",
        "print(\"=\"*60)\n",
        "print(\"Testing if one variable helps predict another (beyond its own history)\\n\")\n",
        "\n",
        "variables = econ_data.columns.tolist()\n",
        "granger_results = pd.DataFrame(index=variables, columns=variables)\n",
        "\n",
        "for cause in variables:\n",
        "    for effect in variables:\n",
        "        if cause != effect:\n",
        "            try:\n",
        "                test_data = trainval_econ[[effect, cause]]\n",
        "                result = grangercausalitytests(test_data, maxlag=best_lag, verbose=False)\n",
        "                # Get p-value from F-test at optimal lag\n",
        "                p_value = result[best_lag][0]['ssr_ftest'][1]\n",
        "                granger_results.loc[cause, effect] = f\"{p_value:.3f}\"\n",
        "            except:\n",
        "                granger_results.loc[cause, effect] = \"N/A\"\n",
        "        else:\n",
        "            granger_results.loc[cause, effect] = \"-\"\n",
        "\n",
        "print(\"Granger Causality p-values (row CAUSES column):\")\n",
        "print(\"(p < 0.05 indicates significant Granger causality)\\n\")\n",
        "print(granger_results)\n",
        "\n",
        "print(\"\\nInterpretation:\")\n",
        "print(\"- Low p-values suggest the ROW variable Granger-causes the COLUMN variable\")\n",
        "print(\"- Example: If gdp_growth -> unemployment has p < 0.05,\")\n",
        "print(\"  GDP growth helps predict future unemployment.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize VAR forecasts\n",
        "fig, axes = plt.subplots(2, 2, figsize=(12, 8))\n",
        "\n",
        "for ax, (i, var) in zip(axes.flat, enumerate(econ_data.columns)):\n",
        "    # Historical\n",
        "    ax.plot(trainval_econ.index, trainval_econ[var], \n",
        "            color=COLORS['blue'], linewidth=1, label='Train+Val')\n",
        "    # Test actual\n",
        "    ax.plot(test_econ.index, test_econ[var], \n",
        "            color=COLORS['blue'], linewidth=2, label='Test Actual')\n",
        "    # Forecast\n",
        "    ax.plot(test_econ.index, test_forecast[:, i], \n",
        "            color=COLORS['red'], linewidth=2, linestyle='--', \n",
        "            label=f'Forecast (RMSE={test_var_metrics[var][\"rmse\"]:.2f})')\n",
        "    ax.axvline(x=trainval_econ.index[-1], color='black', linestyle=':', alpha=0.7)\n",
        "    ax.set_title(var.replace('_', ' ').title(), fontweight='bold')\n",
        "    ax.legend(loc='upper center', bbox_to_anchor=(0.5, -0.15), ncol=2, fontsize=9)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.subplots_adjust(hspace=0.4)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Impulse Response Functions\n",
        "print(\"\\nIMPULSE RESPONSE FUNCTIONS\")\n",
        "print(\"=\"*60)\n",
        "print(\"How does a shock to one variable affect others over time?\\n\")\n",
        "\n",
        "irf = final_var_fitted.irf(periods=12)\n",
        "\n",
        "fig, axes = plt.subplots(2, 2, figsize=(12, 8))\n",
        "\n",
        "# Plot impulse responses to GDP shock\n",
        "shock_var = 'gdp_growth'\n",
        "shock_idx = list(econ_data.columns).index(shock_var)\n",
        "\n",
        "for ax, (i, response_var) in zip(axes.flat, enumerate(econ_data.columns)):\n",
        "    irf_values = irf.irfs[:, i, shock_idx]\n",
        "    ax.plot(range(len(irf_values)), irf_values, color=COLORS['purple'], linewidth=2)\n",
        "    ax.axhline(y=0, color='black', linewidth=0.5, alpha=0.5)\n",
        "    ax.fill_between(range(len(irf_values)), \n",
        "                   irf.irfs[:, i, shock_idx] - 1.96 * irf.stderr()[:, i, shock_idx],\n",
        "                   irf.irfs[:, i, shock_idx] + 1.96 * irf.stderr()[:, i, shock_idx],\n",
        "                   alpha=0.2, color=COLORS['purple'])\n",
        "    ax.set_title(f'Response of {response_var} to GDP Shock', fontweight='bold')\n",
        "    ax.set_xlabel('Quarters')\n",
        "    ax.set_ylabel('Response')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"Interpretation:\")\n",
        "print(\"- Positive GDP shock typically DECREASES unemployment (Okun's Law)\")\n",
        "print(\"- Positive GDP shock may INCREASE inflation (demand-pull)\")\n",
        "print(\"- Fed responds by RAISING rates to control inflation (Taylor Rule)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"CASE STUDY 4 SUMMARY: Multivariate VAR Analysis\")\n",
        "print(\"=\"*60)\n",
        "print(f\"Model: VAR({best_lag})\")\n",
        "print(f\"Variables: GDP Growth, Unemployment, Inflation, Fed Rate\")\n",
        "print(f\"\\nTest Set Performance:\")\n",
        "for var, metrics in test_var_metrics.items():\n",
        "    print(f\"  {var:15s}: RMSE={metrics['rmse']:.4f}, MAE={metrics['mae']:.4f}\")\n",
        "print(\"\\nKey Insights:\")\n",
        "print(\"- VAR captures dynamic relationships between economic variables\")\n",
        "print(\"- Granger causality reveals which variables lead/lag others\")\n",
        "print(\"- Impulse responses show how shocks propagate through the system\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "# Summary: Complete Methodology\n",
        "\n",
        "## Train/Validation/Test Framework\n",
        "\n",
        "| Step | Purpose | Data Used |\n",
        "|------|---------|----------|\n",
        "| 1. Training | Estimate model parameters | 70% of data |\n",
        "| 2. Validation | Select hyperparameters, compare models | 15% of data |\n",
        "| 3. Test | Final unbiased evaluation | 15% of data (held out!) |\n",
        "\n",
        "## Model Selection Summary\n",
        "\n",
        "| Case Study | Data Type | Best Model | Key Feature |\n",
        "|------------|-----------|------------|-------------|\n",
        "| Bitcoin | Financial | GARCH | Volatility clustering |\n",
        "| Sunspots | Long seasonal | ARIMA + Fourier | 11-year cycle |\n",
        "| Unemployment | Structural break | Prophet | Changepoint detection |\n",
        "| Economic | Multivariate | VAR | Cross-variable dynamics |"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"\"\"\n",
        "╔══════════════════════════════════════════════════════════════════════╗\n",
        "║                    CHAPTER 10: KEY TAKEAWAYS                         ║\n",
        "╠══════════════════════════════════════════════════════════════════════╣\n",
        "║                                                                      ║\n",
        "║  1. PROPER VALIDATION METHODOLOGY                                    ║\n",
        "║     - NEVER peek at test data during model selection                 ║\n",
        "║     - Use validation set for hyperparameter tuning                   ║\n",
        "║     - Test set gives unbiased final performance estimate             ║\n",
        "║                                                                      ║\n",
        "║  2. MATCH MODEL TO DATA CHARACTERISTICS                              ║\n",
        "║     - Financial data → GARCH for volatility                          ║\n",
        "║     - Long seasonality → Fourier terms                               ║\n",
        "║     - Structural breaks → Prophet/changepoints                       ║\n",
        "║     - Multiple related series → VAR                                  ║\n",
        "║                                                                      ║\n",
        "║  3. INTERPRET RESULTS CAREFULLY                                      ║\n",
        "║     - GARCH: Focus on volatility, not mean prediction                ║\n",
        "║     - Granger causality ≠ true causality                             ║\n",
        "║     - Confidence intervals matter for decision-making                ║\n",
        "║                                                                      ║\n",
        "║  4. PRACTICAL CONSIDERATIONS                                         ║\n",
        "║     - Simple models often outperform complex ones                    ║\n",
        "║     - Out-of-sample performance is what matters                      ║\n",
        "║     - Always visualize data and residuals                            ║\n",
        "║                                                                      ║\n",
        "╚══════════════════════════════════════════════════════════════════════╝\n",
        "\"\"\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Data Sources\n",
        "\n",
        "- **Bitcoin**: Yahoo Finance (BTC-USD)\n",
        "- **Sunspots**: Wolfer sunspot dataset from statsmodels\n",
        "- **US Unemployment**: Bureau of Labor Statistics via FRED (UNRATE)\n",
        "- **Economic Data**: FRED (GDPC1, UNRATE, CPIAUCSL, FEDFUNDS)\n",
        "\n",
        "---\n",
        "\n",
        "**End of Chapter 10: Comprehensive Review**\n",
        "\n",
        "*Course: Time Series Analysis and Forecasting*  \n",
        "*Bucharest University of Economic Studies*"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
