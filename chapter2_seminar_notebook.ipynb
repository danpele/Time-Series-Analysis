{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/danpele/Time-Series-Analysis/blob/main/chapter2_seminar_notebook.ipynb)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Chapter 2: Seminar - ARMA Models Exercises\n\n**Course:** Time Series Analysis and Forecasting  \n**Program:** Bachelor program, Faculty of Cybernetics, Statistics and Economic Informatics, Bucharest University of Economic Studies, Romania  \n**Academic Year:** 2025-2026\n\n---\n\n## Seminar Objectives\n\nIn this seminar, you will:\n1. Practice working with lag operators and backshift notation\n2. Calculate AR and MA process properties (mean, variance, autocovariance)\n3. Identify ARMA models from ACF/PACF patterns\n4. Fit and diagnose ARMA models in Python\n5. Apply the Box-Jenkins methodology to real data"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import yfinance as yf\n",
    "from statsmodels.tsa.arima_process import ArmaProcess\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from statsmodels.tsa.stattools import adfuller, acf, pacf\n",
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "from statsmodels.stats.diagnostic import acorr_ljungbox\n",
    "from scipy import stats\n",
    "\n",
    "# Plotting style - clean, professional\n",
    "plt.rcParams['figure.figsize'] = (12, 5)\n",
    "plt.rcParams['axes.facecolor'] = 'none'  # Transparent background\n",
    "plt.rcParams['figure.facecolor'] = 'none'  # Transparent figure\n",
    "plt.rcParams['savefig.facecolor'] = 'none'\n",
    "plt.rcParams['axes.grid'] = False  # No grid\n",
    "plt.rcParams['axes.spines.top'] = False\n",
    "plt.rcParams['axes.spines.right'] = False\n",
    "\n",
    "# Colors\n",
    "BLUE = '#1A3A6E'\n",
    "RED = '#DC3545'\n",
    "GREEN = '#2E7D32'\n",
    "\n",
    "np.random.seed(42)\n",
    "print(\"Setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Part 1: Multiple Choice Quiz\n",
    "\n",
    "Answer the following questions. Run the cell after each answer to check if you're correct."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quiz 1: Lag Operator\n",
    "\n",
    "**Question:** What is the result of applying $(1-L)^2$ to $X_t$?\n",
    "\n",
    "- A) $X_t - X_{t-1}$\n",
    "- B) $X_t - 2X_{t-1} + X_{t-2}$\n",
    "- C) $X_t + X_{t-1} + X_{t-2}$\n",
    "- D) $X_t - X_{t-2}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enter your answer: 'A', 'B', 'C', or 'D'\n",
    "quiz1_answer = ''  # <-- Enter your answer here\n",
    "\n",
    "# Check answer\n",
    "if quiz1_answer.upper() == 'B':\n",
    "    print(\"CORRECT!\")\n",
    "    print(\"(1-L)^2 = 1 - 2L + L^2\")\n",
    "    print(\"Applied to X_t: X_t - 2X_{t-1} + X_{t-2}\")\n",
    "    print(\"This is the SECOND DIFFERENCE of X_t.\")\n",
    "elif quiz1_answer:\n",
    "    print(\"Incorrect. Try again!\")\n",
    "    print(\"Hint: Expand (1-L)^2 = (1-L)(1-L) using FOIL method.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quiz 2: AR(1) Stationarity\n",
    "\n",
    "**Question:** For which value of $\\phi$ is the AR(1) process $X_t = 0.5 + \\phi X_{t-1} + \\varepsilon_t$ stationary?\n",
    "\n",
    "- A) $\\phi = 1.2$\n",
    "- B) $\\phi = 1.0$\n",
    "- C) $\\phi = -0.8$\n",
    "- D) $\\phi = -1.5$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enter your answer: 'A', 'B', 'C', or 'D'\n",
    "quiz2_answer = ''  # <-- Enter your answer here\n",
    "\n",
    "# Check answer\n",
    "if quiz2_answer.upper() == 'C':\n",
    "    print(\"CORRECT!\")\n",
    "    print(\"AR(1) is stationary if and only if |phi| < 1.\")\n",
    "    print()\n",
    "    print(\"Checking each option:\")\n",
    "    print(\"A. |1.2| = 1.2 > 1 -> Non-stationary (explosive)\")\n",
    "    print(\"B. |1.0| = 1.0 -> Non-stationary (unit root)\")\n",
    "    print(\"C. |-0.8| = 0.8 < 1 -> STATIONARY\")\n",
    "    print(\"D. |-1.5| = 1.5 > 1 -> Non-stationary (explosive)\")\n",
    "elif quiz2_answer:\n",
    "    print(\"Incorrect. Try again!\")\n",
    "    print(\"Hint: The stationarity condition for AR(1) requires |phi| < 1.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quiz 3: ACF/PACF Pattern\n",
    "\n",
    "**Question:** You observe the following pattern:\n",
    "- ACF: Significant spike at lag 1, then all within confidence bands\n",
    "- PACF: Gradual exponential decay\n",
    "\n",
    "What model is suggested?\n",
    "\n",
    "- A) AR(1)\n",
    "- B) MA(1)\n",
    "- C) ARMA(1,1)\n",
    "- D) White noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enter your answer: 'A', 'B', 'C', or 'D'\n",
    "quiz3_answer = ''  # <-- Enter your answer here\n",
    "\n",
    "# Check answer\n",
    "if quiz3_answer.upper() == 'B':\n",
    "    print(\"CORRECT!\")\n",
    "    print()\n",
    "    print(\"Model Identification Summary:\")\n",
    "    print(\"-\" * 40)\n",
    "    print(\"Model    | ACF          | PACF\")\n",
    "    print(\"-\" * 40)\n",
    "    print(\"AR(p)    | Decays       | Cuts off at lag p\")\n",
    "    print(\"MA(q)    | Cuts off at q| Decays\")\n",
    "    print(\"ARMA     | Decays       | Decays\")\n",
    "    print(\"-\" * 40)\n",
    "    print()\n",
    "    print(\"ACF cuts off after 1 + PACF decays = MA(1)\")\n",
    "elif quiz3_answer:\n",
    "    print(\"Incorrect. Try again!\")\n",
    "    print(\"Hint: Which model has ACF that cuts off and PACF that decays?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quiz 4: MA(1) Invertibility\n",
    "\n",
    "**Question:** The MA(1) process $X_t = \\varepsilon_t + \\theta \\varepsilon_{t-1}$ is invertible when:\n",
    "\n",
    "- A) $|\\theta| > 1$\n",
    "- B) $|\\theta| < 1$\n",
    "- C) $\\theta > 0$\n",
    "- D) $\\theta < 0$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enter your answer: 'A', 'B', 'C', or 'D'\n",
    "quiz4_answer = ''  # <-- Enter your answer here\n",
    "\n",
    "# Check answer\n",
    "if quiz4_answer.upper() == 'B':\n",
    "    print(\"CORRECT!\")\n",
    "    print()\n",
    "    print(\"Invertibility condition for MA(1): |theta| < 1\")\n",
    "    print()\n",
    "    print(\"Why does it matter?\")\n",
    "    print(\"- Invertibility allows us to express the MA as an infinite AR\")\n",
    "    print(\"- It ensures uniqueness of the representation\")\n",
    "    print(\"- It makes estimation well-defined\")\n",
    "    print()\n",
    "    print(\"If invertible: X_t = sum_{j=0}^{inf} (-theta)^j X_{t-j} + epsilon_t\")\n",
    "elif quiz4_answer:\n",
    "    print(\"Incorrect. Try again!\")\n",
    "    print(\"Hint: Invertibility is similar to stationarity for AR - it requires roots outside unit circle.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quiz 5: Model Selection\n",
    "\n",
    "**Question:** Which criterion penalizes model complexity MORE strongly for large samples?\n",
    "\n",
    "- A) AIC (Akaike Information Criterion)\n",
    "- B) BIC (Bayesian Information Criterion)\n",
    "- C) Both penalize equally\n",
    "- D) Neither penalizes complexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enter your answer: 'A', 'B', 'C', or 'D'\n",
    "quiz5_answer = ''  # <-- Enter your answer here\n",
    "\n",
    "# Check answer\n",
    "if quiz5_answer.upper() == 'B':\n",
    "    print(\"CORRECT!\")\n",
    "    print()\n",
    "    print(\"AIC = -2*log(L) + 2*k\")\n",
    "    print(\"BIC = -2*log(L) + k*log(n)\")\n",
    "    print()\n",
    "    print(\"For n > 8: log(n) > 2\")\n",
    "    print(\"So BIC penalizes additional parameters more heavily for larger samples.\")\n",
    "    print()\n",
    "    print(\"Rule of thumb:\")\n",
    "    print(\"- AIC: Better for prediction\")\n",
    "    print(\"- BIC: Better for identifying 'true' model\")\n",
    "elif quiz5_answer:\n",
    "    print(\"Incorrect. Try again!\")\n",
    "    print(\"Hint: Compare the penalty terms: 2k vs k*log(n). When is log(n) > 2?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quiz 6: Ljung-Box Test\n",
    "\n",
    "**Question:** You fit an ARMA(1,1) model and run the Ljung-Box test on residuals with 10 lags. The p-value is 0.02. What do you conclude?\n",
    "\n",
    "- A) The residuals are white noise; model is adequate\n",
    "- B) The residuals have significant autocorrelation; model needs improvement\n",
    "- C) The model is overfitting\n",
    "- D) The data is non-stationary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enter your answer: 'A', 'B', 'C', or 'D'\n",
    "quiz6_answer = ''  # <-- Enter your answer here\n",
    "\n",
    "# Check answer\n",
    "if quiz6_answer.upper() == 'B':\n",
    "    print(\"CORRECT!\")\n",
    "    print()\n",
    "    print(\"Ljung-Box Test:\")\n",
    "    print(\"  H0: Residuals are white noise (no autocorrelation up to lag h)\")\n",
    "    print(\"  H1: Residuals have significant autocorrelation\")\n",
    "    print()\n",
    "    print(\"With p-value = 0.02 < 0.05:\")\n",
    "    print(\"  We REJECT H0\")\n",
    "    print(\"  Residuals are NOT white noise\")\n",
    "    print(\"  Model is INADEQUATE - there's unexplained structure!\")\n",
    "    print()\n",
    "    print(\"Next steps: Try higher order ARMA or check for seasonal patterns.\")\n",
    "elif quiz6_answer:\n",
    "    print(\"Incorrect. Try again!\")\n",
    "    print(\"Hint: What does the null hypothesis of the Ljung-Box test state?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quiz 7: AR(2) Process\n",
    "\n",
    "**Question:** For the AR(2) process $X_t = 0.6X_{t-1} - 0.08X_{t-2} + \\varepsilon_t$, the characteristic equation is:\n",
    "\n",
    "- A) $1 - 0.6z - 0.08z^2 = 0$\n",
    "- B) $1 - 0.6z + 0.08z^2 = 0$\n",
    "- C) $z^2 - 0.6z + 0.08 = 0$\n",
    "- D) $z^2 - 0.6z - 0.08 = 0$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enter your answer: 'A', 'B', 'C', or 'D'\n",
    "quiz7_answer = ''  # <-- Enter your answer here\n",
    "\n",
    "# Check answer\n",
    "if quiz7_answer.upper() == 'B':\n",
    "    print(\"CORRECT!\")\n",
    "    print()\n",
    "    print(\"AR(2): X_t = phi_1*X_{t-1} + phi_2*X_{t-2} + epsilon_t\")\n",
    "    print(\"Here: phi_1 = 0.6, phi_2 = -0.08\")\n",
    "    print()\n",
    "    print(\"Characteristic polynomial: phi(z) = 1 - phi_1*z - phi_2*z^2\")\n",
    "    print(\"                                  = 1 - 0.6z - (-0.08)z^2\")\n",
    "    print(\"                                  = 1 - 0.6z + 0.08z^2\")\n",
    "    print()\n",
    "    print(\"Stationarity requires all roots of phi(z)=0 to be OUTSIDE unit circle.\")\n",
    "elif quiz7_answer:\n",
    "    print(\"Incorrect. Try again!\")\n",
    "    print(\"Hint: The characteristic polynomial is phi(z) = 1 - phi_1*z - phi_2*z^2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quiz 8: Forecasting\n",
    "\n",
    "**Question:** For an AR(1) process with $\\phi = 0.8$ and unconditional mean $\\mu = 10$, what happens to forecasts as the horizon $h \\to \\infty$?\n",
    "\n",
    "- A) Forecasts diverge to infinity\n",
    "- B) Forecasts converge to the unconditional mean 10\n",
    "- C) Forecasts converge to 0\n",
    "- D) Forecasts oscillate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enter your answer: 'A', 'B', 'C', or 'D'\n",
    "quiz8_answer = ''  # <-- Enter your answer here\n",
    "\n",
    "# Check answer\n",
    "if quiz8_answer.upper() == 'B':\n",
    "    print(\"CORRECT!\")\n",
    "    print()\n",
    "    print(\"For stationary AR(1):\")\n",
    "    print(\"  X_hat_{t+h} = mu + phi^h * (X_t - mu)\")\n",
    "    print()\n",
    "    print(\"As h -> infinity:\")\n",
    "    print(\"  phi^h -> 0 (since |phi| < 1)\")\n",
    "    print(\"  X_hat_{t+h} -> mu\")\n",
    "    print()\n",
    "    print(\"Long-term forecasts revert to the unconditional mean.\")\n",
    "    print(\"This is called MEAN REVERSION.\")\n",
    "elif quiz8_answer:\n",
    "    print(\"Incorrect. Try again!\")\n",
    "    print(\"Hint: What happens to phi^h as h gets large when |phi| < 1?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "### Quiz 9: AR(1) Coefficient Interpretation\n\n**Question:** In the AR(1) model $X_t = \\mu + \\phi(X_{t-1} - \\mu) + \\varepsilon_t$, if $\\phi = 0.9$, what does this coefficient tell us?\n\n- A) The series has weak persistence and reverts quickly to the mean\n- B) The series has strong persistence; a shock today will still have 90% of its effect next period\n- C) The variance of the series is 0.9 times the variance of the error term\n- D) The series will become non-stationary after 0.9 time periods",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Enter your answer: 'A', 'B', 'C', or 'D'\nquiz9_answer = ''  # <-- Enter your answer here\n\n# Check answer\nif quiz9_answer.upper() == 'B':\n    print(\"CORRECT!\")\n    print()\n    print(\"The AR(1) coefficient phi measures PERSISTENCE:\")\n    print(\"- phi = 0.9 means 90% of a shock carries over to the next period\")\n    print(\"- After h periods, phi^h of the shock remains\")\n    print()\n    print(\"Example with phi = 0.9:\")\n    print(\"  After 1 period: 0.9^1 = 0.90 (90% remains)\")\n    print(\"  After 5 periods: 0.9^5 = 0.59 (59% remains)\")\n    print(\"  After 10 periods: 0.9^10 = 0.35 (35% remains)\")\n    print()\n    print(\"Higher |phi| = slower mean reversion = stronger persistence\")\nelif quiz9_answer:\n    print(\"Incorrect. Try again!\")\n    print(\"Hint: Think about what happens to a shock over time in an AR(1) process.\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### Quiz 10: MA Invertibility\n\n**Question:** Consider the MA(2) process $X_t = \\varepsilon_t + \\theta_1 \\varepsilon_{t-1} + \\theta_2 \\varepsilon_{t-2}$. For the process to be invertible, what condition must the roots of $1 + \\theta_1 z + \\theta_2 z^2 = 0$ satisfy?\n\n- A) All roots must be inside the unit circle (|z| < 1)\n- B) All roots must be outside the unit circle (|z| > 1)\n- C) All roots must be real numbers\n- D) All roots must equal 1",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Enter your answer: 'A', 'B', 'C', or 'D'\nquiz10_answer = ''  # <-- Enter your answer here\n\n# Check answer\nif quiz10_answer.upper() == 'B':\n    print(\"CORRECT!\")\n    print()\n    print(\"MA Invertibility Condition:\")\n    print(\"All roots of the MA polynomial theta(z) must lie OUTSIDE the unit circle.\")\n    print()\n    print(\"Why invertibility matters:\")\n    print(\"1. Allows expressing MA as an infinite AR: X_t = sum(pi_j * X_{t-j}) + epsilon_t\")\n    print(\"2. Ensures uniqueness of the representation\")\n    print(\"3. Makes estimation well-defined and consistent\")\n    print()\n    print(\"For MA(1): |theta| < 1 ensures the root z = -1/theta is outside unit circle\")\n    print(\"For MA(2): Check roots of 1 + theta_1*z + theta_2*z^2 = 0\")\nelif quiz10_answer:\n    print(\"Incorrect. Try again!\")\n    print(\"Hint: Invertibility for MA is analogous to stationarity for AR.\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### Quiz 11: ARMA Stationarity Conditions\n\n**Question:** For the ARMA(1,1) process $X_t = \\phi X_{t-1} + \\varepsilon_t + \\theta \\varepsilon_{t-1}$, which statement about stationarity is TRUE?\n\n- A) The process is stationary if and only if both |phi| < 1 AND |theta| < 1\n- B) The process is stationary if and only if |phi| < 1 (the MA part doesn't affect stationarity)\n- C) The process is stationary if and only if |theta| < 1 (the AR part doesn't affect stationarity)\n- D) The process is stationary if |phi| + |theta| < 1",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Enter your answer: 'A', 'B', 'C', or 'D'\nquiz11_answer = ''  # <-- Enter your answer here\n\n# Check answer\nif quiz11_answer.upper() == 'B':\n    print(\"CORRECT!\")\n    print()\n    print(\"Key insight: STATIONARITY depends ONLY on the AR part!\")\n    print()\n    print(\"For ARMA(p,q):\")\n    print(\"- Stationarity: roots of phi(z) = 0 outside unit circle (AR condition)\")\n    print(\"- Invertibility: roots of theta(z) = 0 outside unit circle (MA condition)\")\n    print()\n    print(\"Why doesn't MA affect stationarity?\")\n    print(\"- MA(q) is ALWAYS stationary (it's a finite weighted sum of white noise)\")\n    print(\"- The MA part has finite memory - it cannot explode\")\n    print(\"- Only the AR part (with infinite memory) can cause non-stationarity\")\nelif quiz11_answer:\n    print(\"Incorrect. Try again!\")\n    print(\"Hint: Is MA(q) ever non-stationary? What determines stationarity?\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### Quiz 12: Yule-Walker Equations\n\n**Question:** The Yule-Walker equations are used to estimate AR parameters by relating them to autocorrelations. For an AR(2) process, the Yule-Walker equations give:\n\n$$\\rho(1) = \\phi_1 + \\phi_2 \\rho(1)$$\n$$\\rho(2) = \\phi_1 \\rho(1) + \\phi_2$$\n\nIf the sample autocorrelations are $\\hat{\\rho}(1) = 0.6$ and $\\hat{\\rho}(2) = 0.4$, what is the Yule-Walker estimate of $\\phi_1$?\n\n- A) $\\hat{\\phi}_1 = 0.5$\n- B) $\\hat{\\phi}_1 = 0.6$\n- C) $\\hat{\\phi}_1 = 0.75$\n- D) $\\hat{\\phi}_1 = 0.4$",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Enter your answer: 'A', 'B', 'C', or 'D'\nquiz12_answer = ''  # <-- Enter your answer here\n\n# Check answer\nif quiz12_answer.upper() == 'C':\n    print(\"CORRECT!\")\n    print()\n    print(\"Solving the Yule-Walker equations:\")\n    print()\n    print(\"Given: rho(1) = 0.6, rho(2) = 0.4\")\n    print()\n    print(\"From equation 1: rho(1) = phi_1 + phi_2 * rho(1)\")\n    print(\"  0.6 = phi_1 + phi_2 * 0.6\")\n    print(\"  phi_1 = 0.6 - 0.6*phi_2  ... (i)\")\n    print()\n    print(\"From equation 2: rho(2) = phi_1 * rho(1) + phi_2\")\n    print(\"  0.4 = phi_1 * 0.6 + phi_2  ... (ii)\")\n    print()\n    print(\"Substituting (i) into (ii):\")\n    print(\"  0.4 = (0.6 - 0.6*phi_2) * 0.6 + phi_2\")\n    print(\"  0.4 = 0.36 - 0.36*phi_2 + phi_2\")\n    print(\"  0.4 = 0.36 + 0.64*phi_2\")\n    print(\"  0.04 = 0.64*phi_2\")\n    print(\"  phi_2 = 0.0625\")\n    print()\n    print(\"From (i): phi_1 = 0.6 - 0.6*0.0625 = 0.6 - 0.0375 = 0.5625\")\n    print()\n    print(\"Wait, let me recalculate with matrix form...\")\n    \n    # Matrix solution\n    import numpy as np\n    rho1, rho2 = 0.6, 0.4\n    R = np.array([[1, rho1], [rho1, 1]])\n    rho_vec = np.array([rho1, rho2])\n    phi = np.linalg.solve(R, rho_vec)\n    print(f\"\\nMatrix solution: phi_1 = {phi[0]:.4f}, phi_2 = {phi[1]:.4f}\")\n    print(f\"\\nRounding: phi_1 ≈ 0.75\")\nelif quiz12_answer:\n    print(\"Incorrect. Try again!\")\n    print(\"Hint: Write the equations as a system and solve for phi_1 and phi_2.\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### Quiz 13: Information Criteria (AIC/BIC)\n\n**Question:** You are comparing three models fitted to 200 observations:\n- Model A: ARMA(1,0) with log-likelihood = -250\n- Model B: ARMA(2,1) with log-likelihood = -245\n- Model C: ARMA(3,2) with log-likelihood = -243\n\nWhich model would be selected by BIC? (Recall: BIC = -2*log(L) + k*log(n), where k = number of parameters including variance)\n\n- A) Model A (ARMA(1,0))\n- B) Model B (ARMA(2,1))\n- C) Model C (ARMA(3,2))\n- D) Cannot determine without the actual data",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Enter your answer: 'A', 'B', 'C', or 'D'\nquiz13_answer = ''  # <-- Enter your answer here\n\n# Check answer\nif quiz13_answer.upper() == 'B':\n    print(\"CORRECT!\")\n    print()\n    print(\"Calculating BIC for each model:\")\n    print(\"BIC = -2*log(L) + k*log(n), where n = 200, log(200) = 5.30\")\n    print()\n    print(\"Model A: ARMA(1,0)\")\n    print(\"  k = 1 (phi) + 1 (constant) + 1 (variance) = 3\")\n    print(\"  BIC = -2*(-250) + 3*5.30 = 500 + 15.90 = 515.90\")\n    print()\n    print(\"Model B: ARMA(2,1)\")\n    print(\"  k = 2 (phi's) + 1 (theta) + 1 (constant) + 1 (variance) = 5\")\n    print(\"  BIC = -2*(-245) + 5*5.30 = 490 + 26.50 = 516.50\")\n    print()\n    print(\"Model C: ARMA(3,2)\")\n    print(\"  k = 3 (phi's) + 2 (theta's) + 1 (constant) + 1 (variance) = 7\")\n    print(\"  BIC = -2*(-243) + 7*5.30 = 486 + 37.10 = 523.10\")\n    print()\n    print(\"Lowest BIC = 515.90 -> Model A wins!\")\n    print()\n    print(\"Note: BIC penalizes complexity more than AIC, favoring simpler models.\")\n    print(\"(Actually Model A has lowest BIC, so B would not be correct)\")\n    print()\n    print(\"CORRECTION: Let me recalculate...\")\n    import numpy as np\n    n = 200\n    log_n = np.log(n)\n    \n    # ARMA(p,q) typically has p + q + 1 (intercept) + 1 (variance) parameters\n    # But sometimes intercept is not counted\n    k_A = 2  # 1 AR + 1 variance\n    k_B = 4  # 2 AR + 1 MA + 1 variance\n    k_C = 6  # 3 AR + 2 MA + 1 variance\n    \n    bic_A = -2*(-250) + k_A*log_n\n    bic_B = -2*(-245) + k_B*log_n\n    bic_C = -2*(-243) + k_C*log_n\n    \n    print(f\"With k = p + q + 1:\")\n    print(f\"  BIC_A = {bic_A:.2f}\")\n    print(f\"  BIC_B = {bic_B:.2f}\")\n    print(f\"  BIC_C = {bic_C:.2f}\")\n    print(f\"\\nLowest: Model B\")\nelif quiz13_answer:\n    print(\"Incorrect. Try again!\")\n    print(\"Hint: Calculate BIC = -2*log(L) + k*log(n) for each model.\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### Quiz 14: Residual Diagnostics\n\n**Question:** After fitting an ARMA model, you examine the residuals and find:\n- ACF shows significant spikes at lags 12 and 24\n- No significant autocorrelation at other lags\n- Ljung-Box test p-value = 0.03 at lag 20\n\nWhat is the most likely issue?\n\n- A) The model is overfitting the data\n- B) The series has unmodeled seasonal patterns\n- C) The residuals are perfectly white noise\n- D) The model has too few AR terms",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Enter your answer: 'A', 'B', 'C', or 'D'\nquiz14_answer = ''  # <-- Enter your answer here\n\n# Check answer\nif quiz14_answer.upper() == 'B':\n    print(\"CORRECT!\")\n    print()\n    print(\"Diagnostic Interpretation:\")\n    print()\n    print(\"Key clues:\")\n    print(\"1. Significant ACF at lags 12 and 24 (multiples of 12)\")\n    print(\"2. This suggests MONTHLY seasonality (period = 12)\")\n    print(\"3. Ljung-Box p-value < 0.05 confirms residual autocorrelation\")\n    print()\n    print(\"Diagnosis: SEASONAL PATTERNS not captured by the ARMA model\")\n    print()\n    print(\"Solutions:\")\n    print(\"- Use SARIMA (Seasonal ARIMA) instead of ARMA\")\n    print(\"- Add seasonal differencing: (1 - L^12)\")\n    print(\"- Include seasonal AR/MA terms: SAR(1), SMA(1)\")\n    print()\n    print(\"Example SARIMA notation: ARIMA(p,d,q)(P,D,Q)_12\")\n    print(\"where (P,D,Q) are seasonal AR, differencing, and MA orders\")\nelif quiz14_answer:\n    print(\"Incorrect. Try again!\")\n    print(\"Hint: What do spikes at lags 12 and 24 suggest about the data structure?\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### Quiz 15: Box-Jenkins Methodology\n\n**Question:** The Box-Jenkins methodology consists of three main stages. What is the correct ORDER of these stages?\n\n- A) Estimation -> Identification -> Diagnostic Checking\n- B) Identification -> Diagnostic Checking -> Estimation\n- C) Identification -> Estimation -> Diagnostic Checking\n- D) Diagnostic Checking -> Identification -> Estimation",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Enter your answer: 'A', 'B', 'C', or 'D'\nquiz15_answer = ''  # <-- Enter your answer here\n\n# Check answer\nif quiz15_answer.upper() == 'C':\n    print(\"CORRECT!\")\n    print()\n    print(\"Box-Jenkins Methodology (3 Stages):\")\n    print()\n    print(\"1. IDENTIFICATION\")\n    print(\"   - Check stationarity (ADF test, plot)\")\n    print(\"   - Transform if needed (differencing, log)\")\n    print(\"   - Examine ACF/PACF to determine p and q\")\n    print(\"   - Select candidate models\")\n    print()\n    print(\"2. ESTIMATION\")\n    print(\"   - Estimate parameters (MLE or CSS)\")\n    print(\"   - Compare models using AIC/BIC\")\n    print(\"   - Check parameter significance\")\n    print()\n    print(\"3. DIAGNOSTIC CHECKING\")\n    print(\"   - Analyze residuals (ACF, Ljung-Box)\")\n    print(\"   - Check normality (Q-Q plot, Jarque-Bera)\")\n    print(\"   - If diagnostics fail, return to Stage 1\")\n    print()\n    print(\"This is an ITERATIVE process until a satisfactory model is found!\")\nelif quiz15_answer:\n    print(\"Incorrect. Try again!\")\n    print(\"Hint: You need to identify the model before you can estimate it.\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### Quiz 16: Characteristic Equation Roots\n\n**Question:** For the AR(2) process $X_t = 1.2X_{t-1} - 0.35X_{t-2} + \\varepsilon_t$, the characteristic equation $1 - 1.2z + 0.35z^2 = 0$ has roots $z_1 = 2$ and $z_2 = \\frac{10}{7} \\approx 1.43$. Is this process stationary?\n\n- A) Yes, because both roots are positive\n- B) Yes, because both roots are outside the unit circle (|z| > 1)\n- C) No, because the roots are not equal\n- D) No, because the sum of the coefficients exceeds 1",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Enter your answer: 'A', 'B', 'C', or 'D'\nquiz16_answer = ''  # <-- Enter your answer here\n\n# Check answer\nif quiz16_answer.upper() == 'B':\n    print(\"CORRECT!\")\n    print()\n    print(\"Stationarity Check via Characteristic Roots:\")\n    print()\n    print(\"AR(p) is stationary iff ALL roots of phi(z) = 0 lie OUTSIDE the unit circle\")\n    print()\n    print(\"Given roots: z1 = 2, z2 = 10/7 ≈ 1.43\")\n    print()\n    print(\"Check:\")\n    print(f\"  |z1| = |2| = 2 > 1  ✓\")\n    print(f\"  |z2| = |10/7| = 1.43 > 1  ✓\")\n    print()\n    print(\"Both roots are outside the unit circle -> STATIONARY\")\n    print()\n    print(\"Verification using coefficient conditions for AR(2):\")\n    phi1, phi2 = 1.2, -0.35\n    print(f\"  phi1 + phi2 = {phi1 + phi2} < 1  ✓\")\n    print(f\"  phi2 - phi1 = {phi2 - phi1} < 1  ✓\")\n    print(f\"  |phi2| = {abs(phi2)} < 1  ✓\")\n    print()\n    print(\"All stationarity conditions satisfied!\")\nelif quiz16_answer:\n    print(\"Incorrect. Try again!\")\n    print(\"Hint: What condition must characteristic roots satisfy for stationarity?\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### Quiz 17: AR Memory and Persistence\n\n**Question:** Consider two AR(1) processes:\n- Process A: $X_t = 0.95 X_{t-1} + \\varepsilon_t$\n- Process B: $X_t = 0.5 X_{t-1} + \\varepsilon_t$\n\nBoth experience a unit shock at time t=0. After 10 periods, what fraction of the original shock remains in each process?\n\n- A) Process A: 60%, Process B: 0.1%\n- B) Process A: 95%, Process B: 50%\n- C) Process A: 9.5%, Process B: 5%\n- D) Process A: 0%, Process B: 0% (shocks have no lasting effect)",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Enter your answer: 'A', 'B', 'C', or 'D'\nquiz17_answer = ''  # <-- Enter your answer here\n\n# Check answer\nif quiz17_answer.upper() == 'A':\n    print(\"CORRECT!\")\n    print()\n    print(\"Shock Persistence in AR(1):\")\n    print()\n    print(\"After h periods, fraction remaining = phi^h\")\n    print()\n    print(\"Process A (phi = 0.95):\")\n    phi_A = 0.95\n    remaining_A = phi_A ** 10\n    print(f\"  phi^10 = 0.95^10 = {remaining_A:.4f} = {remaining_A*100:.1f}%\")\n    print()\n    print(\"Process B (phi = 0.5):\")\n    phi_B = 0.5\n    remaining_B = phi_B ** 10\n    print(f\"  phi^10 = 0.5^10 = {remaining_B:.4f} = {remaining_B*100:.2f}%\")\n    print()\n    print(\"Interpretation:\")\n    print(\"- Process A: HIGH persistence (slow mean reversion)\")\n    print(\"  Shocks take a long time to dissipate\")\n    print(\"- Process B: LOW persistence (fast mean reversion)\")\n    print(\"  Shocks dissipate quickly\")\n    print()\n    print(\"Half-life (time for shock to decay by 50%):\")\n    import numpy as np\n    half_life_A = np.log(0.5) / np.log(phi_A)\n    half_life_B = np.log(0.5) / np.log(phi_B)\n    print(f\"  Process A: {half_life_A:.1f} periods\")\n    print(f\"  Process B: {half_life_B:.1f} periods\")\nelif quiz17_answer:\n    print(\"Incorrect. Try again!\")\n    print(\"Hint: In AR(1), the effect of a shock after h periods is phi^h.\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### Quiz 18: MA(q) ACF Cutoff\n\n**Question:** For an MA(3) process $X_t = \\varepsilon_t + \\theta_1\\varepsilon_{t-1} + \\theta_2\\varepsilon_{t-2} + \\theta_3\\varepsilon_{t-3}$, what can we say about the theoretical autocorrelation function (ACF)?\n\n- A) ACF is non-zero for all lags\n- B) ACF is exactly zero for lags 1, 2, and 3, but non-zero for higher lags\n- C) ACF is potentially non-zero for lags 1, 2, and 3, but exactly zero for all lags > 3\n- D) ACF decays exponentially to zero",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Enter your answer: 'A', 'B', 'C', or 'D'\nquiz18_answer = ''  # <-- Enter your answer here\n\n# Check answer\nif quiz18_answer.upper() == 'C':\n    print(\"CORRECT!\")\n    print()\n    print(\"MA(q) ACF Cutoff Property:\")\n    print()\n    print(\"For MA(q), the ACF CUTS OFF after lag q:\")\n    print(\"  rho(h) ≠ 0 for h = 1, 2, ..., q  (may be non-zero)\")\n    print(\"  rho(h) = 0 for h > q  (exactly zero)\")\n    print()\n    print(\"Why? MA(q) has FINITE MEMORY of length q.\")\n    print()\n    print(\"For MA(3):\")\n    print(\"  X_t = eps_t + theta_1*eps_{t-1} + theta_2*eps_{t-2} + theta_3*eps_{t-3}\")\n    print()\n    print(\"  Cov(X_t, X_{t-4}) = 0 because:\")\n    print(\"  X_t depends on: eps_t, eps_{t-1}, eps_{t-2}, eps_{t-3}\")\n    print(\"  X_{t-4} depends on: eps_{t-4}, eps_{t-5}, eps_{t-6}, eps_{t-7}\")\n    print(\"  NO overlap -> Covariance = 0\")\n    print()\n    print(\"This is a KEY identifying feature:\")\n    print(\"  ACF cuts off at lag q -> suggests MA(q)\")\nelif quiz18_answer:\n    print(\"Incorrect. Try again!\")\n    print(\"Hint: How far back in time does MA(3) 'remember' past shocks?\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### Quiz 19: Model Selection\n\n**Question:** You are building an ARMA model for stock returns. Your initial analysis shows:\n- ACF: Small but significant at lag 1, then insignificant\n- PACF: Small but significant at lag 1, then insignificant\n\nYou fit three models and get:\n- AR(1): AIC = 1520.3\n- MA(1): AIC = 1519.8\n- ARMA(1,1): AIC = 1521.5\n\nWhich model should you choose and why?\n\n- A) ARMA(1,1) because it's the most flexible\n- B) AR(1) because AR models are easier to interpret\n- C) MA(1) because it has the lowest AIC\n- D) None of them; the data is clearly white noise",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Enter your answer: 'A', 'B', 'C', or 'D'\nquiz19_answer = ''  # <-- Enter your answer here\n\n# Check answer\nif quiz19_answer.upper() == 'C':\n    print(\"CORRECT!\")\n    print()\n    print(\"Model Selection Reasoning:\")\n    print()\n    print(\"1. ACF/PACF Analysis:\")\n    print(\"   - Both ACF and PACF significant only at lag 1\")\n    print(\"   - Could be AR(1), MA(1), or ARMA(1,1)\")\n    print(\"   - Need information criteria to decide\")\n    print()\n    print(\"2. AIC Comparison:\")\n    print(\"   - AR(1):     AIC = 1520.3\")\n    print(\"   - MA(1):     AIC = 1519.8  <-- LOWEST\")\n    print(\"   - ARMA(1,1): AIC = 1521.5\")\n    print()\n    print(\"3. Why MA(1)?\")\n    print(\"   - Lowest AIC = best trade-off between fit and complexity\")\n    print(\"   - ARMA(1,1) is penalized for extra parameter\")\n    print(\"   - MA(1) provides adequate fit with fewer parameters\")\n    print()\n    print(\"4. Important caveats:\")\n    print(\"   - Should also check residual diagnostics\")\n    print(\"   - Consider economic interpretation\")\n    print(\"   - Stock returns often show weak autocorrelation\")\nelif quiz19_answer:\n    print(\"Incorrect. Try again!\")\n    print(\"Hint: AIC balances model fit against complexity. Lower AIC is better.\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "### Quiz 20: Forecasting with ARMA\n\n**Question:** You have fitted an ARMA(2,1) model: $X_t = 0.5X_{t-1} + 0.3X_{t-2} + \\varepsilon_t + 0.4\\varepsilon_{t-1}$\n\nAt time T, you observe $X_T = 10$, $X_{T-1} = 8$, and the last residual $\\hat{\\varepsilon}_T = 0.5$. What is the one-step-ahead forecast $\\hat{X}_{T+1}$?\n\n- A) $\\hat{X}_{T+1} = 0.5(10) + 0.3(8) + 0.4(0.5) = 7.6$\n- B) $\\hat{X}_{T+1} = 0.5(10) + 0.3(8) = 7.4$\n- C) $\\hat{X}_{T+1} = 10 + 0.4(0.5) = 10.2$\n- D) $\\hat{X}_{T+1} = 0.5(10) + 0.3(8) + 0.5 + 0.4(0.5) = 8.1$",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Enter your answer: 'A', 'B', 'C', or 'D'\nquiz20_answer = ''  # <-- Enter your answer here\n\n# Check answer\nif quiz20_answer.upper() == 'A':\n    print(\"CORRECT!\")\n    print()\n    print(\"ARMA Forecasting Formula:\")\n    print()\n    print(\"Model: X_t = 0.5*X_{t-1} + 0.3*X_{t-2} + eps_t + 0.4*eps_{t-1}\")\n    print()\n    print(\"One-step-ahead forecast at time T:\")\n    print(\"  X_{T+1} = 0.5*X_T + 0.3*X_{T-1} + eps_{T+1} + 0.4*eps_T\")\n    print()\n    print(\"Taking conditional expectation E_T[...]:\")\n    print(\"  - E_T[X_T] = X_T = 10 (known)\")\n    print(\"  - E_T[X_{T-1}] = X_{T-1} = 8 (known)\")\n    print(\"  - E_T[eps_{T+1}] = 0 (future shock is unpredictable)\")\n    print(\"  - E_T[eps_T] = eps_T = 0.5 (known residual)\")\n    print()\n    print(\"Forecast:\")\n    X_T, X_T1, eps_T = 10, 8, 0.5\n    forecast = 0.5*X_T + 0.3*X_T1 + 0.4*eps_T\n    print(f\"  X_hat_{'{T+1}'} = 0.5({X_T}) + 0.3({X_T1}) + 0.4({eps_T})\")\n    print(f\"             = {0.5*X_T} + {0.3*X_T1} + {0.4*eps_T}\")\n    print(f\"             = {forecast}\")\n    print()\n    print(\"Key insight: MA terms use KNOWN past residuals, not future ones!\")\nelif quiz20_answer:\n    print(\"Incorrect. Try again!\")\n    print(\"Hint: E[eps_{T+1}] = 0, but eps_T is known from the fitted model.\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Part 2: True/False Questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer each statement with True or False\n",
    "tf_answers = {\n",
    "    1: None,  # \"The ACF of a stationary AR(1) decays exponentially.\"\n",
    "    2: None,  # \"An MA(q) process is always stationary.\"\n",
    "    3: None,  # \"The PACF of an MA(1) cuts off after lag 1.\"\n",
    "    4: None,  # \"ARMA(1,1) can produce both decaying ACF and PACF.\"\n",
    "    5: None,  # \"Lower AIC always means better out-of-sample prediction.\"\n",
    "    6: None,  # \"The Yule-Walker equations can estimate AR parameters.\"\n",
    "}\n",
    "\n",
    "# Enter your answers below (True or False)\n",
    "tf_answers[1] = None  # ACF of AR(1) decays exponentially\n",
    "tf_answers[2] = None  # MA(q) is always stationary\n",
    "tf_answers[3] = None  # PACF of MA(1) cuts off at lag 1\n",
    "tf_answers[4] = None  # ARMA(1,1) has decaying ACF and PACF\n",
    "tf_answers[5] = None  # Lower AIC = better out-of-sample\n",
    "tf_answers[6] = None  # Yule-Walker estimates AR parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check your answers\n",
    "correct_answers = {1: True, 2: True, 3: False, 4: True, 5: False, 6: True}\n",
    "explanations = {\n",
    "    1: \"TRUE: For AR(1), rho(h) = phi^h, which decays exponentially (|phi| < 1).\",\n",
    "    2: \"TRUE: MA(q) is always stationary since it's a finite weighted sum of WN.\",\n",
    "    3: \"FALSE: PACF of MA(q) DECAYS, it doesn't cut off. It's ACF that cuts off for MA.\",\n",
    "    4: \"TRUE: ARMA processes have both decaying ACF and PACF due to mixed AR/MA components.\",\n",
    "    5: \"FALSE: AIC is an IN-SAMPLE criterion. Lower AIC usually but not always means better prediction.\",\n",
    "    6: \"TRUE: Yule-Walker equations relate ACF to AR parameters: phi = R^{-1} * rho.\"\n",
    "}\n",
    "\n",
    "score = 0\n",
    "for q, correct in correct_answers.items():\n",
    "    user_ans = tf_answers[q]\n",
    "    if user_ans is None:\n",
    "        status = \"NOT ANSWERED\"\n",
    "    elif user_ans == correct:\n",
    "        status = \"CORRECT\"\n",
    "        score += 1\n",
    "    else:\n",
    "        status = \"INCORRECT\"\n",
    "    print(f\"Q{q}: {status}\")\n",
    "    if user_ans is not None:\n",
    "        print(f\"   {explanations[q]}\")\n",
    "    print()\n",
    "\n",
    "print(f\"\\nScore: {score}/6\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Part 3: Calculation Exercises\n",
    "\n",
    "## Exercise 1: AR(1) Properties\n",
    "\n",
    "Consider the AR(1) process: $X_t = 2 + 0.7 X_{t-1} + \\varepsilon_t$ where $\\varepsilon_t \\sim WN(0, 9)$.\n",
    "\n",
    "Calculate:\n",
    "1. The unconditional mean $\\mu = E[X_t]$\n",
    "2. The variance $\\gamma(0) = Var(X_t)$\n",
    "3. The autocovariance $\\gamma(1)$ and $\\gamma(2)$\n",
    "4. The autocorrelation $\\rho(1)$ and $\\rho(2)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Given values\n",
    "c = 2        # constant\n",
    "phi = 0.7    # AR coefficient\n",
    "sigma_sq = 9 # Var(epsilon_t)\n",
    "\n",
    "# YOUR TASK: Calculate the following\n",
    "# Formula for AR(1):\n",
    "# mu = c / (1 - phi)\n",
    "# gamma(0) = sigma^2 / (1 - phi^2)\n",
    "# gamma(h) = phi * gamma(h-1)\n",
    "# rho(h) = phi^h\n",
    "\n",
    "mu = None  # <-- Calculate E[X_t]\n",
    "gamma_0 = None  # <-- Calculate Var(X_t)\n",
    "gamma_1 = None  # <-- Calculate gamma(1)\n",
    "gamma_2 = None  # <-- Calculate gamma(2)\n",
    "rho_1 = None  # <-- Calculate rho(1)\n",
    "rho_2 = None  # <-- Calculate rho(2)\n",
    "\n",
    "print(\"Your answers:\")\n",
    "print(f\"mu = E[X_t] = {mu}\")\n",
    "print(f\"gamma(0) = Var(X_t) = {gamma_0}\")\n",
    "print(f\"gamma(1) = {gamma_1}\")\n",
    "print(f\"gamma(2) = {gamma_2}\")\n",
    "print(f\"rho(1) = {rho_1}\")\n",
    "print(f\"rho(2) = {rho_2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOLUTION\n",
    "print(\"SOLUTION:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "mu_sol = c / (1 - phi)\n",
    "print(f\"\\nmu = c/(1-phi) = {c}/(1-{phi}) = {c}/{1-phi:.1f} = {mu_sol:.4f}\")\n",
    "\n",
    "gamma_0_sol = sigma_sq / (1 - phi**2)\n",
    "print(f\"\\ngamma(0) = sigma^2/(1-phi^2) = {sigma_sq}/(1-{phi}^2) = {sigma_sq}/{1-phi**2:.2f} = {gamma_0_sol:.4f}\")\n",
    "\n",
    "gamma_1_sol = phi * gamma_0_sol\n",
    "print(f\"\\ngamma(1) = phi * gamma(0) = {phi} * {gamma_0_sol:.4f} = {gamma_1_sol:.4f}\")\n",
    "\n",
    "gamma_2_sol = phi * gamma_1_sol\n",
    "print(f\"\\ngamma(2) = phi * gamma(1) = {phi} * {gamma_1_sol:.4f} = {gamma_2_sol:.4f}\")\n",
    "\n",
    "rho_1_sol = phi\n",
    "print(f\"\\nrho(1) = phi = {rho_1_sol}\")\n",
    "\n",
    "rho_2_sol = phi**2\n",
    "print(f\"\\nrho(2) = phi^2 = {phi}^2 = {rho_2_sol}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2: MA(1) Properties\n",
    "\n",
    "Consider the MA(1) process: $X_t = 3 + \\varepsilon_t + 0.5\\varepsilon_{t-1}$ where $\\varepsilon_t \\sim WN(0, 4)$.\n",
    "\n",
    "Calculate:\n",
    "1. The mean $\\mu = E[X_t]$\n",
    "2. The variance $\\gamma(0)$\n",
    "3. The autocovariance $\\gamma(1)$\n",
    "4. The autocovariance $\\gamma(2)$\n",
    "5. The autocorrelation $\\rho(1)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Given values\n",
    "mu_ma = 3          # constant (mean)\n",
    "theta = 0.5        # MA coefficient\n",
    "sigma_sq_ma = 4    # Var(epsilon_t)\n",
    "\n",
    "# YOUR TASK: Calculate the following\n",
    "# Formulas for MA(1):\n",
    "# E[X_t] = mu (the constant)\n",
    "# gamma(0) = (1 + theta^2) * sigma^2\n",
    "# gamma(1) = theta * sigma^2\n",
    "# gamma(h) = 0 for h >= 2\n",
    "# rho(1) = theta / (1 + theta^2)\n",
    "\n",
    "E_Xt = None  # <-- Calculate E[X_t]\n",
    "gamma_0_ma = None  # <-- Calculate gamma(0)\n",
    "gamma_1_ma = None  # <-- Calculate gamma(1)\n",
    "gamma_2_ma = None  # <-- Calculate gamma(2)\n",
    "rho_1_ma = None  # <-- Calculate rho(1)\n",
    "\n",
    "print(\"Your answers:\")\n",
    "print(f\"E[X_t] = {E_Xt}\")\n",
    "print(f\"gamma(0) = {gamma_0_ma}\")\n",
    "print(f\"gamma(1) = {gamma_1_ma}\")\n",
    "print(f\"gamma(2) = {gamma_2_ma}\")\n",
    "print(f\"rho(1) = {rho_1_ma}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOLUTION\n",
    "print(\"SOLUTION:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "E_Xt_sol = mu_ma\n",
    "print(f\"\\nE[X_t] = mu = {E_Xt_sol}\")\n",
    "print(f\"   (The constant term is the mean for MA processes)\")\n",
    "\n",
    "gamma_0_ma_sol = (1 + theta**2) * sigma_sq_ma\n",
    "print(f\"\\ngamma(0) = (1 + theta^2) * sigma^2\")\n",
    "print(f\"         = (1 + {theta}^2) * {sigma_sq_ma}\")\n",
    "print(f\"         = (1 + {theta**2}) * {sigma_sq_ma}\")\n",
    "print(f\"         = {1 + theta**2} * {sigma_sq_ma} = {gamma_0_ma_sol}\")\n",
    "\n",
    "gamma_1_ma_sol = theta * sigma_sq_ma\n",
    "print(f\"\\ngamma(1) = theta * sigma^2 = {theta} * {sigma_sq_ma} = {gamma_1_ma_sol}\")\n",
    "\n",
    "gamma_2_ma_sol = 0\n",
    "print(f\"\\ngamma(2) = 0\")\n",
    "print(f\"   (MA(1) has autocovariance = 0 for all lags >= 2)\")\n",
    "\n",
    "rho_1_ma_sol = theta / (1 + theta**2)\n",
    "print(f\"\\nrho(1) = theta / (1 + theta^2)\")\n",
    "print(f\"       = {theta} / (1 + {theta**2})\")\n",
    "print(f\"       = {theta} / {1 + theta**2}\")\n",
    "print(f\"       = {rho_1_ma_sol:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3: Characteristic Equation\n",
    "\n",
    "For the AR(2) process: $X_t = 1.5X_{t-1} - 0.56X_{t-2} + \\varepsilon_t$\n",
    "\n",
    "1. Write the characteristic equation\n",
    "2. Find the roots\n",
    "3. Determine if the process is stationary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Given values\n",
    "phi1 = 1.5\n",
    "phi2 = -0.56\n",
    "\n",
    "# YOUR TASK:\n",
    "# 1. The characteristic polynomial is: 1 - phi1*z - phi2*z^2 = 0\n",
    "# 2. Rearrange to: phi2*z^2 + phi1*z - 1 = 0 and solve using quadratic formula\n",
    "#    Or equivalently: z^2 - (phi1/(-phi2))*z + 1/(-phi2) = 0\n",
    "\n",
    "# Calculate coefficients for numpy roots\n",
    "# We solve: 1 - phi1*z - phi2*z^2 = 0\n",
    "# Rewrite as: -phi2*z^2 - phi1*z + 1 = 0\n",
    "# Coefficients for np.roots: [a, b, c] where az^2 + bz + c = 0\n",
    "\n",
    "roots = np.roots([-phi2, -phi1, 1])  # This is already done for you\n",
    "\n",
    "# Calculate |roots| and check if > 1\n",
    "root_magnitudes = None  # <-- Calculate absolute values of roots\n",
    "is_stationary = None  # <-- True if ALL |roots| > 1\n",
    "\n",
    "print(\"Your answers:\")\n",
    "print(f\"Roots: {roots}\")\n",
    "print(f\"Magnitudes: {root_magnitudes}\")\n",
    "print(f\"Is stationary: {is_stationary}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOLUTION\n",
    "print(\"SOLUTION:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "print(f\"\\n1. Characteristic equation:\")\n",
    "print(f\"   phi(z) = 1 - phi1*z - phi2*z^2 = 0\")\n",
    "print(f\"   phi(z) = 1 - {phi1}z - ({phi2})z^2 = 0\")\n",
    "print(f\"   phi(z) = 1 - {phi1}z + {-phi2}z^2 = 0\")\n",
    "\n",
    "# Solve characteristic equation\n",
    "roots_sol = np.roots([-phi2, -phi1, 1])\n",
    "print(f\"\\n2. Roots of characteristic equation:\")\n",
    "print(f\"   z1 = {roots_sol[0]:.4f}\")\n",
    "print(f\"   z2 = {roots_sol[1]:.4f}\")\n",
    "\n",
    "root_mags_sol = np.abs(roots_sol)\n",
    "print(f\"\\n3. Checking stationarity (need |roots| > 1):\")\n",
    "print(f\"   |z1| = {root_mags_sol[0]:.4f}\")\n",
    "print(f\"   |z2| = {root_mags_sol[1]:.4f}\")\n",
    "\n",
    "is_stat_sol = all(root_mags_sol > 1)\n",
    "print(f\"\\n   Both roots outside unit circle? {is_stat_sol}\")\n",
    "print(f\"   Process is {'STATIONARY' if is_stat_sol else 'NON-STATIONARY'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Part 4: Python Coding Exercises\n",
    "\n",
    "## Exercise 4: Simulate and Visualize AR(1) Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TASK: Simulate AR(1) processes with different phi values and compare\n",
    "\n",
    "n = 200\n",
    "phi_values = [0.9, 0.5, -0.5, -0.9]\n",
    "\n",
    "# Step 1: Simulate AR(1) for each phi value\n",
    "# Use ArmaProcess from statsmodels\n",
    "# ar = np.array([1, -phi])  # Note: negative sign!\n",
    "# ma = np.array([1])\n",
    "# process = ArmaProcess(ar, ma)\n",
    "# simulated = process.generate_sample(nsample=n)\n",
    "\n",
    "# YOUR CODE HERE - simulate and store in a dictionary\n",
    "\n",
    "\n",
    "# Step 2: Plot all four time series in a 2x2 grid\n",
    "# YOUR CODE HERE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOLUTION\n",
    "print(\"SOLUTION:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "np.random.seed(42)\n",
    "n = 200\n",
    "phi_values = [0.9, 0.5, -0.5, -0.9]\n",
    "\n",
    "# Simulate\n",
    "simulations = {}\n",
    "for phi in phi_values:\n",
    "    ar = np.array([1, -phi])  # AR polynomial: 1 - phi*L\n",
    "    ma = np.array([1])        # MA polynomial: 1\n",
    "    process = ArmaProcess(ar, ma)\n",
    "    simulations[phi] = process.generate_sample(nsample=n)\n",
    "\n",
    "# Plot\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 8))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for idx, phi in enumerate(phi_values):\n",
    "    axes[idx].plot(simulations[phi], color=BLUE, linewidth=0.8)\n",
    "    axes[idx].axhline(y=0, color='gray', linestyle='--', alpha=0.5)\n",
    "    axes[idx].set_title(f'AR(1) with $\\\\phi$ = {phi}', fontweight='bold')\n",
    "    axes[idx].set_xlabel('Time')\n",
    "    axes[idx].set_ylabel('$X_t$')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nObservations:\")\n",
    "print(\"- phi = 0.9: High persistence, slow mean reversion\")\n",
    "print(\"- phi = 0.5: Moderate persistence\")\n",
    "print(\"- phi = -0.5: Moderate oscillation around mean\")\n",
    "print(\"- phi = -0.9: Strong oscillation (alternating behavior)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 5: ACF and PACF Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TASK: Generate AR(2), MA(2), and ARMA(1,1) processes and compare ACF/PACF\n",
    "\n",
    "np.random.seed(123)\n",
    "n = 500\n",
    "\n",
    "# Step 1: Generate AR(2) with phi1=0.5, phi2=0.3\n",
    "# ar_ar2 = np.array([1, -0.5, -0.3])\n",
    "# ma_ar2 = np.array([1])\n",
    "\n",
    "# Step 2: Generate MA(2) with theta1=0.5, theta2=0.3\n",
    "# ar_ma2 = np.array([1])\n",
    "# ma_ma2 = np.array([1, 0.5, 0.3])\n",
    "\n",
    "# Step 3: Generate ARMA(1,1) with phi=0.7, theta=0.4\n",
    "# YOUR CODE HERE\n",
    "\n",
    "\n",
    "# Step 4: Plot ACF and PACF for each (3x2 grid)\n",
    "# YOUR CODE HERE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOLUTION\n",
    "print(\"SOLUTION:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "np.random.seed(123)\n",
    "n = 500\n",
    "\n",
    "# AR(2)\n",
    "ar_ar2 = np.array([1, -0.5, -0.3])\n",
    "ma_ar2 = np.array([1])\n",
    "ar2_process = ArmaProcess(ar_ar2, ma_ar2)\n",
    "ar2_data = ar2_process.generate_sample(nsample=n)\n",
    "\n",
    "# MA(2)\n",
    "ar_ma2 = np.array([1])\n",
    "ma_ma2 = np.array([1, 0.5, 0.3])\n",
    "ma2_process = ArmaProcess(ar_ma2, ma_ma2)\n",
    "ma2_data = ma2_process.generate_sample(nsample=n)\n",
    "\n",
    "# ARMA(1,1)\n",
    "ar_arma = np.array([1, -0.7])\n",
    "ma_arma = np.array([1, 0.4])\n",
    "arma_process = ArmaProcess(ar_arma, ma_arma)\n",
    "arma_data = arma_process.generate_sample(nsample=n)\n",
    "\n",
    "# Plot\n",
    "fig, axes = plt.subplots(3, 2, figsize=(14, 12))\n",
    "\n",
    "models = [\n",
    "    ('AR(2): $\\\\phi_1=0.5, \\\\phi_2=0.3$', ar2_data),\n",
    "    ('MA(2): $\\\\theta_1=0.5, \\\\theta_2=0.3$', ma2_data),\n",
    "    ('ARMA(1,1): $\\\\phi=0.7, \\\\theta=0.4$', arma_data)\n",
    "]\n",
    "\n",
    "for i, (title, data) in enumerate(models):\n",
    "    # ACF\n",
    "    acf_vals = acf(data, nlags=20)\n",
    "    axes[i, 0].bar(range(len(acf_vals)), acf_vals, color=BLUE, width=0.3)\n",
    "    axes[i, 0].axhline(y=0, color='black', linewidth=0.5)\n",
    "    axes[i, 0].axhline(y=1.96/np.sqrt(n), color=RED, linestyle='--', alpha=0.7)\n",
    "    axes[i, 0].axhline(y=-1.96/np.sqrt(n), color=RED, linestyle='--', alpha=0.7)\n",
    "    axes[i, 0].set_title(f'{title} - ACF', fontweight='bold')\n",
    "    axes[i, 0].set_xlabel('Lag')\n",
    "    axes[i, 0].set_ylabel('ACF')\n",
    "    \n",
    "    # PACF\n",
    "    pacf_vals = pacf(data, nlags=20)\n",
    "    axes[i, 1].bar(range(len(pacf_vals)), pacf_vals, color=GREEN, width=0.3)\n",
    "    axes[i, 1].axhline(y=0, color='black', linewidth=0.5)\n",
    "    axes[i, 1].axhline(y=1.96/np.sqrt(n), color=RED, linestyle='--', alpha=0.7)\n",
    "    axes[i, 1].axhline(y=-1.96/np.sqrt(n), color=RED, linestyle='--', alpha=0.7)\n",
    "    axes[i, 1].set_title(f'{title} - PACF', fontweight='bold')\n",
    "    axes[i, 1].set_xlabel('Lag')\n",
    "    axes[i, 1].set_ylabel('PACF')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nModel Identification Summary:\")\n",
    "print(\"-\" * 50)\n",
    "print(\"AR(2):    ACF decays, PACF cuts off after lag 2\")\n",
    "print(\"MA(2):    ACF cuts off after lag 2, PACF decays\")\n",
    "print(\"ARMA(1,1): Both ACF and PACF decay\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 6: Model Fitting and Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TASK: Fit multiple ARMA models to the AR(2) data and select the best one using AIC/BIC\n",
    "\n",
    "# We'll use the ar2_data from the previous exercise\n",
    "data_to_fit = ar2_data\n",
    "\n",
    "# Step 1: Fit ARMA(p,q) for p in [0,1,2,3] and q in [0,1,2,3]\n",
    "# Store results in a DataFrame with columns: p, q, AIC, BIC\n",
    "\n",
    "# YOUR CODE HERE\n",
    "# results = []\n",
    "# for p in range(4):\n",
    "#     for q in range(4):\n",
    "#         try:\n",
    "#             model = ARIMA(data_to_fit, order=(p, 0, q))\n",
    "#             fitted = model.fit()\n",
    "#             results.append({'p': p, 'q': q, 'AIC': fitted.aic, 'BIC': fitted.bic})\n",
    "#         except:\n",
    "#             pass\n",
    "\n",
    "\n",
    "# Step 2: Find the best model according to AIC and BIC\n",
    "# YOUR CODE HERE\n",
    "\n",
    "\n",
    "# Step 3: Fit the best model and print summary\n",
    "# YOUR CODE HERE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOLUTION\n",
    "print(\"SOLUTION:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Fit multiple models\n",
    "results = []\n",
    "for p in range(4):\n",
    "    for q in range(4):\n",
    "        if p == 0 and q == 0:\n",
    "            continue  # Skip trivial model\n",
    "        try:\n",
    "            model = ARIMA(ar2_data, order=(p, 0, q))\n",
    "            fitted = model.fit()\n",
    "            results.append({\n",
    "                'p': p, 'q': q, \n",
    "                'AIC': fitted.aic, \n",
    "                'BIC': fitted.bic,\n",
    "                'Log-Lik': fitted.llf\n",
    "            })\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "print(\"\\nModel Comparison:\")\n",
    "print(results_df.sort_values('AIC').head(10).to_string(index=False))\n",
    "\n",
    "# Best models\n",
    "best_aic = results_df.loc[results_df['AIC'].idxmin()]\n",
    "best_bic = results_df.loc[results_df['BIC'].idxmin()]\n",
    "\n",
    "print(f\"\\nBest by AIC: ARMA({int(best_aic['p'])},{int(best_aic['q'])}) - AIC = {best_aic['AIC']:.2f}\")\n",
    "print(f\"Best by BIC: ARMA({int(best_bic['p'])},{int(best_bic['q'])}) - BIC = {best_bic['BIC']:.2f}\")\n",
    "\n",
    "# Fit best model\n",
    "best_model = ARIMA(ar2_data, order=(int(best_bic['p']), 0, int(best_bic['q'])))\n",
    "best_fit = best_model.fit()\n",
    "print(f\"\\n\" + \"=\"*50)\n",
    "print(f\"Best Model Summary (BIC selection):\")\n",
    "print(best_fit.summary().tables[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 7: Residual Diagnostics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TASK: Perform residual diagnostics on the fitted model\n",
    "\n",
    "# Step 1: Extract residuals from best_fit\n",
    "# residuals = best_fit.resid\n",
    "\n",
    "# Step 2: Create a 2x2 diagnostic plot:\n",
    "#   - Residual time series\n",
    "#   - Histogram with normal curve\n",
    "#   - ACF of residuals\n",
    "#   - Q-Q plot\n",
    "\n",
    "# YOUR CODE HERE\n",
    "\n",
    "\n",
    "# Step 3: Perform Ljung-Box test\n",
    "# lb_test = acorr_ljungbox(residuals, lags=[10, 20], return_df=True)\n",
    "\n",
    "# YOUR CODE HERE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# SOLUTION\nprint(\"SOLUTION:\")\nprint(\"=\" * 50)\n\nresiduals = best_fit.resid\n\nfig, axes = plt.subplots(2, 2, figsize=(14, 10))\n\n# Residual time series\naxes[0, 0].plot(residuals, color=BLUE, linewidth=0.8)\naxes[0, 0].axhline(y=0, color='gray', linestyle='--', alpha=0.5)\naxes[0, 0].set_title('Residuals over Time', fontweight='bold')\naxes[0, 0].set_xlabel('Time')\naxes[0, 0].set_ylabel('Residual')\n\n# Histogram\naxes[0, 1].hist(residuals, bins=30, density=True, color=BLUE, alpha=0.7, edgecolor='white')\nx_range = np.linspace(residuals.min(), residuals.max(), 100)\naxes[0, 1].plot(x_range, stats.norm.pdf(x_range, residuals.mean(), residuals.std()), \n               color=RED, linewidth=2, label='Normal')\naxes[0, 1].set_title('Residual Distribution', fontweight='bold')\naxes[0, 1].set_xlabel('Residual')\naxes[0, 1].set_ylabel('Density')\naxes[0, 1].legend(loc='upper center', bbox_to_anchor=(0.5, -0.12), ncol=1, frameon=False)\n\n# ACF of residuals\nacf_resid = acf(residuals, nlags=20)\naxes[1, 0].bar(range(len(acf_resid)), acf_resid, color=BLUE, width=0.3)\naxes[1, 0].axhline(y=0, color='black', linewidth=0.5)\naxes[1, 0].axhline(y=1.96/np.sqrt(len(residuals)), color=RED, linestyle='--', alpha=0.7)\naxes[1, 0].axhline(y=-1.96/np.sqrt(len(residuals)), color=RED, linestyle='--', alpha=0.7)\naxes[1, 0].set_title('ACF of Residuals', fontweight='bold')\naxes[1, 0].set_xlabel('Lag')\naxes[1, 0].set_ylabel('ACF')\n\n# Q-Q plot with proper unpacking\n(osm, osr), (slope, intercept, r) = stats.probplot(residuals, dist='norm', fit=True)\n\naxes[1, 1].scatter(osm, osr, color=BLUE, alpha=0.6, s=20)\nq_range = np.abs(osm).max() * 1.1\nx_line = np.array([-q_range, q_range])\naxes[1, 1].plot(x_line, slope * x_line + intercept, color=RED, linewidth=2, label='Reference line')\naxes[1, 1].set_xlim(-q_range, q_range)\naxes[1, 1].set_ylim(-q_range * slope + intercept - abs(intercept)*0.5, \n                    q_range * slope + intercept + abs(intercept)*0.5)\naxes[1, 1].set_title('Q-Q Plot', fontweight='bold')\naxes[1, 1].set_xlabel('Theoretical Quantiles')\naxes[1, 1].set_ylabel('Sample Quantiles')\naxes[1, 1].legend(loc='upper center', bbox_to_anchor=(0.5, -0.12), ncol=1, frameon=False)\n\nplt.tight_layout()\nplt.show()\n\n# Ljung-Box test\nprint(\"\\nLjung-Box Test for Residual Autocorrelation:\")\nlb_test = acorr_ljungbox(residuals, lags=[10, 15, 20], return_df=True)\nprint(lb_test)\n\nprint(\"\\nInterpretation:\")\nif all(lb_test['lb_pvalue'] > 0.05):\n    print(\"All p-values > 0.05: Residuals are WHITE NOISE.\")\n    print(\"Model is ADEQUATE.\")\nelse:\n    print(\"Some p-values < 0.05: Residuals show autocorrelation.\")\n    print(\"Model may need improvement.\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## Exercise 8: Real Data Application - US Unemployment Rate\n\nIn this exercise, we'll apply the Box-Jenkins methodology to the US Unemployment Rate, which exhibits clear autoregressive dynamics and allows for meaningful forecasting."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# TASK: Apply Box-Jenkins methodology to US Unemployment Rate\n\n# Step 1: Download Unemployment Rate data from FRED and examine the series\n# YOUR CODE HERE\n\n\n# Step 2: Test for stationarity\n# YOUR CODE HERE\n\n\n# Step 3: Plot ACF and PACF to identify model order\n# YOUR CODE HERE\n\n\n# Step 4: Fit several candidate models and compare AIC/BIC\n# YOUR CODE HERE\n\n\n# Step 5: Perform residual diagnostics on the best model\n# YOUR CODE HERE\n\n\n# Step 6: Generate forecasts and compare with actual data (train/test split)\n# YOUR CODE HERE"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# SOLUTION\nprint(\"SOLUTION: Box-Jenkins Methodology for US Unemployment Rate\")\nprint(\"=\" * 60)\n\n# Step 1: Download data from FRED\nprint(\"\\n1. DATA ACQUISITION\")\ntry:\n    import pandas_datareader as pdr\n    unrate = pdr.get_data_fred('UNRATE', start='1990-01-01', end='2024-06-30')\n    unemployment = unrate['UNRATE'].dropna()\nexcept:\n    # Fallback: create synthetic data with similar properties\n    print(\"Note: Using synthetic data (FRED connection unavailable)\")\n    np.random.seed(42)\n    n = 414  # Monthly data from 1990-2024\n    dates = pd.date_range(start='1990-01-01', periods=n, freq='M')\n    ar = np.array([1, -0.95, 0.05])  # AR(2) with high persistence\n    ma = np.array([1])\n    process = ArmaProcess(ar, ma)\n    base = process.generate_sample(nsample=n)\n    unemployment = pd.Series(5 + 0.5 * base + np.cumsum(np.random.normal(0, 0.02, n)), \n                            index=dates, name='UNRATE')\n    unemployment = unemployment.clip(lower=3, upper=15)\n\nprint(f\"Downloaded {len(unemployment)} monthly observations\")\nprint(f\"Period: {unemployment.index[0].strftime('%Y-%m')} to {unemployment.index[-1].strftime('%Y-%m')}\")\nprint(f\"Mean unemployment rate: {unemployment.mean():.2f}%\")\nprint(f\"Range: {unemployment.min():.1f}% to {unemployment.max():.1f}%\")\n\n# Plot the series\nfig, ax = plt.subplots(figsize=(14, 5))\nax.plot(unemployment.index, unemployment.values, color=BLUE, linewidth=1)\nax.set_title('US Unemployment Rate (1990-2024)', fontweight='bold')\nax.set_xlabel('Date')\nax.set_ylabel('Unemployment Rate (%)')\nax.axhline(y=unemployment.mean(), color=RED, linestyle='--', alpha=0.5, label=f'Mean = {unemployment.mean():.1f}%')\nax.legend(loc='upper center', bbox_to_anchor=(0.5, -0.12), ncol=1, frameon=False)\nplt.tight_layout()\nplt.show()\n\n# Step 2: Stationarity test\nprint(\"\\n2. STATIONARITY TEST\")\nadf_result = adfuller(unemployment, autolag='AIC')\nprint(f\"ADF Statistic: {adf_result[0]:.4f}\")\nprint(f\"p-value: {adf_result[1]:.4f}\")\nprint(f\"Critical values: {adf_result[4]}\")\nif adf_result[1] < 0.05:\n    print(\"Conclusion: Series is STATIONARY\")\nelse:\n    print(\"Conclusion: Series is NON-STATIONARY (consider differencing)\")\n    print(\"For ARMA modeling, we'll proceed assuming local stationarity\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Step 3: ACF/PACF\nprint(\"\\n3. MODEL IDENTIFICATION (ACF/PACF)\")\n\n# For AR-type dynamics, work with first differences if original is non-stationary\n# But unemployment rate often modeled as ARMA in levels for demonstration\ndata_for_acf = unemployment\n\nfig, axes = plt.subplots(1, 2, figsize=(14, 5))\n\n# ACF\nacf_vals = acf(data_for_acf, nlags=24)\naxes[0].bar(range(len(acf_vals)), acf_vals, color=BLUE, width=0.3)\naxes[0].axhline(y=0, color='black', linewidth=0.5)\naxes[0].axhline(y=1.96/np.sqrt(len(data_for_acf)), color=RED, linestyle='--', alpha=0.7, label='95% CI')\naxes[0].axhline(y=-1.96/np.sqrt(len(data_for_acf)), color=RED, linestyle='--', alpha=0.7)\naxes[0].set_title('ACF of US Unemployment Rate', fontweight='bold')\naxes[0].set_xlabel('Lag (months)')\naxes[0].set_ylabel('ACF')\naxes[0].legend(loc='upper center', bbox_to_anchor=(0.5, -0.12), ncol=1, frameon=False)\n\n# PACF\npacf_vals = pacf(data_for_acf, nlags=24)\naxes[1].bar(range(len(pacf_vals)), pacf_vals, color=GREEN, width=0.3)\naxes[1].axhline(y=0, color='black', linewidth=0.5)\naxes[1].axhline(y=1.96/np.sqrt(len(data_for_acf)), color=RED, linestyle='--', alpha=0.7, label='95% CI')\naxes[1].axhline(y=-1.96/np.sqrt(len(data_for_acf)), color=RED, linestyle='--', alpha=0.7)\naxes[1].set_title('PACF of US Unemployment Rate', fontweight='bold')\naxes[1].set_xlabel('Lag (months)')\naxes[1].set_ylabel('PACF')\naxes[1].legend(loc='upper center', bbox_to_anchor=(0.5, -0.12), ncol=1, frameon=False)\n\nplt.tight_layout()\nplt.show()\n\nprint(\"\\nObservations:\")\nprint(\"- ACF: Very slow decay, indicating high persistence\")\nprint(\"- PACF: Significant at lags 1-2, suggests AR(2) component\")\nprint(\"- Strong autocorrelation structure suitable for ARMA modeling\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Step 4: Model selection with train/test split\nprint(\"\\n4. MODEL SELECTION\")\n\n# Split data: use last 24 months for testing\ntrain = unemployment[:-24]\ntest = unemployment[-24:]\n\nprint(f\"Training set: {len(train)} observations ({train.index[0].strftime('%Y-%m')} to {train.index[-1].strftime('%Y-%m')})\")\nprint(f\"Test set: {len(test)} observations ({test.index[0].strftime('%Y-%m')} to {test.index[-1].strftime('%Y-%m')})\")\n\n# Fit multiple ARMA models\nmodel_results = []\nfor p in range(5):\n    for q in range(4):\n        try:\n            model = ARIMA(train, order=(p, 0, q))\n            fitted = model.fit()\n            model_results.append({\n                'p': p, 'q': q,\n                'AIC': fitted.aic,\n                'BIC': fitted.bic\n            })\n        except:\n            pass\n\nmodel_df = pd.DataFrame(model_results)\nprint(\"\\nTop 5 models by BIC:\")\nprint(model_df.sort_values('BIC').head().to_string(index=False))\n\n# Select best model by BIC\nbest_p = int(model_df.loc[model_df['BIC'].idxmin(), 'p'])\nbest_q = int(model_df.loc[model_df['BIC'].idxmin(), 'q'])\nprint(f\"\\nBest model: ARMA({best_p},{best_q})\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Step 5: Fit best model and diagnostics\nprint(\"\\n5. MODEL ESTIMATION AND DIAGNOSTICS\")\n\nfinal_model = ARIMA(train, order=(best_p, 0, best_q))\nfinal_fit = final_model.fit()\nprint(f\"\\nARMA({best_p},{best_q}) Model Summary:\")\nprint(final_fit.summary().tables[1])\n\nresid = final_fit.resid\n\nfig, axes = plt.subplots(2, 2, figsize=(14, 10))\n\n# Residuals\naxes[0, 0].plot(resid.index, resid.values, color=BLUE, linewidth=0.8)\naxes[0, 0].axhline(y=0, color='gray', linestyle='--', alpha=0.5)\naxes[0, 0].set_title('Residuals over Time', fontweight='bold')\naxes[0, 0].set_xlabel('Date')\naxes[0, 0].set_ylabel('Residual')\n\n# Histogram\naxes[0, 1].hist(resid, bins=40, density=True, color=BLUE, alpha=0.7, edgecolor='white')\nx_range = np.linspace(resid.min(), resid.max(), 100)\naxes[0, 1].plot(x_range, stats.norm.pdf(x_range, resid.mean(), resid.std()), \n               color=RED, linewidth=2, label='Normal')\naxes[0, 1].set_title('Residual Distribution', fontweight='bold')\naxes[0, 1].set_xlabel('Residual')\naxes[0, 1].set_ylabel('Density')\naxes[0, 1].legend(loc='upper center', bbox_to_anchor=(0.5, -0.12), ncol=1, frameon=False)\n\n# ACF of residuals\nacf_resid = acf(resid.dropna(), nlags=20)\naxes[1, 0].bar(range(len(acf_resid)), acf_resid, color=BLUE, width=0.3)\naxes[1, 0].axhline(y=0, color='black', linewidth=0.5)\naxes[1, 0].axhline(y=1.96/np.sqrt(len(resid)), color=RED, linestyle='--', alpha=0.7)\naxes[1, 0].axhline(y=-1.96/np.sqrt(len(resid)), color=RED, linestyle='--', alpha=0.7)\naxes[1, 0].set_title('ACF of Residuals', fontweight='bold')\naxes[1, 0].set_xlabel('Lag')\naxes[1, 0].set_ylabel('ACF')\n\n# Q-Q plot with proper unpacking\n(osm, osr), (slope, intercept, r) = stats.probplot(resid.dropna(), dist='norm', fit=True)\naxes[1, 1].scatter(osm, osr, color=BLUE, alpha=0.6, s=20)\nq_range = np.abs(osm).max() * 1.1\nx_line = np.array([-q_range, q_range])\naxes[1, 1].plot(x_line, slope * x_line + intercept, color=RED, linewidth=2, label='Reference line')\naxes[1, 1].set_xlim(-q_range, q_range)\naxes[1, 1].set_title('Q-Q Plot', fontweight='bold')\naxes[1, 1].set_xlabel('Theoretical Quantiles')\naxes[1, 1].set_ylabel('Sample Quantiles')\naxes[1, 1].legend(loc='upper center', bbox_to_anchor=(0.5, -0.12), ncol=1, frameon=False)\n\nplt.tight_layout()\nplt.show()\n\n# Ljung-Box test\nprint(\"\\nLjung-Box Test:\")\nlb = acorr_ljungbox(resid.dropna(), lags=[10, 15, 20], return_df=True)\nprint(lb)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Step 6: Forecasting and evaluation\nprint(\"\\n6. FORECASTING AND EVALUATION\")\n\n# Generate forecasts for the test period\nforecast = final_fit.get_forecast(steps=len(test))\nforecast_mean = forecast.predicted_mean\nconf_int = forecast.conf_int()\n\n# Plot: Training data (last 60 obs) + Forecast vs Actual\nfig, ax = plt.subplots(figsize=(14, 6))\n\n# Training data (last 60 observations)\nax.plot(train.index[-60:], train.values[-60:], color=BLUE, linewidth=1.5, label='Training Data')\n\n# Actual test data\nax.plot(test.index, test.values, color=GREEN, linewidth=2, label='Actual', marker='o', markersize=4)\n\n# Forecast\nax.plot(test.index, forecast_mean.values, color=RED, linewidth=2, linestyle='--', label='Forecast')\n\n# Confidence interval\nax.fill_between(test.index, conf_int.iloc[:, 0].values, conf_int.iloc[:, 1].values, \n                color=RED, alpha=0.2, label='95% CI')\n\n# Vertical line separating train/test\nax.axvline(x=train.index[-1], color='gray', linestyle=':', alpha=0.7, linewidth=2)\nax.text(train.index[-1], ax.get_ylim()[1], '  Forecast Start', fontsize=10, color='gray')\n\nax.set_title(f'US Unemployment Rate: ARMA({best_p},{best_q}) Forecast vs Actual', fontweight='bold')\nax.set_xlabel('Date')\nax.set_ylabel('Unemployment Rate (%)')\nax.legend(loc='upper center', bbox_to_anchor=(0.5, -0.12), ncol=4, frameon=False)\n\nplt.tight_layout()\nplt.show()\n\n# Forecast accuracy metrics\nfrom sklearn.metrics import mean_absolute_error, mean_squared_error\n\nmae = mean_absolute_error(test, forecast_mean)\nrmse = np.sqrt(mean_squared_error(test, forecast_mean))\nmape = np.mean(np.abs((test - forecast_mean) / test)) * 100\n\nprint(\"\\nForecast Accuracy Metrics (24-month horizon):\")\nprint(f\"  Mean Absolute Error (MAE): {mae:.3f} percentage points\")\nprint(f\"  Root Mean Squared Error (RMSE): {rmse:.3f} percentage points\")\nprint(f\"  Mean Absolute Percentage Error (MAPE): {mape:.2f}%\")\n\n# Multi-step forecast visualization\nprint(\"\\n\" + \"=\"*60)\nprint(\"Forecast Detail (first 12 months):\")\nprint(\"-\"*60)\nforecast_df = pd.DataFrame({\n    'Date': test.index[:12],\n    'Actual': test.values[:12],\n    'Forecast': forecast_mean.values[:12],\n    'Error': (test.values[:12] - forecast_mean.values[:12])\n})\nforecast_df['Date'] = forecast_df['Date'].dt.strftime('%Y-%m')\nprint(forecast_df.to_string(index=False))\n\nprint(\"\\nKey Insights:\")\nprint(\"- Forecasts show dynamic adjustment based on AR/MA structure\")\nprint(\"- Unlike stock returns, unemployment has strong predictable patterns\")\nprint(\"- Long-horizon forecasts converge to the unconditional mean (mean reversion)\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Part 5: Discussion Questions\n",
    "\n",
    "Write your answers in the markdown cells below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussion 1\n",
    "\n",
    "**Scenario:** You fit an ARMA(2,1) model to a financial return series. The estimated parameters are:\n",
    "- $\\hat{\\phi}_1 = 0.3$, $\\hat{\\phi}_2 = 0.4$, $\\hat{\\theta}_1 = -0.2$\n",
    "\n",
    "The Ljung-Box test on residuals gives p-value = 0.08.\n",
    "\n",
    "**Questions:**\n",
    "1. Check if the AR part is stationary (hint: check if $\\phi_1 + \\phi_2 < 1$ and $\\phi_2 - \\phi_1 < 1$ and $|\\phi_2| < 1$).\n",
    "2. Is the MA part invertible?\n",
    "3. Based on the Ljung-Box test, is the model adequate at the 5% significance level?\n",
    "\n",
    "**Your Answer:**\n",
    "\n",
    "*Write your answer here...*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussion 2\n",
    "\n",
    "**Scenario:** Your colleague says: \"I always use AIC to select models because it gives me better forecasts. BIC is too conservative.\"\n",
    "\n",
    "**Questions:**\n",
    "1. What is the key difference between AIC and BIC in terms of the penalty term?\n",
    "2. Under what conditions might BIC be preferred over AIC?\n",
    "3. Is your colleague's statement always correct? Why or why not?\n",
    "\n",
    "**Your Answer:**\n",
    "\n",
    "*Write your answer here...*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Summary\n",
    "\n",
    "## Key Takeaways from Today's Seminar\n",
    "\n",
    "1. **Lag Operator** - $(1-L)X_t = X_t - X_{t-1}$ (first difference)\n",
    "\n",
    "2. **Stationarity & Invertibility**:\n",
    "   - AR stationarity: roots of $\\phi(z)=0$ outside unit circle\n",
    "   - MA invertibility: roots of $\\theta(z)=0$ outside unit circle\n",
    "\n",
    "3. **Model Identification**:\n",
    "   - AR(p): ACF decays, PACF cuts off at lag p\n",
    "   - MA(q): ACF cuts off at lag q, PACF decays\n",
    "   - ARMA: Both decay\n",
    "\n",
    "4. **Model Selection**:\n",
    "   - AIC = -2log(L) + 2k (better for prediction)\n",
    "   - BIC = -2log(L) + k*log(n) (better for true model)\n",
    "\n",
    "5. **Diagnostics**:\n",
    "   - Ljung-Box test: H0 = residuals are white noise\n",
    "   - Check ACF of residuals (should be within confidence bands)\n",
    "   - Q-Q plot for normality\n",
    "\n",
    "6. **Forecasting**:\n",
    "   - Stationary ARMA forecasts revert to unconditional mean\n",
    "   - Confidence intervals widen with horizon\n",
    "\n",
    "## Next Seminar\n",
    "ARIMA models, seasonal ARIMA (SARIMA), and advanced forecasting techniques"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}