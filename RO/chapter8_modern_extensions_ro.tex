% Capitolul 8: Extensii Moderne ale Seriilor de Timp
% Prezentare academică de calitate Harvard
% Program de licență, Academia de Studii Economice din București

\documentclass[9pt, aspectratio=169, t]{beamer}

% Asigură încadrarea conținutului pe diapozitive
\setbeamersize{text margin left=8mm, text margin right=8mm}

%=============================================================================
% CONFIGURARE TEMĂ ȘI STIL
%=============================================================================
\usetheme{Madrid}
\usecolortheme{seahorse}

% Paletă de Culori Profesională
\definecolor{MainBlue}{RGB}{26, 58, 110}
\definecolor{AccentBlue}{RGB}{42, 82, 140}
\definecolor{IDAred}{RGB}{220, 53, 69}
\definecolor{DarkGray}{RGB}{51, 51, 51}
\definecolor{MediumGray}{RGB}{128, 128, 128}
\definecolor{LightGray}{RGB}{248, 248, 248}
\definecolor{VeryLightGray}{RGB}{235, 235, 235}
\definecolor{Crimson}{RGB}{220, 53, 69}
\definecolor{Forest}{RGB}{46, 125, 50}
\definecolor{Amber}{RGB}{181, 133, 63}
\definecolor{Orange}{RGB}{230, 126, 34}
\definecolor{HarvardCrimson}{RGB}{165, 28, 48}

\setbeamercolor{palette primary}{bg=MainBlue, fg=white}
\setbeamercolor{palette secondary}{bg=MainBlue!85, fg=white}
\setbeamercolor{palette tertiary}{bg=MainBlue!70, fg=white}
\setbeamercolor{structure}{fg=MainBlue}
\setbeamercolor{title}{fg=MainBlue}
\setbeamercolor{frametitle}{fg=MainBlue, bg=white}
\setbeamercolor{block title}{bg=MainBlue, fg=white}
\setbeamercolor{block body}{bg=VeryLightGray, fg=DarkGray}
\setbeamercolor{block title alerted}{bg=Crimson, fg=white}
\setbeamercolor{block body alerted}{bg=Crimson!8, fg=DarkGray}
\setbeamercolor{block title example}{bg=Forest, fg=white}
\setbeamercolor{block body example}{bg=Forest!8, fg=DarkGray}
\setbeamercolor{item}{fg=MainBlue}

\setbeamertemplate{navigation symbols}{}

\setbeamertemplate{footline}{
    \leavevmode%
    \hbox{%
        \begin{beamercolorbox}[wd=.333333\paperwidth,ht=2.5ex,dp=1ex,center]{author in head/foot}%
            \usebeamerfont{author in head/foot}\insertshortauthor
        \end{beamercolorbox}%
        \begin{beamercolorbox}[wd=.333333\paperwidth,ht=2.5ex,dp=1ex,center]{title in head/foot}%
            \usebeamerfont{title in head/foot}\insertshorttitle
        \end{beamercolorbox}%
        \begin{beamercolorbox}[wd=.333333\paperwidth,ht=2.5ex,dp=1ex,right]{date in head/foot}%
            \usebeamerfont{date in head/foot}\insertshortdate{}\hspace*{2em}
            \insertframenumber{} / \inserttotalframenumber\hspace*{2ex}
        \end{beamercolorbox}}%
    \vskip0pt%
}

%=============================================================================
% PACHETE
%=============================================================================
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{amsmath, amssymb, amsthm}
\usepackage{mathtools}
\usepackage{bm}
\usepackage{tikz}
\usetikzlibrary{arrows.meta, positioning, shapes, calc, decorations.pathreplacing}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{array}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{colortbl}
\hypersetup{colorlinks=false, pdfborder={0 0 0}}
\graphicspath{{../logos/}{../charts/}}

%=============================================================================
% MEDII PENTRU TEOREME
%=============================================================================
\theoremstyle{definition}
\setbeamertemplate{theorems}[numbered]
\newtheorem{defn}{Definiție}
\newtheorem{thm}{Teoremă}
\newtheorem{prop}{Propoziție}
\newtheorem{rmk}{Observație}

%=============================================================================
% COMENZI PERSONALIZATE
%=============================================================================
\newcommand{\E}{\mathbb{E}}
\newcommand{\Var}{\text{Var}}
\newcommand{\Cov}{\text{Cov}}
\newcommand{\Corr}{\text{Corr}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\N}{\mathbb{N}}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\RMSE}{\text{RMSE}}
\newcommand{\MAE}{\text{MAE}}
\newcommand{\MAPE}{\text{MAPE}}

%=============================================================================
% INFORMAȚII TITLU
%=============================================================================
\title[Capitolul 8: Extensii Moderne]{Capitolul 8: Extensii Moderne ale Seriilor de Timp}
\subtitle{Program de licență, Facultatea de Cibernetică, Statistică și Informatică Economică, Academia de Studii Economice din București}
\author[Prof. dr. Daniel Traian Pele]{Prof. dr. Daniel Traian Pele\\[0.2cm]\footnotesize\texttt{danpele@ase.ro}}
\institute{Academia de Studii Economice din București}
\date{An Universitar 2025--2026}

\begin{document}

%=============================================================================
% DIAPOZITIV TITLU
%=============================================================================
\begin{frame}[plain]
    \begin{tikzpicture}[remember picture, overlay]
        \fill[IDAred] (current page.north west) rectangle ([yshift=-0.15cm]current page.north east);
        \node[anchor=north west] at ([xshift=0.5cm, yshift=-0.3cm]current page.north west) {
            \href{https://www.ase.ro}{\includegraphics[height=1.1cm]{ase_logo.png}}
        };
        \node[anchor=north] at ([yshift=-0.3cm]current page.north) {
            \href{https://ai4efin.ase.ro}{\includegraphics[height=1.1cm]{ai4efin_logo.png}}
        };
        \node[anchor=north east] at ([xshift=-0.5cm, yshift=-0.3cm]current page.north east) {
            \href{https://www.digital-finance-msca.com}{\includegraphics[height=1.1cm]{msca_logo.png}}
        };
    \end{tikzpicture}
    \vfill
    \begin{center}
        {\Large\textcolor{MediumGray}{Analiza și Prognoză Seriilor de Timp}}\\[0.3cm]
        {\Huge\textbf{\textcolor{MainBlue}{Capitolul 8: Extensii Moderne}}}\\[0.5cm]
        {\Large\textcolor{IDAred}{ARFIMA, Machine Learning, Deep Learning}}
    \end{center}
    \vfill

    \begin{tikzpicture}[remember picture, overlay]
        \fill[IDAred] (current page.south west) rectangle ([yshift=0.15cm]current page.south east);
        \node[anchor=south west] at ([xshift=0.5cm, yshift=0.8cm]current page.south west) {
            \href{https://theida.net}{\includegraphics[height=0.9cm]{ida_logo.png}}
        };
        \node[anchor=south] at ([xshift=-3cm, yshift=0.8cm]current page.south) {
            \href{https://blockchain-research-center.com}{\includegraphics[height=0.9cm]{brc_logo.png}}
        };
        \node[anchor=south] at ([yshift=0.8cm]current page.south) {
            \href{https://quantinar.com}{\includegraphics[height=0.9cm]{qr_logo.png}}
        };
        \node[anchor=south] at ([xshift=3cm, yshift=0.8cm]current page.south) {
            \href{https://quantlet.com}{\includegraphics[height=0.9cm]{ql_logo.png}}
        };
        \node[anchor=south east] at ([xshift=-0.5cm, yshift=0.8cm]current page.south east) {
            \href{https://ipe.ro/new}{\includegraphics[height=0.9cm]{acad_logo.png}}
        };
    \end{tikzpicture}
\end{frame}

%=============================================================================
% OUTLINE
%=============================================================================
\begin{frame}{Cuprins}
    \vspace{-0.5cm}
    {\scriptsize
    \begin{columns}[T]
        \begin{column}{0.32\textwidth}
            \tableofcontents[sections={1-4}, hideallsubsections]
        \end{column}
        \begin{column}{0.32\textwidth}
            \tableofcontents[sections={5-7}, hideallsubsections]
        \end{column}
        \begin{column}{0.32\textwidth}
            \tableofcontents[sections={8-10}, hideallsubsections]
        \end{column}
    \end{columns}
    }
\end{frame}

%=============================================================================
% LEARNING OBJECTIVES
%=============================================================================
\begin{frame}{Obiective de Învățare}
    \begin{block}{La finalul acestui capitol, veți fi capabili să:}
        \begin{enumerate}
            \item Înțelegeți conceptul de \textbf{memorie lungă} în seriile de timp
            \item Eștimați și interpretați modele \textbf{ARFIMA}
            \item Aplicați \textbf{Random Forest} pentru prognoză seriilor de timp
            \item Construiți rețele \textbf{LSTM} pentru serii temporale
            \item Comparăți performanța modelelor clasice vs ML
            \item Alegeți metoda potrivită în funcție de context
            \item Implementați aceste metode în \textbf{Python}
        \end{enumerate}
    \end{block}
\end{frame}

%=============================================================================
\section{Motivație}
%=============================================================================

\begin{frame}{De la Modele Clasice la Machine Learning}
    \begin{block}{Limitările Modelelor ARIMA}
        \begin{itemize}
            \item Presupun \textbf{memorie scurtă}: autocorelațiile scad exponențial
            \item Relații \textbf{liniare} între variabile
            \item Dificultăți cu \textbf{pattern-uri complexe} și neliniare
            \item Necesită \textbf{staționaritate} (prin diferențiere)
        \end{itemize}
    \end{block}

    \vspace{0.2cm}

    \begin{alertblock}{Soluții Moderne}
        \begin{itemize}
            \item \textbf{ARFIMA}: Captează memoria lungă (autocorelații care scad lent)
            \item \textbf{Random Forest}: Relații neliniare, robustețe la outlieri
            \item \textbf{LSTM}: Pattern-uri secvențiale complexe, dependențe pe termen lung
        \end{itemize}
    \end{alertblock}
\end{frame}

\begin{frame}{Când să Folosim Fiecare Metodă?}
    \begin{center}
    \begin{tabular}{l|c|c|c|c}
        \toprule
        \textbf{Caracteristică} & \textbf{ARIMA} & \textbf{ARFIMA} & \textbf{RF} & \textbf{LSTM} \\
        \midrule
        Memorie lungă & $\times$ & $\checkmark$ & $\checkmark$ & $\checkmark$ \\
        Relații neliniare & $\times$ & $\times$ & $\checkmark$ & $\checkmark$ \\
        Interpretabilitate & $\checkmark$ & $\checkmark$ & $\sim$ & $\times$ \\
        Date puține & $\checkmark$ & $\checkmark$ & $\times$ & $\times$ \\
        Variabile exogene & $\checkmark$ & $\checkmark$ & $\checkmark$ & $\checkmark$ \\
        Incertitudine & $\checkmark$ & $\checkmark$ & $\sim$ & $\times$ \\
        \bottomrule
    \end{tabular}
    \end{center}

    \vspace{0.3cm}

    \begin{exampleblock}{Regula de Aur}
        Începe \textbf{simplu} (ARIMA), apoi crește complexitatea doar dacă este justificat de date și performanță.
    \end{exampleblock}
\end{frame}

%=============================================================================
\section{ARFIMA: Modele cu Memorie Lungă}
%=============================================================================

\begin{frame}{Ce este Memoria Lungă?}
    \begin{block}{Memorie Scurtă (ARMA)}
        \begin{itemize}
            \item Autocorelațiile $\rho_k$ scad \textbf{exponențial}: $|\rho_k| \leq C \cdot r^k$, $r < 1$
            \item Efectele șocurilor dispar \textbf{rapid}
            \item Sumă finită: $\sum_{k=0}^{\infty} |\rho_k| < \infty$
        \end{itemize}
    \end{block}

    \vspace{0.2cm}

    \begin{alertblock}{Memorie Lungă (ARFIMA)}
        \begin{itemize}
            \item Autocorelațiile scad \textbf{hiperbolic}: $\rho_k \sim C \cdot k^{2d-1}$
            \item Efectele șocurilor persistă \textbf{mult timp}
            \item Sumă infinită: $\sum_{k=0}^{\infty} |\rho_k| = \infty$ (pentru $d > 0$)
        \end{itemize}
    \end{alertblock}

    \vspace{0.2cm}

    \begin{exampleblock}{Exemple cu Memorie Lungă}
        Volatilitatea piețelor financiare, debite râuri, trafic rețea, inflație
    \end{exampleblock}
\end{frame}

\begin{frame}{Comparație ACF: Memorie Scurtă vs Lungă}
    \begin{center}
        \includegraphics[width=0.95\textwidth, height=0.75\textheight, keepaspectratio]{charts/ch8_acf_comparison.pdf}
    \end{center}
    \vspace{-0.3cm}
    {\footnotesize
    \textbf{Stânga}: AR(1) --- autocorelații care scad exponențial (memorie scurtă)\\
    \textbf{Dreapta}: ARFIMA cu $d=0.35$ --- autocorelații care scad hiperbolic (memorie lungă)
    }
\end{frame}

\begin{frame}{Modelul ARFIMA(p,d,q)}
    \begin{defn}[ARFIMA]
        Un proces $\{Y_t\}$ urmează un model \textbf{ARFIMA(p,d,q)} dacă:
        $$\phi(L)(1-L)^d Y_t = \theta(L)\varepsilon_t$$
        unde $d \in (-0.5, 0.5)$ este parametrul de \textbf{diferențiere fracționară}.
    \end{defn}

    \vspace{0.3cm}

    \begin{block}{Operatorul de Diferențiere Fracționară}
        $$(1-L)^d = \sum_{k=0}^{\infty} \binom{d}{k}(-L)^k = 1 - dL - \frac{d(1-d)}{2!}L^2 - \frac{d(1-d)(2-d)}{3!}L^3 - \cdots$$
    \end{block}

    \vspace{0.2cm}

    {\small
    \begin{itemize}
        \item $d = 0$: ARMA standard (memorie scurtă)
        \item $0 < d < 0.5$: Memorie lungă, staționaritate
        \item $d = 0.5$: Limita staționarității
        \item $0.5 \leq d < 1$: Nestaționaritate, dar mean-reverting
        \item $d = 1$: Random walk (ARIMA standard)
    \end{itemize}
    }
\end{frame}

\begin{frame}{Interpretarea Parametrului $d$}
    \begin{center}
    \begin{tabular}{c|l|l}
        \toprule
        \textbf{Valoare $d$} & \textbf{Comportament ACF} & \textbf{Interpretare} \\
        \midrule
        $d = 0$ & Scădere exponențială & Memorie scurtă \\
        $0 < d < 0.5$ & Scădere hiperbolică & Memorie lungă, staționară \\
        $d = 0.5$ & ACF nesumabilă & La limită \\
        $0.5 < d < 1$ & Scădere foarte lentă & Memorie lungă, nestaționară \\
        $d = 1$ & ACF = 1 (constant) & Random walk \\
        \bottomrule
    \end{tabular}
    \end{center}

    \vspace{0.3cm}

    \begin{block}{Parametrul Hurst $H$}
        Relația cu exponentul Hurst: $d = H - 0.5$
        \begin{itemize}
            \item $H = 0.5$: Mers aleator (fără memorie)
            \item $H > 0.5$: Persistență (trend-following)
            \item $H < 0.5$: Anti-persistență (mean-reverting)
        \end{itemize}
    \end{block}
\end{frame}

\begin{frame}{Efectul Parametrului $d$ asupra ACF}
    \begin{center}
        \includegraphics[width=0.95\textwidth, height=0.75\textheight, keepaspectratio]{charts/ch8_arfima_d_effect.pdf}
    \end{center}
    \vspace{-0.2cm}
    {\footnotesize
    Cu cât $d$ este mai mare, cu atât autocorelațiile scad mai lent. Pentru $d \to 0.5$, autocorrelațiile rămân semnificative chiar și la lag-uri foarte mari.
    }
\end{frame}

\begin{frame}{Exponentul Hurst: Interpretare Vizuală}
    \begin{center}
        \includegraphics[width=0.95\textwidth, height=0.70\textheight, keepaspectratio]{charts/ch8_hurst_interpretation.pdf}
    \end{center}
    \vspace{-0.2cm}
    {\footnotesize
    \textbf{H $<$ 0.5}: Serie care revine frecvent la medie (mean-reverting)\\
    \textbf{H = 0.5}: Mers aleator, imprevizibil\\
    \textbf{H $>$ 0.5}: Serie persistentă, tendințele continuă
    }
\end{frame}

\begin{frame}{Estimarea Parametrului $d$}
    \begin{block}{Metode de Estimare}
        \begin{enumerate}
            \item \textbf{GPH (Geweke-Porter-Hudak)}: Regresie în domeniul frecvență
            $$\ln I(\omega_j) = c - d \cdot \ln\left(4\sin^2\frac{\omega_j}{2}\right) + \varepsilon_j$$

            \item \textbf{R/S (Rescaled Range)}: Metoda lui Hurst
            $$\frac{R}{S}(n) \sim c \cdot n^H$$

            \item \textbf{MLE (Maximum Likelihood)}: Estimare completă ARFIMA

            \item \textbf{Whittle}: Aproximare eficientă în domeniul frecvență
        \end{enumerate}
    \end{block}

    \vspace{0.2cm}

    {\footnotesize
    În Python: \texttt{arch} package, \texttt{statsmodels.tsa.arima.model.ARIMA} cu \texttt{order=(p,d,q)} unde $d$ poate fi fracționar.
    }
\end{frame}

\begin{frame}{Exemplu ARFIMA în Python}
    \begin{block}{Cod Python}
        {\footnotesize
        \texttt{from statsmodels.tsa.arima.model import ARIMA}

        \texttt{model = ARIMA(y, order=(1, 0.3, 1))}

        \texttt{results = model.fit()}
        }
    \end{block}

    \vspace{0.2cm}

    \begin{alertblock}{Notă}
        Estimarea ARFIMA necesită pachete specializate. În practică, se folosește adesea \texttt{arch} sau \texttt{fracdiff} în Python.
    \end{alertblock}
\end{frame}

\begin{frame}{Exemplu Real: Memorie Lungă în Volatilitate}
    \begin{center}
        \includegraphics[width=0.98\textwidth, height=0.72\textheight, keepaspectratio]{charts/ch8_volatility_long_memory.pdf}
    \end{center}
    \vspace{-0.3cm}
    {\footnotesize
    \textbf{Fapt Stilizat}: Randamentele financiare au memorie scurtă, dar volatilitatea (|randamente|) are memorie lungă! Aceasta este baza modelelor FIGARCH.
    }
\end{frame}

%=============================================================================
\section{Random Forest pentru Serii de Timp}
%=============================================================================

\begin{frame}{Random Forest: Concepte de Bază}
    \begin{block}{Ce este Random Forest?}
        \begin{itemize}
            \item \textbf{Ansamblu} de arbori de decizie
            \item Fiecare arbore antrenat pe un \textbf{subset bootstrap} al datelor
            \item La fiecare nod, se selectează \textbf{aleator} un subset de features
            \item Predicția finală = \textbf{media} predicțiilor tuturor arborilor
        \end{itemize}
    \end{block}

    \vspace{0.2cm}

    \begin{exampleblock}{Avantaje pentru Serii de Timp}
        \begin{itemize}
            \item Captează \textbf{relații neliniare}
            \item \textbf{Robust} la outlieri și zgomot
            \item Nu necesită \textbf{staționaritate}
            \item Oferă \textbf{importanța features} (interpretabilitate)
            \item Funcționează bine cu \textbf{multe variabile}
        \end{itemize}
    \end{exampleblock}
\end{frame}

\begin{frame}{Pregătirea Datelor pentru Random Forest}
    \begin{block}{Feature Engineering pentru Serii de Timp}
        \begin{enumerate}
            \item \textbf{Lag features}: $Y_{t-1}, Y_{t-2}, \ldots, Y_{t-p}$
            \item \textbf{Rolling statistics}: medie mobilă, deviație standard
            \item \textbf{Calendar features}: ziua săptămânii, luna, sezon
            \item \textbf{Trend features}: timp, trend pătratic
            \item \textbf{Variabile exogene}: indicători economici, evenimente
        \end{enumerate}
    \end{block}

    \vspace{0.2cm}

    \begin{alertblock}{Atenție: Data Leakage!}
        \begin{itemize}
            \item Nu folosi informații din viitor în features
            \item Train/test split: \textbf{temporal}, nu aleator!
            \item Rolling statistics: calculează doar pe date \textbf{anterioare}
        \end{itemize}
    \end{alertblock}
\end{frame}

\begin{frame}{Feature Engineering: Ilustrare}
    \begin{center}
        \includegraphics[width=0.98\textwidth, height=0.75\textheight, keepaspectratio]{charts/ch8_feature_engineering.pdf}
    \end{center}
    \vspace{-0.3cm}
    {\footnotesize
    Transformăm seria temporală în features: lag-uri, statistici rolling, iar modelul RF învață relațiile dîntre acestea și valorile viitoare.
    }
\end{frame}

\begin{frame}{Random Forest: Implementare Python}
    \begin{block}{Cod Python}
        {\footnotesize
        \texttt{from sklearn.ensemble import RandomForestRegressor}

        \texttt{rf = RandomForestRegressor(n\_eștimators=100, max\_depth=10)}

        \texttt{rf.fit(X\_train, y\_train)}

        \texttt{predictions = rf.predict(X\_test)}
        }
    \end{block}
\end{frame}

\begin{frame}{Importanța Features și Interpretare}
    \begin{block}{Feature Importance}
        Random Forest oferă măsuri de importanță:
        \begin{itemize}
            \item \textbf{Mean Decrease Impurity (MDI)}: Reducerea impurității la fiecare split
            \item \textbf{Permutation Importance}: Cât scade performanța când feature-ul e permutat aleator
        \end{itemize}
    \end{block}

    \vspace{0.2cm}

    \begin{exampleblock}{Interpretare Tipică pentru Serii de Timp}
        \begin{itemize}
            \item \texttt{lag\_1} foarte important $\Rightarrow$ Autocorelare puternică
            \item \texttt{rolling\_mean} important $\Rightarrow$ Trend local contează
            \item \texttt{month} important $\Rightarrow$ Sezonalitate prezentă
        \end{itemize}
    \end{exampleblock}

    \vspace{0.2cm}

    {\footnotesize
    \texttt{rf.feature\_importances\_} sau \texttt{permutation\_importance(rf, X\_test, y\_test)}
    }
\end{frame}

\begin{frame}{Random Forest: Exemplu de Prognoză}
    \begin{center}
        \includegraphics[width=0.98\textwidth, height=0.72\textheight, keepaspectratio]{charts/ch8_rf_prediction.pdf}
    \end{center}
    \vspace{-0.3cm}
    {\footnotesize
    Modelul Random Forest antrenat pe date istorice (albastru) produce prognoze (roșu punctat) care urmăresc bine valorile reale din perioadă de test (verde).
    }
\end{frame}

%=============================================================================
\section{LSTM: Deep Learning pentru Serii de Timp}
%=============================================================================

\begin{frame}{Rețele Neuronale Recurente (RNN)}
    \begin{block}{Ideea de Bază}
        \begin{itemize}
            \item Rețele care procesează \textbf{secvențe} de date
            \item Au \textbf{memorie internă} (hidden state)
            \item Starea curentă depinde de input + starea anterioară
        \end{itemize}
    \end{block}

    \vspace{0.2cm}

    \begin{center}
    \begin{tikzpicture}[scale=0.8, transform shape]
        \node[draw, circle, minimum size=1cm, fill=MainBlue!20] (h1) at (0,0) {$h_1$};
        \node[draw, circle, minimum size=1cm, fill=MainBlue!20] (h2) at (2,0) {$h_2$};
        \node[draw, circle, minimum size=1cm, fill=MainBlue!20] (h3) at (4,0) {$h_3$};
        \node[draw, circle, minimum size=1cm, fill=MainBlue!20] (ht) at (6,0) {$h_t$};

        \node (x1) at (0,-1.5) {$x_1$};
        \node (x2) at (2,-1.5) {$x_2$};
        \node (x3) at (4,-1.5) {$x_3$};
        \node (xt) at (6,-1.5) {$x_t$};

        \node (y1) at (0,1.5) {$y_1$};
        \node (y2) at (2,1.5) {$y_2$};
        \node (y3) at (4,1.5) {$y_3$};
        \node (yt) at (6,1.5) {$y_t$};

        \draw[->, thick] (x1) -- (h1);
        \draw[->, thick] (x2) -- (h2);
        \draw[->, thick] (x3) -- (h3);
        \draw[->, thick] (xt) -- (ht);

        \draw[->, thick] (h1) -- (y1);
        \draw[->, thick] (h2) -- (y2);
        \draw[->, thick] (h3) -- (y3);
        \draw[->, thick] (ht) -- (yt);

        \draw[->, thick, IDAred] (h1) -- (h2);
        \draw[->, thick, IDAred] (h2) -- (h3);
        \draw[->, thick, IDAred, dashed] (h3) -- (5,0);
        \draw[->, thick, IDAred, dashed] (5,0) -- (ht);
    \end{tikzpicture}
    \end{center}

    \begin{alertblock}{Problema: Vanishing Gradient}
        RNN simple ``uită'' informația din trecut îndepărtat.
    \end{alertblock}
\end{frame}

\begin{frame}{LSTM: Long Short-Term Memory}
    \begin{block}{Soluția LSTM}
        Celule speciale cu \textbf{3 porți} care controlează fluxul informației:
        \begin{itemize}
            \item \textbf{Forget Gate} ($f_t$): Ce să uităm din memoria anterioară
            \item \textbf{Input Gate} ($i_t$): Ce informație nouă să adăugăm
            \item \textbf{Output Gate} ($o_t$): Ce să trimitem la ieșire
        \end{itemize}
    \end{block}

    \vspace{0.2cm}

    \begin{block}{Ecuațiile LSTM}
        {\small
        \begin{align*}
            f_t &= \sigma(W_f \cdot [h_{t-1}, x_t] + b_f) & \text{(Forget)} \\
            i_t &= \sigma(W_i \cdot [h_{t-1}, x_t] + b_i) & \text{(Input)} \\
            \tilde{C}_t &= \tanh(W_C \cdot [h_{t-1}, x_t] + b_C) & \text{(Candidate)} \\
            C_t &= f_t \odot C_{t-1} + i_t \odot \tilde{C}_t & \text{(Cell state)} \\
            o_t &= \sigma(W_o \cdot [h_{t-1}, x_t] + b_o) & \text{(Output)} \\
            h_t &= o_t \odot \tanh(C_t) & \text{(Hidden state)}
        \end{align*}
        }
    \end{block}
\end{frame}

\begin{frame}{Arhitectura Celulei LSTM}
    \begin{center}
        \includegraphics[width=0.95\textwidth, height=0.75\textheight, keepaspectratio]{charts/ch8_lstm_architecture.pdf}
    \end{center}
    \vspace{-0.2cm}
    {\footnotesize
    Porțile (forget, input, output) controlează ce informație este uitată, adăugată și transmisă. \textbf{Cell state} permite gradienților să ``curgă'' fără degradare.
    }
\end{frame}

\begin{frame}{Avantajele LSTM pentru Serii de Timp}
    \begin{exampleblock}{De ce LSTM?}
        \begin{itemize}
            \item Captează \textbf{dependențe pe termen lung} (spre deosebire de RNN simplu)
            \item Învață \textbf{pattern-uri complexe} și neliniare
            \item Gestionează \textbf{secvențe de lungimi variabile}
            \item Funcționează bine cu \textbf{date multivariate}
        \end{itemize}
    \end{exampleblock}

    \vspace{0.2cm}

    \begin{alertblock}{Dezavantaje}
        \begin{itemize}
            \item Necesită \textbf{multe date} pentru antrenare
            \item \textbf{Computațional intensiv}
            \item ``\textbf{Black box}'' - greu de interpretat
            \item Sensibil la \textbf{hiperparametri}
            \item Poate face \textbf{overfitting} ușor
        \end{itemize}
    \end{alertblock}
\end{frame}

\begin{frame}{LSTM: Implementare în Python cu Keras}
    \begin{block}{Cod Python}
        {\footnotesize
        \texttt{from tensorflow.keras.models import Sequential}

        \texttt{from tensorflow.keras.layers import LSTM, Dense, Dropout}

        \vspace{0.2cm}
        \texttt{model = Sequential([}

        \texttt{~~~~LSTM(50, return\_sequences=True, input\_shape=(n, 1)),}

        \texttt{~~~~Dropout(0.2),}

        \texttt{~~~~LSTM(50),}

        \texttt{~~~~Dense(1)}

        \texttt{])}

        \texttt{model.compile(optimizer='adam', loss='mse')}
        }
    \end{block}
\end{frame}

\begin{frame}{Pregătirea Datelor pentru LSTM}
    \begin{block}{Pași Esențiali}
        \begin{enumerate}
            \item \textbf{Normalizare/Scalare}: MinMaxScaler sau StandardScaler
            \item \textbf{Creare secvențe}: Sliding window pentru input
            \item \textbf{Reshape}: Format 3D (samples, timesteps, features)
            \item \textbf{Train/Test split}: Temporal, nu aleator!
        \end{enumerate}
    \end{block}

    \vspace{0.2cm}

    \begin{block}{Exemplu Creare Secvențe}
        {\footnotesize
        \texttt{def create\_sequences(data, n\_steps):}

        \texttt{~~~~X, y = [], []}

        \texttt{~~~~for i în range(len(data) - n\_steps):}

        \texttt{~~~~~~~~X.append(data[i:(i + n\_steps)])}

        \texttt{~~~~return np.array(X), np.array(y)}

        \vspace{0.1cm}
        \texttt{X, y = create\_sequences(scaled\_data, 10)}
        }
    \end{block}
\end{frame}

%=============================================================================
\section{Comparație și Selecția Modelului}
%=============================================================================

\begin{frame}{Metrici de Evaluare}
    \begin{block}{Metrici Comune}
        \begin{itemize}
            \item \textbf{RMSE}: $\sqrt{\frac{1}{n}\sum_{i=1}^n (y_i - \hat{y}_i)^2}$ --- Eroare în unități originale
            \item \textbf{MAE}: $\frac{1}{n}\sum_{i=1}^n |y_i - \hat{y}_i|$ --- Robust la outlieri
            \item \textbf{MAPE}: $\frac{100}{n}\sum_{i=1}^n \left|\frac{y_i - \hat{y}_i}{y_i}\right|$ --- Eroare procentuală
            \item \textbf{MASE}: Comparăt cu benchmark naiv
        \end{itemize}
    \end{block}

    \vspace{0.2cm}

    \begin{alertblock}{Validare pentru Serii de Timp}
        \begin{itemize}
            \item \textbf{Nu} folosiți cross-validation standard!
            \item Folosiți \textbf{Time Series Cross-Validation} (walk-forward)
            \item Sau \textbf{train/validation/test} split temporal
        \end{itemize}
    \end{alertblock}
\end{frame}

\begin{frame}{Time Series Cross-Validation}
    \begin{center}
        \includegraphics[width=0.95\textwidth, height=0.60\textheight, keepaspectratio]{charts/ch8_timeseries_cv.pdf}
    \end{center}

    \vspace{0.2cm}

    \begin{block}{Implementare Python}
        {\footnotesize
        \texttt{from sklearn.model\_selection import TimeSeriesSplit}

        \texttt{tscv = TimeSeriesSplit(n\_splits=5)}
        }
    \end{block}

    {\footnotesize
    \textbf{Important}: Setul de antrenare crește progresiv, iar testul este întotdeauna în viitor. Astfel evităm data leakage.
    }
\end{frame}

\begin{frame}{Ghid de Selecție a Modelului}
    {\small
    \begin{center}
    \begin{tikzpicture}[
        node distance=1.5cm,
        decision/.style={diamond, draw, fill=MainBlue!20, text width=2.5cm, align=center, inner sep=1pt},
        block/.style={rectangle, draw, fill=Forest!20, text width=2cm, align=center, rounded corners},
        arrow/.style={thick,->,>=stealth}
    ]
        \node[decision] (start) {Date puține?};
        \node[block, below left=1cm and 1.5cm of start] (arima) {ARIMA/ ARFIMA};
        \node[decision, below right=1cm and 1.5cm of start] (nonlin) {Relații neliniare?};
        \node[block, below left=1cm and 0.5cm of nonlin] (arfima) {ARFIMA};
        \node[decision, below right=1cm and 0.5cm of nonlin] (interp) {Interpretabil?};
        \node[block, below left=0.8cm and 0cm of interp] (rf) {Random Forest};
        \node[block, below right=0.8cm and 0cm of interp] (lstm) {LSTM};

        \draw[arrow] (start) -- node[above left] {Da} (arima);
        \draw[arrow] (start) -- node[above right] {Nu} (nonlin);
        \draw[arrow] (nonlin) -- node[above left] {Nu} (arfima);
        \draw[arrow] (nonlin) -- node[above right] {Da} (interp);
        \draw[arrow] (interp) -- node[above left] {Da} (rf);
        \draw[arrow] (interp) -- node[above right] {Nu} (lstm);
    \end{tikzpicture}
    \end{center}
    }
\end{frame}

\begin{frame}{Comparație Modele: Acuratețe vs Cost Computațional}
    \begin{center}
        \includegraphics[width=0.98\textwidth, height=0.72\textheight, keepaspectratio]{charts/ch8_case_comparison.pdf}
    \end{center}
    \vspace{-0.3cm}
    {\footnotesize
    \textbf{Trade-off}: Modelele ML pot avea acuratețe ușor mai bună, dar costul computațional crește semnificativ. Pentru date puține sau interpretabilitate, ARIMA/ARFIMA rămân alegeri excelente.
    }
\end{frame}

%=============================================================================
\section{Aplicații Practice}
%=============================================================================

\begin{frame}{Studiu de Caz: Prognoză Prețului Bitcoin}
    \begin{block}{De ce Bitcoin?}
        \begin{itemize}
            \item Volatilitate \textbf{extremă} și pattern-uri complexe
            \item Potențială \textbf{memorie lungă} în volatilitate
            \item Relații \textbf{neliniare} cu variabile exogene
            \item Date disponibile la \textbf{frecvență înaltă}
        \end{itemize}
    \end{block}

    \vspace{0.2cm}

    \begin{exampleblock}{Abordare Comparătivă}
        \begin{enumerate}
            \item ARIMA pe randamente
            \item ARFIMA pentru memorie lungă
            \item Random Forest cu features tehnice
            \item LSTM pe secvențe de prețuri
        \end{enumerate}
    \end{exampleblock}
\end{frame}

\begin{frame}{Studiu de Caz: Prognoză Consumului de Energie}
    \begin{block}{Caracteristici}
        \begin{itemize}
            \item \textbf{Sezonalitate multiplă}: zilnică, săptămânală, anuală
            \item \textbf{Tendință} de creștere pe termen lung
            \item \textbf{Variabile exogene}: temperatură, zi liberă, preț
            \item \textbf{Anomalii}: evenimente speciale, defecțiuni
        \end{itemize}
    \end{block}

    \vspace{0.2cm}

    \begin{alertblock}{Provocări}
        \begin{itemize}
            \item Pattern-uri la scale temporale diferite
            \item Interacțiuni complexe între variabile
            \item Necesitatea prognozelor pe orizonturi diferite
        \end{itemize}
    \end{alertblock}
\end{frame}

%=============================================================================
% KEY FORMULAS SUMMARY
%=============================================================================
\begin{frame}{Formule Cheie -- Rezumat}
    \vspace{-0.2cm}
    {\small
    \begin{columns}[T]
        \begin{column}{0.48\textwidth}
            \begin{block}{ARFIMA(p,d,q)}
                $\phi(L)(1-L)^d Y_t = \theta(L)\varepsilon_t$

                \vspace{0.1cm}
                {\footnotesize $d \in (-0.5, 0.5)$: memorie lungă}
            \end{block}

            \begin{block}{Memorie Lungă}
                ACF: $\rho_k \sim C \cdot k^{2d-1}$

                Hurst: $d = H - 0.5$

                \vspace{0.1cm}
                {\footnotesize $H > 0.5$: persistență}
            \end{block}

            \begin{block}{Random Forest}
                $\hat{y} = \frac{1}{B}\sum_{b=1}^{B} T_b(x)$

                \vspace{0.1cm}
                {\footnotesize $B$ arbori, features aleatorii}
            \end{block}
        \end{column}

        \begin{column}{0.48\textwidth}
            \begin{block}{LSTM Cell}
                $f_t = \sigma(W_f[h_{t-1}, x_t] + b_f)$

                $C_t = f_t \odot C_{t-1} + i_t \odot \tilde{C}_t$

                \vspace{0.1cm}
                {\footnotesize Forget, Input, Output gates}
            \end{block}

            \begin{block}{Metrici Evaluare}
                RMSE $= \sqrt{\frac{1}{n}\sum(y_i - \hat{y}_i)^2}$

                MAPE $= \frac{100}{n}\sum\left|\frac{y_i - \hat{y}_i}{y_i}\right|$
            \end{block}

            \begin{block}{Time Series CV}
                Walk-forward validation

                Train $\rightarrow$ Test (temporal split)
            \end{block}
        \end{column}
    \end{columns}
    }
\end{frame}

%=============================================================================
\section{Studiu de Caz Complet: Cursul EUR/RON}
%=============================================================================

\begin{frame}{Studiu de Caz: Prognoză Cursului EUR/RON}
    \begin{block}{De ce EUR/RON?}
        \begin{itemize}
            \item Relevanță pentru economia românească
            \item Potențială \textbf{memorie lungă} (persistența șocurilor)
            \item Pattern-uri influențate de \textbf{factori macroeconomici}
            \item Date ușor accesibile (BNR, Yahoo Finance)
        \end{itemize}
    \end{block}

    \vspace{0.2cm}

    \begin{exampleblock}{Obiectiv}
        Comparăm ARIMA, ARFIMA, Random Forest și LSTM pe aceleași date pentru a înțelege punctele forte ale fiecărei metode.
    \end{exampleblock}
\end{frame}

\begin{frame}[fragile]{Pasul 1: Încărcarea și Vizualizarea Datelor}
    \begin{block}{Cod Python -- Descărcare Date}
        {\footnotesize
\begin{verbatim}
import yfinance as yf
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

# Descărcăm datele EUR/RON (sau EURRON=X)
data = yf.download('EURRON=X', start='2015-01-01', end='2024-12-31')
df = data[['Close']].dropna()
df.columns = ['EURRON']

# Calculăm randamentele logaritmice
df['Returns'] = np.log(df['EURRON']).diff() * 100
df = df.dropna()

print(f"Perioada: {df.index[0]} - {df.index[-1]}")
print(f"Observații: {len(df)}")
print(f"Media randamentelor: {df['Returns'].mean():.4f}%")
print(f"Volatilitate: {df['Returns'].std():.4f}%")
\end{verbatim}
        }
    \end{block}
\end{frame}

\begin{frame}{Vizualizarea Seriei EUR/RON}
    \begin{center}
        \includegraphics[width=0.98\textwidth, height=0.72\textheight, keepaspectratio]{charts/ch8_case_raw_data.pdf}
    \end{center}
    \vspace{-0.3cm}
    {\footnotesize
    \textbf{Sus}: Cursul EUR/RON -- observăm tendința de depreciere a leului și perioadele de volatilitate ridicată.\\
    \textbf{Jos}: Randamentele zilnice -- volatilitate clustering (perioadele de volatilitate mare sunt urmate de alte perioade similare).
    }
\end{frame}

\begin{frame}{Analiză ACF: Randamente vs Randamente Pătrate}
    \begin{center}
        \includegraphics[width=0.98\textwidth, height=0.72\textheight, keepaspectratio]{charts/ch8_case_acf_analysis.pdf}
    \end{center}
    \vspace{-0.3cm}
    {\footnotesize
    \textbf{Stânga}: ACF al randamentelor -- scădere rapidă, fără autocorelație semnificativă după lag 1.\\
    \textbf{Dreapta}: ACF al randamentelor pătrate -- scădere lentă indică \textbf{volatilitate clustering} (efecte ARCH).
    }
\end{frame}

\begin{frame}[fragile]{Pasul 2: Testarea Memoriei Lungi}
    \begin{block}{Cod Python -- Estimarea lui $d$ și Testul Hurst}
        {\footnotesize
\begin{verbatim}
from arch.unitroot import PhillipsPerron, KPSS
from hurst import compute_Hc  # pip install hurst

# Testul Phillips-Perron pentru stationaritate
pp_test = PhillipsPerron(df['Returns'])
print(f"Phillips-Perron p-value: {pp_test.pvalue:.4f}")

# Estimarea exponentului Hurst
H, c, data_rs = compute_Hc(df['Returns'].values, kind='change')
d_eștimated = H - 0.5

print(f"Exponentul Hurst (H): {H:.4f}")
print(f"Parametrul d eștimat: {d_eștimated:.4f}")

# Interpretare
if H > 0.5:
    print("Serie PERSISTENTĂ (trend-following)")
elif H < 0.5:
    print("Serie ANTI-PERSISTENTĂ (mean-reverting)")
else:
    print("Mers aleator")
\end{verbatim}
        }
    \end{block}
\end{frame}

\begin{frame}{Rezultate Test Memorie Lungă -- EUR/RON}
    \begin{block}{Output Tipic}
        {\footnotesize
        \texttt{Phillips-Perron p-value: 0.0001} (randamentele sunt staționare)\\
        \texttt{Exponentul Hurst (H): 0.47}\\
        \texttt{Parametrul d eștimat: -0.03}\\
        \texttt{Serie ușor ANTI-PERSISTENTĂ (mean-reverting)}
        }
    \end{block}

    \vspace{0.2cm}

    \begin{alertblock}{Interpretare}
        \begin{itemize}
            \item Randamentele EUR/RON sunt \textbf{staționare} (p-value $< 0.05$)
            \item $H \approx 0.47 < 0.5$: ușoară tendință de revenire la medie
            \item $d \approx 0$: \textbf{memorie scurtă} -- ARMA poate fi suficient
            \item Totuși, \textbf{volatilitatea} poate avea memorie lungă!
        \end{itemize}
    \end{alertblock}
\end{frame}

\begin{frame}[fragile]{Pasul 3: Model ARIMA}
    \begin{block}{Cod Python -- ARIMA cu selecție automată}
        {\footnotesize
\begin{verbatim}
from statsmodels.tsa.arima.model import ARIMA
from sklearn.metrics import mean_squared_error, mean_absolute_error
import warnings
warnings.filterwarnings('ignore')

# Împărțim datele: 80% train, 20% test
train_size = int(len(df) * 0.8)
train, test = df['Returns'][:train_size], df['Returns'][train_size:]

# Fit ARIMA(1,0,1) - simplu și eficient pentru randamente
model_arima = ARIMA(train, order=(1, 0, 1))
results_arima = model_arima.fit()

# Prognoză
forecast_arima = results_arima.forecast(steps=len(test))

# Evaluare
rmse_arima = np.sqrt(mean_squared_error(test, forecast_arima))
mae_arima = mean_absolute_error(test, forecast_arima)
print(f"ARIMA(1,0,1) - RMSE: {rmse_arima:.4f}, MAE: {mae_arima:.4f}")
\end{verbatim}
        }
    \end{block}
\end{frame}

\begin{frame}[fragile]{Pasul 4: Model ARFIMA (Memorie Lungă)}
    \begin{block}{Cod Python -- ARFIMA cu arch package}
        {\footnotesize
\begin{verbatim}
from arch import arch_model

# ARFIMA(1,d,1) folosind arch pentru estimare robustă
# Notă: arch eștimează d automat în contextul GARCH

# Alternativ, folosim statsmodels cu d fracționar
from statsmodels.tsa.arima.model import ARIMA

# Eștimăm d folosind GPH sau setăm manual
d_frac = 0.1  # sau valoarea eștimată anterior

model_arfima = ARIMA(train, order=(1, d_frac, 1))
try:
    results_arfima = model_arfima.fit()
    forecast_arfima = results_arfima.forecast(steps=len(test))
    rmse_arfima = np.sqrt(mean_squared_error(test, forecast_arfima))
    print(f"ARFIMA(1,{d_frac},1) - RMSE: {rmse_arfima:.4f}")
except:
    print("ARFIMA necesită d între -0.5 și 0.5 pentru stationaritate")
\end{verbatim}
        }
    \end{block}
\end{frame}

\begin{frame}[fragile]{Pasul 5: Random Forest -- Pregătire Date}
    \begin{block}{Cod Python -- Feature Engineering}
        {\footnotesize
\begin{verbatim}
from sklearn.ensemble import RandomForestRegressor

# Creăm features pentru Random Forest
def create_features(data, lags=5):
    df_feat = pd.DataFrame(index=data.index)
    df_feat['target'] = data.values

    # Lag features
    for i în range(1, lags + 1):
        df_feat[f'lag_{i}'] = data.shift(i)

    # Rolling statistics
    df_feat['rolling_mean_5'] = data.rolling(5).mean()
    df_feat['rolling_std_5'] = data.rolling(5).std()
    df_feat['rolling_mean_20'] = data.rolling(20).mean()

    # Calendar features
    df_feat['dayofweek'] = data.index.dayofweek
    df_feat['month'] = data.index.month

    return df_feat.dropna()

df_rf = create_features(df['Returns'], lags=10)
\end{verbatim}
        }
    \end{block}
\end{frame}

\begin{frame}[fragile]{Pasul 5: Random Forest -- Antrenare și Evaluare}
    \begin{block}{Cod Python -- Model Random Forest}
        {\footnotesize
\begin{verbatim}
# Împărțim datele
X = df_rf.drop('target', axis=1)
y = df_rf['target']

train_size = int(len(df_rf) * 0.8)
X_train, X_test = X[:train_size], X[train_size:]
y_train, y_test = y[:train_size], y[train_size:]

# Antrenăm Random Forest
rf_model = RandomForestRegressor(
    n_eștimators=100,
    max_depth=10,
    min_samples_split=5,
    random_state=42
)
rf_model.fit(X_train, y_train)

# Predicție și evaluare
pred_rf = rf_model.predict(X_test)
rmse_rf = np.sqrt(mean_squared_error(y_test, pred_rf))
print(f"Random Forest - RMSE: {rmse_rf:.4f}")
\end{verbatim}
        }
    \end{block}
\end{frame}

\begin{frame}{Random Forest: Importanța Features}
    \begin{center}
        \includegraphics[width=0.95\textwidth, height=0.70\textheight, keepaspectratio]{charts/ch8_case_feature_importance.pdf}
    \end{center}
    \vspace{-0.2cm}
    {\footnotesize
    \textbf{Insight}: Lag-urile recente (lag\_1, lag\_2) și volatilitatea rolling sunt cele mai importante. Features calendaristice au impact minor pentru randamente zilnice.
    }
\end{frame}

\begin{frame}[fragile]{Pasul 6: LSTM -- Pregătire Date}
    \begin{block}{Cod Python -- Secvențe pentru LSTM}
        {\footnotesize
\begin{verbatim}
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense, Dropout
from sklearn.preprocessing import MinMaxScaler

# Scalăm datele între 0 și 1
scaler = MinMaxScaler()
scaled_data = scaler.fit_transform(df['Returns'].values.reshape(-1, 1))

# Creăm secvențe
def create_sequences(data, seq_length=20):
    X, y = [], []
    for i în range(seq_length, len(data)):
        X.append(data[i-seq_length:i, 0])
        y.append(data[i, 0])
    return np.array(X), np.array(y)

X_lstm, y_lstm = create_sequences(scaled_data, seq_length=20)
X_lstm = X_lstm.reshape((X_lstm.shape[0], X_lstm.shape[1], 1))

# Split
split = int(len(X_lstm) * 0.8)
X_train_lstm, X_test_lstm = X_lstm[:split], X_lstm[split:]
y_train_lstm, y_test_lstm = y_lstm[:split], y_lstm[split:]
\end{verbatim}
        }
    \end{block}
\end{frame}

\begin{frame}[fragile]{Pasul 6: LSTM -- Arhitectură și Antrenare}
    \begin{block}{Cod Python -- Model LSTM}
        {\footnotesize
\begin{verbatim}
# Construim modelul LSTM
model_lstm = Sequential([
    LSTM(50, return_sequences=True, input_shape=(20, 1)),
    Dropout(0.2),
    LSTM(50, return_sequences=False),
    Dropout(0.2),
    Dense(25),
    Dense(1)
])

model_lstm.compile(optimizer='adam', loss='mse')

# Antrenăm
history = model_lstm.fit(
    X_train_lstm, y_train_lstm,
    epochs=50, batch_size=32,
    validation_split=0.1, verbose=0
)

# Predicție
pred_lstm_scaled = model_lstm.predict(X_test_lstm)
pred_lstm = scaler.inverse_transform(pred_lstm_scaled)
y_test_original = scaler.inverse_transform(y_test_lstm.reshape(-1, 1))
rmse_lstm = np.sqrt(mean_squared_error(y_test_original, pred_lstm))
print(f"LSTM - RMSE: {rmse_lstm:.4f}")
\end{verbatim}
        }
    \end{block}
\end{frame}

\begin{frame}{LSTM: Curba de Învățare}
    \begin{center}
        \includegraphics[width=0.95\textwidth, height=0.70\textheight, keepaspectratio]{charts/ch8_case_lstm_training.pdf}
    \end{center}
    \vspace{-0.2cm}
    {\footnotesize
    \textbf{Loss Training}: Scade rapid în primele epoci, apoi se stabilizează.\\
    \textbf{Loss Validation}: Urmărește training loss -- nu avem overfitting sever.
    }
\end{frame}

%=============================================================================
\section{Comparație Finală: Toate Metodele}
%=============================================================================

\begin{frame}{Comparație: Rezultate pe EUR/RON}
    \begin{center}
    \begin{tabular}{l|c|c|c|c}
        \toprule
        \textbf{Model} & \textbf{RMSE} & \textbf{MAE} & \textbf{Timp (s)} & \textbf{Interpretabil?} \\
        \midrule
        ARIMA(1,0,1) & 0.0938 & 0.0716 & 0.12 & Da \\
        Random Forest & 0.0626 & 0.0483 & 0.85 & Da (features) \\
        LSTM & 0.0683 & 0.0526 & 12.3 & Nu \\
        \bottomrule
    \end{tabular}
    \end{center}

    \vspace{0.3cm}

    \begin{alertblock}{Concluzii}
        \begin{itemize}
            \item Pentru EUR/RON, diferențele sunt \textbf{mici} -- piața este eficientă
            \item Random Forest oferă cel mai bun compromis \textbf{acuratețe/interpretabilitate}
            \item LSTM are cost computațional mare pentru câștig marginal
            \item ARIMA rămâne o alegere solidă pentru \textbf{baseline}
        \end{itemize}
    \end{alertblock}
\end{frame}

\begin{frame}{Vizualizare: Predicții vs Valori Reale}
    \begin{center}
        \includegraphics[width=0.98\textwidth, height=0.72\textheight, keepaspectratio]{charts/ch8_case_predictions.pdf}
    \end{center}
    \vspace{-0.3cm}
    {\footnotesize
    Toate modelele captează pattern-ul general, dar niciuna nu prezice perfect vârfurile de volatilitate. Aceasta reflectă \textbf{eficiența pieței} și \textbf{limitele predicției} pentru serii financiare.
    }
\end{frame}

\begin{frame}{Comparație Modele: Metrici de Performanță}
    \begin{center}
        \includegraphics[width=0.98\textwidth, height=0.72\textheight, keepaspectratio]{charts/ch8_case_comparison.pdf}
    \end{center}
    \vspace{-0.3cm}
    {\footnotesize
    \textbf{Stânga}: Metrici de eroare (mai mic = mai bine) -- Random Forest obține cel mai mic RMSE și MAE.\\
    \textbf{Dreapta}: Timp de antrenare (scală logaritmică) -- LSTM necesită semnificativ mai multe resurse computaționale.
    }
\end{frame}

\begin{frame}{Când să Alegem Fiecare Model?}
    \begin{columns}[T]
        \begin{column}{0.48\textwidth}
            \begin{block}{ARIMA/ARFIMA}
                \begin{itemize}
                    \item Date puține ($< 500$ obs.)
                    \item Interpretare importantă
                    \item Memorie lungă suspectată
                    \item Baseline rapid
                \end{itemize}
            \end{block}

            \begin{block}{Random Forest}
                \begin{itemize}
                    \item Multe variabile exogene
                    \item Relații neliniare
                    \item Importanța features
                    \item Date moderate
                \end{itemize}
            \end{block}
        \end{column}

        \begin{column}{0.48\textwidth}
            \begin{block}{LSTM/Deep Learning}
                \begin{itemize}
                    \item Date foarte mari ($> 10.000$)
                    \item Secvențe complexe
                    \item Resurse computaționale
                    \item Pattern-uri ascunse
                \end{itemize}
            \end{block}

            \begin{alertblock}{Regula de Aur}
                Începe simplu (ARIMA), adaugă complexitate doar dacă performanța crește semnificativ!
            \end{alertblock}
        \end{column}
    \end{columns}
\end{frame}

%=============================================================================
\section{Exemple Suplimentare cu Date Reale}
%=============================================================================

\begin{frame}{Exemplu 2: Indicele BET (Bursa București)}
    \begin{block}{Caracteristici}
        \begin{itemize}
            \item \textbf{Volatilitate clustering} puternic
            \item Influențat de piețele internaționale
            \item Lichiditate mai redusă decât piețele dezvoltate
            \item Potențial pentru memorie lungă în volatilitate
        \end{itemize}
    \end{block}

    \vspace{0.2cm}

    \begin{exampleblock}{Rezultate Tipice (RMSE pe randamente)}
        \begin{itemize}
            \item GARCH(1,1): 1.45 -- cel mai bun pentru volatilitate
            \item ARFIMA pentru volatilitate: 1.52
            \item Random Forest: 1.48
            \item LSTM: 1.51
        \end{itemize}
    \end{exampleblock}
\end{frame}

\begin{frame}{Exemplu 3: Rata Inflației în România}
    \begin{block}{Caracteristici}
        \begin{itemize}
            \item Serie \textbf{lunară} (frecvență redusă)
            \item \textbf{Persistență ridicată} -- șocurile durează
            \item Influențată de politica monetară
            \item Potențial puternic pentru \textbf{memorie lungă}
        \end{itemize}
    \end{block}

    \vspace{0.2cm}

    \begin{alertblock}{Rezultate Tipice}
        \begin{itemize}
            \item ARFIMA cu $d \approx 0.35$ -- captează persistența
            \item ARIMA subeștimează persistența șocurilor
            \item ML nu funcționează bine (date puține, ~300 obs.)
        \end{itemize}
    \end{alertblock}

    \vspace{0.2cm}

    {\footnotesize
    \textbf{Lecție}: Pentru serii lunare cu puține date, modelele clasice (ARFIMA) sunt superioare!
    }
\end{frame}

\begin{frame}{Rezumat Practic: Alegerea Modelului}
    \begin{center}
    \begin{tabular}{l|c|c|c|c}
        \toprule
        \textbf{Criteriu} & \textbf{ARIMA} & \textbf{ARFIMA} & \textbf{RF} & \textbf{LSTM} \\
        \midrule
        Date necesare & Puține & Puține & Medii & Multe \\
        Memorie lungă & Nu & \textbf{Da} & Parțial & Parțial \\
        Neliniaritate & Nu & Nu & \textbf{Da} & \textbf{Da} \\
        Interpretabil & \textbf{Da} & \textbf{Da} & Parțial & Nu \\
        Timp calcul & Rapid & Rapid & Mediu & Lent \\
        Var. exogene & Limitat & Limitat & \textbf{Da} & \textbf{Da} \\
        \bottomrule
    \end{tabular}
    \end{center}

    \vspace{0.3cm}

    \begin{block}{Fluxul Recomandat}
        \begin{enumerate}
            \item Începe cu \textbf{ARIMA} ca baseline
            \item Testează \textbf{memorie lungă} $\rightarrow$ ARFIMA dacă $d$ semnificativ
            \item Adaugă \textbf{features} $\rightarrow$ Random Forest
            \item Doar cu date multe și resurse $\rightarrow$ LSTM
        \end{enumerate}
    \end{block}
\end{frame}

%=============================================================================
\section{Rezumat și Quiz}
%=============================================================================

\begin{frame}{Rezumat}
    \begin{block}{Ce am învățat}
        \begin{itemize}
            \item \textbf{ARFIMA}: Extinde ARIMA pentru memorie lungă ($d$ fracționar)
            \item \textbf{Random Forest}: Ansamblu de arbori, relații neliniare, interpretabil
            \item \textbf{LSTM}: Deep learning pentru secvențe, dependențe complexe
            \item \textbf{Trade-offs}: Complexitate vs interpretabilitate vs date necesare
        \end{itemize}
    \end{block}

    \vspace{0.2cm}

    \begin{alertblock}{Recomandări Practice}
        \begin{itemize}
            \item Începe cu modele \textbf{simple} (ARIMA) ca baseline
            \item Folosește \textbf{Time Series CV} pentru evaluare corectă
            \item ML necesită \textbf{feature engineering} atent
            \item LSTM: doar cu \textbf{multe date} și resurse computaționale
        \end{itemize}
    \end{alertblock}
\end{frame}

\begin{frame}{Quiz Rapid}
    \begin{enumerate}
        \item Ce semnifică $d = 0.3$ într-un model ARFIMA?

        \vspace{0.3cm}

        \item De ce folosim Time Series CV în loc de k-fold standard?

        \vspace{0.3cm}

        \item Care este avantajul principal al LSTM față de RNN simplu?

        \vspace{0.3cm}

        \item Ce tip de model ai alege pentru date puține și relații liniare?

        \vspace{0.3cm}

        \item Ce înseamnă ``data leakage'' în contextul ML pentru serii de timp?
    \end{enumerate}
\end{frame}

\begin{frame}{Răspunsuri Quiz}
    {\small
    \begin{enumerate}
        \item \textbf{$d = 0.3$}: Memorie lungă, seria este staționară dar autocorelațiile scad lent (hiperbolic). Persistență moderată.

        \vspace{0.15cm}

        \item \textbf{Time Series CV}: Pentru a respecta ordinea temporală. K-fold standard ar folosi date viitoare pentru a prezice trecutul (data leakage).

        \vspace{0.15cm}

        \item \textbf{LSTM vs RNN}: LSTM rezolvă problemă ``vanishing gradient'' prin mecanismul de porți, permițând învățarea dependențelor pe termen lung.

        \vspace{0.15cm}

        \item \textbf{Date puține, relații liniare}: ARIMA sau ARFIMA. ML necesită multe date pentru a generaliza bine.

        \vspace{0.15cm}

        \item \textbf{Data leakage}: Folosirea informației din viitor în features sau în antrenare. Ex: calcularea mediei mobile folosind și date viitoare, sau k-fold standard care amestecă ordinea temporală.
    \end{enumerate}
    }
\end{frame}

\begin{frame}{Ce urmează?}
    \begin{block}{Extensii și Subiecte Avansate}
        \begin{itemize}
            \item \textbf{Transformer} pentru serii de timp (Temporal Fusion Transformer)
            \item \textbf{Prophet} (Facebook/Meta) pentru sezonalitate
            \item \textbf{Neural Prophet}: Prophet + rețele neuronale
            \item \textbf{Ensemble methods}: Combinarea mai multor modele
            \item \textbf{Anomaly detection} cu ML
        \end{itemize}
    \end{block}

    \vspace{0.5cm}

    \begin{center}
        \Large\textcolor{MainBlue}{Întrebări?}
    \end{center}
\end{frame}

\end{document}
