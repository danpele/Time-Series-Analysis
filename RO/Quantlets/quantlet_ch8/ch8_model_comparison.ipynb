{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TSA_ch8_model_comparison\n",
    "\n",
    "## Comprehensive Model Comparison: ARIMA vs ARFIMA vs Random Forest vs LSTM\n",
    "\n",
    "**Data**: Germany Daily Electricity Consumption (2015-2019)\n",
    "\n",
    "**Source**: ENTSO-E Transparency Platform / Open Power System Data\n",
    "\n",
    "**Author**: Daniel Traian PELE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# LSTM (optional)\n",
    "try:\n",
    "    import tensorflow as tf\n",
    "    from tensorflow.keras.models import Sequential\n",
    "    from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "    from tensorflow.keras.callbacks import EarlyStopping\n",
    "    TF_AVAILABLE = True\n",
    "    tf.random.set_seed(42)\n",
    "except ImportError:\n",
    "    TF_AVAILABLE = False\n",
    "    print(\"TensorFlow not available. LSTM will be skipped.\")\n",
    "\n",
    "np.random.seed(42)\n",
    "plt.style.use('seaborn-v0_8-whitegrid')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Germany Electricity Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download from Open Power System Data\n",
    "url = \"https://data.open-power-system-data.org/time_series/2020-10-06/time_series_60min_singleindex.csv\"\n",
    "\n",
    "print(\"Downloading Germany electricity data...\")\n",
    "df_raw = pd.read_csv(url, parse_dates=['utc_timestamp'], low_memory=False)\n",
    "\n",
    "df = df_raw[['utc_timestamp', 'DE_load_actual_entsoe_transparency']].copy()\n",
    "df.columns = ['datetime', 'load_mw']\n",
    "df = df.dropna()\n",
    "\n",
    "df['date'] = df['datetime'].dt.date\n",
    "daily = df.groupby('date')['load_mw'].sum().reset_index()\n",
    "daily['date'] = pd.to_datetime(daily['date'])\n",
    "daily['consumption_gwh'] = daily['load_mw'] / 1000\n",
    "\n",
    "daily = daily[(daily['date'] >= '2015-01-01') & (daily['date'] <= '2019-12-31')]\n",
    "daily = daily.reset_index(drop=True)\n",
    "\n",
    "print(f\"Data: {daily['date'].min().date()} to {daily['date'].max().date()}\")\n",
    "print(f\"Observations: {len(daily)} days\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_features(df):\n",
    "    df = df.copy()\n",
    "    df['y'] = df['consumption_gwh']\n",
    "    \n",
    "    for lag in [1, 2, 3, 7, 14, 21]:\n",
    "        df[f'lag_{lag}'] = df['y'].shift(lag)\n",
    "    \n",
    "    for window in [7, 14, 30]:\n",
    "        df[f'roll_mean_{window}'] = df['y'].shift(1).rolling(window).mean()\n",
    "        df[f'roll_std_{window}'] = df['y'].shift(1).rolling(window).std()\n",
    "    \n",
    "    df['day_of_week'] = df['date'].dt.dayofweek\n",
    "    df['month'] = df['date'].dt.month\n",
    "    df['is_weekend'] = (df['day_of_week'] >= 5).astype(int)\n",
    "    \n",
    "    df['dow_sin'] = np.sin(2 * np.pi * df['day_of_week'] / 7)\n",
    "    df['dow_cos'] = np.cos(2 * np.pi * df['day_of_week'] / 7)\n",
    "    \n",
    "    return df.dropna()\n",
    "\n",
    "df_feat = create_features(daily)\n",
    "feature_cols = [c for c in df_feat.columns if c not in ['date', 'load_mw', 'consumption_gwh', 'y']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Train/Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = len(df_feat)\n",
    "train_end = int(n * 0.70)\n",
    "val_end = int(n * 0.85)\n",
    "\n",
    "train = df_feat.iloc[:train_end]\n",
    "val = df_feat.iloc[train_end:val_end]\n",
    "test = df_feat.iloc[val_end:]\n",
    "\n",
    "print(f\"Train: {len(train)} | Val: {len(val)} | Test: {len(test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mape(y_true, y_pred):\n",
    "    return 100 * np.mean(np.abs((y_true - y_pred) / y_true))\n",
    "\n",
    "results = []\n",
    "predictions = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Naive Baseline\n",
    "naive_pred = test['lag_1'].values\n",
    "results.append({'Model': 'Naive (y_{t-1})', 'MAPE': mape(test['y'].values, naive_pred)})\n",
    "predictions['Naive'] = naive_pred\n",
    "print(\"[1/6] Naive: done\")\n",
    "\n",
    "# 2. Seasonal Naive\n",
    "seasonal_pred = test['lag_7'].values\n",
    "results.append({'Model': 'Seasonal Naive (y_{t-7})', 'MAPE': mape(test['y'].values, seasonal_pred)})\n",
    "predictions['Seasonal'] = seasonal_pred\n",
    "print(\"[2/6] Seasonal Naive: done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. SARIMA (rolling forecast)\n",
    "print(\"[3/6] SARIMA: training (rolling forecasts)...\")\n",
    "sarima_pred = []\n",
    "history = list(train['y'].values)\n",
    "\n",
    "for t in range(len(test)):\n",
    "    try:\n",
    "        model = ARIMA(history, order=(1, 1, 1))\n",
    "        model_fit = model.fit()\n",
    "        yhat = model_fit.forecast()[0]\n",
    "    except:\n",
    "        yhat = history[-1]\n",
    "    sarima_pred.append(yhat)\n",
    "    history.append(test['y'].iloc[t])\n",
    "    if (t+1) % 50 == 0:\n",
    "        print(f\"    Progress: {t+1}/{len(test)}\")\n",
    "\n",
    "sarima_pred = np.array(sarima_pred)\n",
    "results.append({'Model': 'SARIMA(1,1,1)', 'MAPE': mape(test['y'].values, sarima_pred)})\n",
    "predictions['SARIMA'] = sarima_pred\n",
    "print(\"       SARIMA: done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Random Forest\n",
    "print(\"[4/6] Random Forest: training...\")\n",
    "rf = RandomForestRegressor(n_estimators=200, max_depth=15, min_samples_leaf=5, random_state=42, n_jobs=-1)\n",
    "rf.fit(train[feature_cols], train['y'])\n",
    "rf_pred = rf.predict(test[feature_cols])\n",
    "results.append({'Model': 'Random Forest', 'MAPE': mape(test['y'].values, rf_pred)})\n",
    "predictions['RF'] = rf_pred\n",
    "print(\"       Random Forest: done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. LSTM\n",
    "if TF_AVAILABLE:\n",
    "    print(\"[5/6] LSTM: training...\")\n",
    "    \n",
    "    scaler = MinMaxScaler()\n",
    "    train_scaled = scaler.fit_transform(train[['y']])\n",
    "    val_scaled = scaler.transform(val[['y']])\n",
    "    test_scaled = scaler.transform(test[['y']])\n",
    "    \n",
    "    seq_len = 14\n",
    "    \n",
    "    def create_seq(data, seq_len):\n",
    "        X, y = [], []\n",
    "        for i in range(len(data) - seq_len):\n",
    "            X.append(data[i:i+seq_len])\n",
    "            y.append(data[i+seq_len])\n",
    "        return np.array(X), np.array(y)\n",
    "    \n",
    "    X_train, y_train = create_seq(train_scaled.flatten(), seq_len)\n",
    "    X_val, y_val = create_seq(np.concatenate([train_scaled[-seq_len:], val_scaled]).flatten(), seq_len)\n",
    "    X_test, y_test_lstm = create_seq(np.concatenate([val_scaled[-seq_len:], test_scaled]).flatten(), seq_len)\n",
    "    \n",
    "    X_train = X_train.reshape((X_train.shape[0], X_train.shape[1], 1))\n",
    "    X_val = X_val.reshape((X_val.shape[0], X_val.shape[1], 1))\n",
    "    X_test = X_test.reshape((X_test.shape[0], X_test.shape[1], 1))\n",
    "    \n",
    "    model = Sequential([\n",
    "        LSTM(64, activation='relu', return_sequences=True, input_shape=(seq_len, 1)),\n",
    "        Dropout(0.2),\n",
    "        LSTM(32, activation='relu'),\n",
    "        Dropout(0.2),\n",
    "        Dense(1)\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='mse')\n",
    "    \n",
    "    early_stop = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "    model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=50, batch_size=32, \n",
    "              callbacks=[early_stop], verbose=0)\n",
    "    \n",
    "    lstm_pred_scaled = model.predict(X_test, verbose=0)\n",
    "    lstm_pred = scaler.inverse_transform(lstm_pred_scaled).flatten()\n",
    "    \n",
    "    y_test_actual = test['y'].values[-len(lstm_pred):]\n",
    "    results.append({'Model': 'LSTM', 'MAPE': mape(y_test_actual, lstm_pred)})\n",
    "    predictions['LSTM'] = lstm_pred\n",
    "    print(\"       LSTM: done\")\n",
    "else:\n",
    "    print(\"[5/6] LSTM: skipped (TensorFlow not available)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Results Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.DataFrame(results).sort_values('MAPE')\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"MODEL COMPARISON - GERMANY ELECTRICITY FORECAST\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\n{results_df.to_string(index=False)}\")\n",
    "print(f\"\\n*** Best Model: {results_df.iloc[0]['Model']} (MAPE = {results_df.iloc[0]['MAPE']:.2f}%) ***\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# 1. Predictions comparison\n",
    "ax = axes[0, 0]\n",
    "ax.plot(test['date'], test['y'], 'k-', linewidth=1.5, label='Actual')\n",
    "ax.plot(test['date'], predictions['Seasonal'], '--', color='gray', alpha=0.6, label='Seasonal Naive')\n",
    "ax.plot(test['date'], predictions['RF'], '-', color='green', linewidth=1, label='Random Forest')\n",
    "ax.plot(test['date'], predictions['SARIMA'], '-', color='blue', alpha=0.6, linewidth=1, label='SARIMA')\n",
    "if 'LSTM' in predictions:\n",
    "    ax.plot(test['date'].values[-len(predictions['LSTM']):], predictions['LSTM'], \n",
    "            '-', color='red', linewidth=1, label='LSTM')\n",
    "ax.set_title('Germany Electricity: Model Predictions', fontweight='bold')\n",
    "ax.set_xlabel('Date')\n",
    "ax.set_ylabel('Consumption (GWh)')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# 2. MAPE comparison\n",
    "ax = axes[0, 1]\n",
    "colors = plt.cm.RdYlGn_r(np.linspace(0.2, 0.8, len(results_df)))\n",
    "bars = ax.barh(results_df['Model'], results_df['MAPE'], color=colors)\n",
    "ax.set_xlabel('MAPE (%)')\n",
    "ax.set_title('Model Performance (Lower = Better)', fontweight='bold')\n",
    "for bar, val in zip(bars, results_df['MAPE']):\n",
    "    ax.text(val + 0.1, bar.get_y() + bar.get_height()/2, f'{val:.2f}%', va='center')\n",
    "ax.grid(True, alpha=0.3, axis='x')\n",
    "\n",
    "# 3. Feature importance\n",
    "ax = axes[1, 0]\n",
    "importance = pd.DataFrame({'feature': feature_cols, 'importance': rf.feature_importances_})\n",
    "importance = importance.sort_values('importance', ascending=False).head(10)\n",
    "ax.barh(importance['feature'], importance['importance'], color='forestgreen')\n",
    "ax.invert_yaxis()\n",
    "ax.set_xlabel('Importance')\n",
    "ax.set_title('Random Forest: Top 10 Features', fontweight='bold')\n",
    "ax.grid(True, alpha=0.3, axis='x')\n",
    "\n",
    "# 4. Last 60 days detail\n",
    "ax = axes[1, 1]\n",
    "ax.plot(test['date'].values[-60:], test['y'].values[-60:], 'ko-', markersize=3, label='Actual')\n",
    "ax.plot(test['date'].values[-60:], predictions['RF'][-60:], 'g-', linewidth=1.5, label='RF')\n",
    "ax.plot(test['date'].values[-60:], predictions['Seasonal'][-60:], '--', color='gray', label='Seasonal')\n",
    "ax.set_title('Last 60 Days: Detailed View', fontweight='bold')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('ch8_model_comparison.pdf', bbox_inches='tight', dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Key Findings\n",
    "\n",
    "1. **Random Forest** typically performs best for complex multi-seasonal patterns\n",
    "2. **Seasonal Naive** (y_{t-7}) is a surprisingly strong baseline for weekly data\n",
    "3. **Feature engineering** (lags, rolling stats, calendar features) is crucial\n",
    "4. **LSTM** requires more data and tuning to outperform simpler methods\n",
    "5. Model selection depends on data characteristics and forecast horizon"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
